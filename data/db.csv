section,subsection,question,answer
Classical models,Linear Regression,What Is a Linear Regression Model? List Its Drawbacks.,"A linear regression model is a model in which there is a linear relationship between the dependent and independent variables. 
Here are the drawbacks of linear regression: 
Only the mean of the dependent variable is taken into consideration. 
It assumes that the data is independent. 
The method is sensitive to outlier data values. 
"
Classical models,Linear Regression,What are various assumptions used in linear regression? What would happen if they are violated?,"Linear regression is done under the following assumptions:
The sample data used for modeling represents the entire population.
There exists a linear relationship between the X-axis variable and the mean of the Y variable.
The residual variance is the same for any X values. This is called homoscedasticity. Residual Variance (also called unexplained variance or error variance) is the variance of any error (residual).
The errors or residuals of the data are normally distributed and independent from each other. 
There is minimal multicollinearity between explanatory variables 
Extreme violations of the above assumptions lead to redundant results. Smaller violations of these result in greater variance or bias of the estimates.
"
Classical models,Linear Regression,What methods for solving linear regression do you know?,"To solve linear regression, you need to find the coefficients which minimize the sum of squared errors.
Matrix Algebra method: Let's say you have X, a matrix of features, and y, a vector with the values you want to predict. After going through the matrix algebra and minimization problem, you get this solution:
But solving this requires you to find an inverse, which can be time-consuming, if not impossible. Luckily, there are methods like Singular Value Decomposition (SVD) or QR Decomposition that can reliably calculate this part  (called the pseudo-inverse) without actually needing to find an inverse. The popular python ML library sklearn uses SVD to solve least squares.
Alternative method: Gradient Descent. 
"
Classical models,Linear Regression,Maximum Likelihood Estimation for Linear Regression,"Linear regression can be written as a CPD in the following manner:
For linear regression we assume that μ(x) is linear and so μ(x)=βTx. We must also assume that the variance in the model is fixed (i.e. that it doesn't depend on x) and as such σ2(x)=σ2, a constant. This then implies that our parameter vector θ=(β,σ2).
In linear regression problems we need to make the assumption that the feature vectors are all independent and identically distributed (iid). This makes it far simpler to solve the log-likelihood problem, using properties of natural logarithms. Since we will be differentiating these values it is far easier to differentiate a sum than a product, hence the logarithm:
For reasons of increased computational ease, it is often easier to minimise the negative of the log-likelihood rather than maximise the log-likelihood itself. Hence, we can ""stick a minus sign in front of the log-likelihood"" to give us the negative log-likelihood (NLL):
Where RSS(β):=∑i=1N(yi−βTxi)2 is the Residual Sum of Squares, also known as the Sum of Squared Errors (SSE).
Since the first term in the equation is a constant we simply need to concern ourselves with minimising the RSS, which will be sufficient for producing the optimal parameter estimate.
To simply the notation we can write this latter term in matrix form. By defining the N×(p+1) matrix X we can write the RSS term as:
At this stage we now want to differentiate this term w.r.t. the parameter variable 

There is an extremely key assumption to make here. We need XTX to be positive-definite, which is only the case if there are more observations than there are dimensions. If this is not the case (which is extremely common in high-dimensional settings) then it is not possible to find a unique set of β coefficients and thus the following matrix equation will not hold.
Under the assumption of a positive-definite XTX we can set the differentiated equation to zero and solve for β:
The solution to this matrix equation provides 
"
Classical models,Linear Regression,Ordinary least squares,"The ordinary least squares (OLS) method can be defined as a linear regression technique that is used to estimate the unknown parameters in a model. The OLS method minimizes the sum of squared residuals (SSR), defined as the difference between the actual (observed values of the dependent variable) and the predicted values from the model. The Ordinary Least Squares (OLS) method achieves the minimization of the Sum of Squared Residuals (SSR) by optimizing the values of the coefficients in a regression model so that the total of the squares of the differences between the observed values and the values predicted by the model is as small as possible. The resulting line representing the dependent variable of the linear regression model is called the regression line. This regression line represents the best fit for the data.
The OLS method relies on several assumptions to be valid. The following is the list of key assumptions:
Linearity: There must be linear relationship between the dependent variable and the independent variables.
Independence: The observations must be independent of each other.
Homoscedasticity: The variance of the residuals should be constant across all levels of the independent variables.
Normality: The residuals / errors should be normally distributed.
No multicollinearity: The independent variables should not be highly correlated with each other.
"
Classical models,Linear Regression,Maximum a-posteriori Estimation as Regularization,"Maximum a-posteriori (MAP) method adds a prior distribution of the parameters θ:
θMAP=argmaxθp(x|y,θ)p(θ)
The optimal solution must still match the data but it has also to conform to your prior knowledge about the parameter distribution.
How is this related to adding a regularizer term to a loss function?
Instead of optimizing the posterior directly, one often optimizes negative of the logarithm instead:
θMAP=argminθ−logp(x|y,θ)p(θ)=argminθ−(logp(x|y,θ)+logp(θ))
Assuming you want the parameters θ to be normally distributed around zero, you get logp(θ)∝||θ||^2
Gaussian prior p(θ) = N(0, b^2I) is equivalent to the loss function ∥y − Φθ∥^2 + λ ∥θ∥^2, λ = σ 2/ b 2.
Minimizing the regularized least-squares loss function yields θRLS = (Φ ⊤Φ + λI) ^−1Φ ⊤ y
"
Classical models,Linear Regression,Bayesian Linear Regression,"Bayesian linear regression pushes the idea of the parameter prior a step regression further and does not even attempt to compute a point estimate of the parameters, but instead the full posterior distribution over the parameters is taken into account when making predictions. This means we do not fit any parameters, but we compute a mean over all plausible parameters settings (according to the posterior).
In Bayesian linear regression, we consider the model 
prior p(θ) = N(m0, S0),
likelihood p(y|x, θ) = N(y| ϕ ⊤ (x)θ, σ2)
where we now explicitly place a Gaussian prior on θ, which turns the parameter vector into a random variable.
In practice, we are usually not so much interested in the parameter values θ themselves. Instead, our focus often lies in the predictions we make with those parameter values. In a Bayesian setting, we take the parameter distribution and average over all plausible parameter settings when we make predictions. More specifically, to make predictions at an input x∗, we integrate out θ.
"
Classical models,Linear Regression,Your linear regression didn’t run and communicates that there are an infinite number of best estimates for the regression coefficients. What could be wrong?,"p > n.
If some of the explanatory variables are perfectly correlated (positively or negatively) then the coefficients would not be unique. 
"
Classical models,Linear Regression,What kind of regularization techniques are applicable to linear models? ,"AIC/BIC, Ridge regression, Lasso, Elastic Net, Basis pursuit denoising, Rudin–Osher–Fatemi model (TV), Potts model, RLAD, Dantzig Selector, SLOPE
"
Classical models,Linear Regression,"You have several variables that are positively correlated with your response, and you think combining all of the variables could give you a good prediction of your response. However, you see that in the multiple linear regression, one of the weights on the predictors is negative. What could be the issue?","Multicollinearity refers to a situation in which two or more explanatory variables in a multiple regression model are highly linearly related. 
Leave the model as is, despite multicollinearity. The presence of multicollinearity doesn't affect the efficiency of extrapolating the fitted model to new data provided that the predictor variables follow the same pattern of multicollinearity in the new data as in the data on which the regression model is based.
principal component regression
"
Classical models,Linear Regression,When do we need to perform feature normalization for linear models? When it’s okay not to do it? ,"Feature normalization is necessary for L1 and L2 regularizations. The idea of both methods is to penalize all the features relatively equally. This can't be done effectively if every feature is scaled differently.
Linear regression without regularization techniques can be used without feature normalization. Also, regularization can help to make the analytical solution more stable, — it adds the regularization matrix to the feature matrix before inverting it.
"
Classical models,Logistic Regression,"What are various ways to predict a binary response variable? Can you compare two of them and tell me when one would be more appropriate? What’s the difference between these? (SVM, Logistic Regression, Naive Bayes, Decision Tree, etc.)","Things to look at: N, P, linearly seperable?, features independent?, likely to overfit?, speed, performance, memory usage
Logistic Regression
features roughly linear, problem roughly linearly separable
robust to noise, use l1, l2 regularization for model selection, avoid overfitting
the output come as probabilities
efficient and the computation can be distributed
can be used as a baseline for other algorithms
(-) can hardly handle categorical features
SVM
with a nonlinear kernel, can deal with problems that are not linearly separable
(-) slow to train, for most industry scale applications, not really efficient
Naive Bayes
computationally efficient when P is large by alleviating the curse of dimensionality
works surprisingly well for some cases even if the condition doesn’t hold
with word frequencies as features, the independence assumption can be seen reasonable. So the algorithm can be used in text categorization
(-) conditional independence of every other feature should be met
Tree Ensembles
good for large N and large P, can deal with categorical features very well
non parametric, so no need to worry about outliers
GBT’s work better but the parameters are harder to tune
RF works out of the box, but usually performs worse than GBT
Deep Learning
works well for some classification tasks (e.g. image)
used to squeeze something out of the problem
"
Classical models,Logistic Regression,What is Logistic Regression?,"Logistic regression is used for binary classification where we use sigmoid function, that takes input as independent variables and produces a probability value between 0 and 1.
The logistic regression model transforms the linear regression function continuous value output into categorical value output using a sigmoid function, which maps any real-valued set of independent variables input into a value between 0 and 1. This function is known as the logistic function.
Let the independent input features be X  and the dependent variable is Y having only binary value i.e. 0 or 1, then, apply the multi-linear function to the input variables X.
z=w⋅X+b
Now we use the sigmoid function where the input will be z and we find the probability between 0 and 1. i.e. predicted y.
P(y=1)=σ(z)
P(y=0)=1−σ(z)
The odd is the ratio of something occurring to something not occurring. it is different from probability as the probability is the ratio of something occurring to everything that could possibly occur. so odd will be:
p(x)/(1−p(x)) =e^z
The final logistic regression equation will be:
p(X;b,w)=1/(1+e^(-w⋅X+b))

"
Classical models,Logistic Regression,What is the difference between odds and probability?,"The probability that an event will occur is the fraction of times you expect to see that event in many trials. Probabilities always range between 0 and 1.
The odds are defined as the probability that the event will occur divided by the probability that the event will not occur.
"
Classical models,Logistic Regression,Logistic Regression Parameter Interpretation,"log  p / 1−p  = α + β1x1 + β2x2, where x1 is binary (as before) and x2 is a continuous predictor.
e^β1 is the relative increase in the odds, going from x1 = 0 to x1 = 1 holding x2 fixed
If you increase x2 from k to k + ∆, then odds increase e^β2∆ times.
"
Classical models,Logistic Regression,Assumptions of Logistic Regression,"We will explore the assumptions of logistic regression as understanding these assumptions is important to ensure that we are using appropriate application of the model. The assumption include:
Independent observations: Each observation is independent of the other, meaning there is no correlation between any input variables.
Binary dependent variables: It takes the assumption that the dependent variable must be binary or dichotomous, meaning it can take only two values. For more than two categories SoftMax functions are used.
Linearity relationship between independent variables and log odds: The relationship between the independent variables and the log odds of the dependent variable should be linear.
No outliers: There should be no outliers in the dataset.
Large sample size: The sample size is sufficiently large
"
Classical models,Logistic Regression,What distinguishes Logistic Regression from Linear Regression?,"While Linear Regression is used to predict continuous outcomes, Logistic Regression is used to predict the likelihood of an observation falling into a specific category. Logistic Regression employs an S-shaped logistic function to map predicted values between 0 and 1.
"
Classical models,Logistic Regression,What is the effect on the coefficients of logistic regression if two predictors are highly correlated? ,"When predictor variables are correlated, the estimated regression coefficient of any one variable depends on which other predictor variables are included in the model. When predictor variables are correlated, the precision of the estimated regression coefficients decreases as more predictor variables are added to the model.
In statistics, multicollinearity (also collinearity) is a phenomenon in which two or more predictor variables in a multiple regression model are highly correlated, meaning that one can be linearly predicted from the others with a substantial degree of accuracy. In this situation the coefficient estimates of the multiple regression may change erratically in response to small changes in the model or the data. Multicollinearity does not reduce the predictive power or reliability of the model as a whole, at least within the sample data set; it only affects calculations regarding individual predictors. That is, a multiple regression model with correlated predictors can indicate how well the entire bundle of predictors predicts the outcome variable, but it may not give valid results about any individual predictor, or about which predictors are redundant with respect to others.
The consequences of multicollinearity:
Ratings estimates remain unbiased.
Standard coefficient errors increase.
The calculated t-statistics are underestimated.
Multicollinearity reduces the precision of the estimated coefficients, which weakens the statistical power of your regression model. You might not be able to trust the p-values to identify independent variables that are statistically significant.
Estimates become very sensitive to changes in specifications and changes in individual observations.
The overall quality of the equation, as well as estimates of variables not related to multicollinearity, remain unaffected.
The closer multicollinearity to perfect (strict), the more serious its consequences.
Indicators of multicollinearity:
High R2 and negligible odds.
Strong pair correlation of predictors.
Strong partial correlations of predictors.
High VIF - variance inflation factor.
"
Classical models,KNN,Nearest Neighbors Method,"The nearest neighbors method (k-Nearest Neighbors, or k-NN) is another very popular classification method that is also sometimes used in regression problems. This, like decision trees, is one of the most comprehensible approaches to classification. The underlying intuition is that you look like your neighbors. More formally, the method follows the compactness hypothesis: if the distance between the examples is measured well enough, then similar examples are much more likely to belong to the same class.

To classify each sample from the test set, one needs to perform the following operations in order:
Calculate the distance to each of the samples in the training set.
Select k samples from the training set with the minimal distance to them.
The class of the test sample will be the most frequent class among those k nearest neighbors.
"
Classical models,KNN,Nearest Neighbors Method in Real Applications,"k-NN can serve as a good starting point (baseline) in some cases;
In Kaggle competitions, k-NN is often used for the construction of meta-features (i.e. k-NN predictions as input to other models) or for stacking/blending;
The nearest neighbors method extends to other tasks like recommendation systems. The initial decision could be a recommendation of a product (or service) that is popular among the closest neighbors of the person for whom we want to make a recommendation;
In practice, on large datasets, approximate methods of search are often used for nearest neighbors. There is a number of open source libraries that implement such algorithms; check out Spotify’s library Annoy.
The quality of classification/regression with k-NN depends on several parameters:
The number of neighbors k.
The distance measure between samples (common ones include Hamming, Euclidean, cosine, and Minkowski distances). Note that most of these metrics require data to be scaled. Simply speaking, we do not want the “salary” feature, which is on the order of thousands, to affect the distance more than “age”, which is generally less than 100.
Weights of neighbors (each neighbor may contribute different weights; for example, the further the sample, the lower the weight).
"
Classical models,KNN,Pros and Cons of The nearest neighbors method,"Pros:
Simple implementation;
Well studied;
Typically, the method is a good first solution not only for classification or regression, but also recommendations;
It can be adapted to a certain problem by choosing the right metrics or kernel (in a nutshell, the kernel may set the similarity operation for complex objects such as graphs while keeping the k-NN approach the same). By the way, Alexander Dyakonov, a former top-1 kaggler, loves the simplest k-NN but with the tuned object similarity metric;
Good interpretability. There are exceptions: if the number of neighbors is large, the interpretability deteriorates (“We did not give him a loan, because he is similar to the 350 clients, of which 70 are the bad, and that is 12% higher than the average for the dataset”).
Cons:
Method considered fast in comparison with compositions of algorithms, but the number of neighbors used for classification is usually large (100-150) in real life, in which case the algorithm will not operate as fast as a decision tree.
If a dataset has many variables, it is difficult to find the right weights and to determine which features are not important for classification/regression.
Dependency on the selected distance metric between the objects. Selecting the Euclidean distance by default is often unfounded. You can find a good solution by grid searching over parameters, but this becomes very time consuming for large datasets.
There are no theoretical ways to choose the number of neighbors – only grid search (though this is often true for all hyperparameters of all models). In the case of a small number of neighbors, the method is sensitive to outliers, that is, it is inclined to overfit.
As a rule, it does not work well when there are a lot of features due to the “curse of dimensionality”. Professor Pedro Domingos, a well-known member in the ML community, talks about it here in his popular paper, “A Few Useful Things to Know about Machine Learning”; also “the curse of dimensionality” is described in the Deep Learning book in this chapter.
"
Classical models,KNN,"Which machine learning algorithm is known as the lazy learner, and why is it called so?","KNN is a Machine Learning algorithm known as a lazy learner. K-NN is a lazy learner because it doesn’t learn any machine-learned values or variables from the training data but dynamically calculates distance every time it wants to classify, hence memorizing the training dataset instead. 
"
Classical models,Decision Trees,Tree-building Algorithm,"At the heart of the popular algorithms for decision tree construction, such as ID3 or C4.5, lies the principle of greedy maximization of information gain: at each step, the algorithm chooses the variable that gives the greatest information gain upon splitting. Then the procedure is repeated recursively until the entropy is zero (or some small value to account for overfitting). Different algorithms use different heuristics for “early stopping” or “cut-off” to avoid constructing an overfitted tree.
An attribute with the highest information gain from a set should be selected as the parent (root) node.
Build child nodes for every value of attribute A.
Repeat iteratively until you finish constructing the whole tree.
"
Classical models,Decision Trees,Information Gain,"We can define information gain as a measure of how much information a feature provides about a class. Information gain helps to determine the order of attributes in the nodes of a decision tree.
The main node is referred to as the parent node, whereas sub-nodes are known as child nodes. We can use information gain to determine how good the splitting of nodes in a decision tree.
It can help us determine the quality of splitting, as we shall soon see. The calculation of information gain should help us understand this concept better.
Gain=Eparent−Echildren
The term Gain represents information gain. EparentEparent is the entropy of the parent node and E_{children} is the average entropy of the child nodes. Let’s use an example to visualize information gain and its calculation.
The more the entropy removed, the greater the information gain. The higher the information gain, the better the split.
"
Classical models,Decision Trees,Tree’s Feature Importance from Mean Decrease in Impurity (MDI),"The impurity-based feature importance ranks the numerical features to be the most important features. 
This problem stems from two limitations of impurity-based feature importances:
impurity-based importances are biased towards high cardinality features;
impurity-based importances are computed on training set statistics and therefore do not reflect the ability of feature to be useful to make predictions that generalize to the test set (when the model has enough capacity).
"
Classical models,Decision Trees,Pros and Cons of Decision trees,"Pros:
Generation of clear human-understandable classification rules, e.g. “if age <25 and is interested in motorcycles, deny the loan”. This property is called interpretability of the model.
Decision trees can be easily visualized, i.e. both the model itself (the tree) and prediction for a certain test object (a path in the tree) can “be interpreted”.
Fast training and forecasting.
Small number of model parameters.
Supports both numerical and categorical features.
Cons:
The trees are very sensitive to the noise in input data; the whole model could change if the training set is slightly modified (e.g. remove a feature, add some objects). This impairs the interpretability of the model.
Separating border built by a decision tree has its limitations – it consists of hyperplanes perpendicular to one of the coordinate axes, which is inferior in quality to some other methods, in practice.
We need to avoid overfitting by pruning, setting a minimum number of samples in each leaf, or defining a maximum depth for the tree. Note that overfitting is an issue for all machine learning methods.
Instability. Small changes to the data can significantly change the decision tree. This problem is tackled with decision tree ensembles (discussed next time).
The optimal decision tree search problem is NP-complete. Some heuristics are used in practice such as greedy search for a feature with maximum information gain, but it does not guarantee finding the globally optimal tree.
Difficulties to support missing values in the data. Friedman estimated that it took about 50% of the code to support gaps in data in CART (an improved version of this algorithm is implemented in sklearn).
The model can only interpolate but not extrapolate (the same is true for random forests and tree boosting). That is, a decision tree makes constant prediction for the objects that lie beyond the bounding box set by the training set in the feature space. In our example with the yellow and blue balls, it would mean that the model gives the same predictions for all balls with positions >19 or <0.
"
Classical models,Decision Forest,Bootstrapping,"The bootstrap method goes as follows. Let there be a sample X of size N. We can make a new sample from the original sample by drawing N elements from the latter randomly and uniformly, with replacement. In other words, we select a random element from the original sample of size 
 and do this N times. All elements are equally likely to be selected, thus each element is drawn with the equal probability 1/N.
By repeating this procedure M times, we create M bootstrap samples X1,…XM. In the end, we have a sufficient number of samples and can compute various statistics of the original distribution.
"
Classical models,Decision Forest,Random Forest,"The random forest algorithm is an expansion of decision tree, in that you first construct a multitude of decision trees with training data, then fit your new data within one of the trees as a “random forest.”
It, essentially, averages your data to connect it to the nearest tree on the data scale. Random forest models are helpful as they remedy for the decision tree’s problem of “forcing” data points within a category unnecessarily. 
While an individual tree is overfit to the training data and is likely to have large error, bagging (Bootstrap Aggregating) uses the insight that a suitably large number of uncorrelated errors average out to zero to solve this problem. Bagging chooses multiple random samples of observations from the training data, with replacement, constructing a tree from each one. Since each tree learns from different data, they are fairly uncorrelated from one another. Plotting the R² of our model as we increase the number of “bagged” trees ( scikit-learn calls these trees estimators) illustrates the power of this technique.
The algorithm for constructing a random forest of N trees goes as follows:
For each k=1,…,N:
Generate a bootstrap sample Xk.
Build a decision tree bk on the sample Xk:
Pick the best feature according to the given criteria. Split the sample by this feature to create a new tree level. Repeat this procedure until the sample is exhausted.
Building the tree until any of its leaves contains no more than nmin instances or until a certain depth is reached.
For each split, we first randomly pick m features from the d original ones and then search for the next best split only among the subset.
The final classifier is defined by:
a(x)=1N∑k=1Nbk(x)
We use the majority voting for classification and the mean for regression.
For classification problems, it is advisable to set m=sqrt(d). For regression problems, we usually take m=d/3, where d is the number of features. It is recommended to build each tree until all of its leaves contain only nmin=1 examples for classification and nmin=5 examples for regression.
You can see random forest as bagging of decision trees with the modification of selecting a random subset of features at each split.
The main difference between random forests and bagging is that, in a random forest, the best feature for a split is selected from a random subset of the available features while, in bagging, all features are considered for the next best split.
Decision boundary of the decision tree is quite jagged and has a lot of acute angles that suggest overfitting and a weak ability to generalize. We would have trouble making reliable predictions on new test data. In contrast, the bagging algorithm has a rather smooth boundary and has no obvious signs of overfitting.
"
Classical models,Decision Forest,Parameters of Random Forest,"n_estimators — the number of trees in the forest (default = 10)
criterion — the function used to measure the quality of a split. Supported criteria are “mse” for the mean squared error, which is equal to variance reduction as feature selection criterion, and “mae” for the mean absolute error (default = “mse”)
max_features — the number of features to consider when looking for the best split. You can specify the number or percentage of features, or choose from the available values: “auto” (all features), “sqrt”, “log2”. (default = “auto”)
max_depth — the maximum depth of the tree (default means that nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples)
min_samples_split — the minimum number of samples required to split an internal node. Can be specified as the number or as a percentage of a total number of samples (default = 2)
min_samples_leaf — the minimum number of samples required at a leaf node(default = 1)
min_weight_fraction_leaf — the minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided (default = 0)
max_leaf_nodes — the maximum number of leaves (default = no restrictions)
min_impurity_split — threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf (default = 1е-7)
bootstrap — whether bootstrap samples are used when building trees(default = True)
oob_score — whether to use out-of-bag samples to estimate the R^2 on unseen data (default = False)
n_jobs — the number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores (default = 1)
random_state — if int, random_state is the seed used by the random number generator; if RandomState instance, random_state is the random number generator; if None, the random number generator is the RandomState instance used by np.random (default = None)
verbose — controls the verbosity of the tree building process (default = 0)
warm_start — when set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest (default = False)
In case of classification, parameters are mostly the same. Only the following differ for RandomForestClassifier as compared to RandomForestRegressor:
criterion — the function used to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific (default = “gini”)
class_weight — the weight of each class (by default all weights equal to 1, but you can create a dictionary with weights or specify it as “balanced” - uses the values of classes to automatically adjust weights inversely proportional to class frequencies in the input data or as “balanced_subsample” - the same as “balanced” except that weights are computed based on the bootstrap sample for every tree grown)
Below are the parameters which we need to pay attention to when we are building a new model:
n_estimators — the number of trees in the forest;
criterion — the function used to measure the quality of a split;
max_features — the number of features to consider when looking for the best split;
min_samples_leaf — the minimum number of samples required to be at a leaf node;
max_depth — the maximum depth of the tree.
"
Classical models,Decision Forest,Random Forest Feature importance,"Permutation importance. The average reduction in accuracy caused by a variable is determined during the calculation of the out-of-bag error. The greater the reduction in accuracy due to an exclusion or permutation of the variable, the higher its importance score. For this reason, variables with a greater average reduction in accuracy are generally more significant for classification.
Sklearn library uses another approach to determine feature importance. The rationale for that method is that the more gain in information the node (with splitting feature Xj) provides, the higher its importance. The average reduction in the Gini impurity – or MSE for regression – represents the contribution of each feature to the homogeneity of nodes and leaves in the resulting Random Forest model. Each time a selected feature is used for splitting, the Gini impurity of the child nodes is calculated and compared with that of the original node.
"
Classical models,Decision Forest,Variance and Decorrelation of Random Forests,"Let’s write the variance of a random forest as
Var f(x)=ρ(x)σ2(x)
ρ(x)=Corr[T(x1,Θ1(Z)),T(x2,Θ2(Z))],
where
ρ(x) is the sample correlation coefficient between any two trees used in averaging:
Θ1(Z) and Θ2(Z) are a randomly selected pair of trees on randomly selected elements of the sample Z;
T(x,Θi(Z)) is the output of the i-th tree classifier on an input vector x;
σ2(x) is the sample variance of any randomly selected tree:
σ2(x)=Var[T(x,Θ(X))]
It is easy to confuse ρ(x) with the average correlation between the trained trees in a given random forest when we consider trees as N-vectors and calculate the average pairwise correlation between them. But this is not the case.
In fact, this conditional correlation is not directly related to the averaging process, and the dependence of ρ(x) on x warns us of this difference. ρ(x) is the theoretical correlation between a pair of random trees estimated on the input x. Its value comes from the repeated sampling of the training set from the population Z and the subsequent random choice of a pair of trees. In statistics jargon, this is the correlation caused by the sampling distribution of Z and Θ.
The conditional covariance of any pair of trees is equal to 0 because bootstrapping and feature selection are independent and identically distributed.
If we consider the variance of a single tree, it barely depends on the parameters of the splitting (m). But they are crucial for ensembles. The variance of a tree is much higher than the one of an ensemble. The book The Elements of Statistical Learning (Trevor Hastie, Robert Tibshirani and Jerome Friedman) has a great example that demonstrates this fact:
Just as in bagging, the bias of a random forest is the same as the bias of a single tree T(x,Θ(Z)):
In absolute value, the bias is usually higher than that of an unpruned tree because randomization and sample space reduction impose their own restrictions on the model. Therefore, the improvements in prediction accuracy obtained by bagging and random forests are solely the result of variance reduction.
"
Classical models,Decision Forest,Pros and cons of random forests,"Pros:
High prediction accuracy; will perform better than linear algorithms in most problems; the accuracy is comparable with that of boosting.
Robust to outliers, thanks to random sampling.
Insensitive to the scaling of features as well as any other monotonic transformations due to the random subspace selection.
Doesn’t require fine-grained parameter tuning, works quite well out-of-the-box. With tuning, it is possible to achieve a 0.5–3% gain in accuracy, depending on the problem setting and data.
Efficient for datasets with a large number of features and classes.
Handles both continuous and discrete variables equally well.
Rarely overfits. In practice, an increase in the tree number almost always improves the composition. But, after reaching a certain number of trees, the learning curve is very close to the asymptote.
There are developed methods to estimate feature importance.
Works well with missing data and maintains good accuracy even when a large part of data is missing.
Provides means to weight classes on the whole dataset as well as for each tree sample.
Under the hood, calculates proximities between pairs of examples that can subsequently be used in clustering, outlier detection, or interesting data representations.
The above functionality and properties may be extended to unlabeled data to enable unsupervised clustering, data visualization, and outlier detection.
Easily parallelized and highly scalable.
Cons:
In comparison with a single decision tree, Random Forest’s output is more difficult to interpret.
There are no formal p-values for feature significance estimation.
Performs worse than linear methods in the case of sparse data: text inputs, bag of words, etc.
Unlike linear regression, Random Forest is unable to extrapolate. But, this can be also regarded as an advantage because outliers do not cause extreme values in Random Forests.
Prone to overfitting in some problems, especially, when dealing with noisy data.
In the case of categorical variables with varying level numbers, random forests favor variables with a greater number of levels. The tree will fit more towards a feature with many levels because this gains greater accuracy.
If a dataset contains groups of correlated features, preference might be given to groups of smaller size (“correlation bias”). See this work
The resulting model is large and requires a lot of RAM.
"
Classical models,Bagging,Bagging,"Suppose that we have a training set X. Using bootstrapping, we generate samples X1,…,XM. Now, for each bootstrap sample, we train its own classifier ai(x). The final classifier will average the outputs from all these individual classifiers. In the case of classification, this technique corresponds to voting: 
Bagging reduces the variance of a classifier by decreasing the difference in error when we train the model on different datasets. In other words, bagging prevents overfitting. The efficiency of bagging comes from the fact that the individual models are quite different due to the different training data and their errors cancel each other out during voting. Additionally, outliers are likely omitted in some of the training bootstrap samples.
Bagging is effective on small datasets. Dropping even a small part of training data leads to constructing substantially different base classifiers. If you have a large dataset, you would generate bootstrap samples of a much smaller size.
"
Classical models,Bagging,Out-of-bag error,"Looking ahead, in case of Random Forest, there is no need to use cross-validation or hold-out samples in order to get an unbiased error estimation. Why? Because, in ensemble techniques, the error estimation takes place internally.
Random trees are constructed using different bootstrap samples of the original dataset. Approximately 37% of inputs are left out of a particular bootstrap sample and are not used in the construction of the k-th tree.
This is easy to prove. Suppose there are ℓ examples in our dataset. At each step, each data point has equal probability of ending up in a bootstrap sample with replacement, probability 1/ℓ. The probability that there is no such bootstrap sample that contains a particular dataset element (i.e. it has been omitted ℓ times) equals (1−1/ℓ)^ℓ. When ℓ→+∞, it becomes equal to the Second Remarkable Limit 1e. Then, the probability of selecting a specific example is ≈1−1e≈63%.
The Out-of-Bag error is then computed in the following way:
take all instances that have been chosen as a part of test set for some tree (in the picture above that would be all instances in the lower-right picture). All together, they form an Out-of-Bag dataset;
take a specific instance from the Out-of-Bag dataset and all models (trees) that were not trained with this instance;
compare the majority vote of these trees’ classifications and compare it with the true label for this instance;
do this for all instances in the Out-of-Bag dataset and get the average OOB error.
"
Classical models,Boosting,AdaBoost,"AdaBoost (Adaptive Boosting) is a very popular boosting technique that aims at combining multiple weak classifiers to build one strong classifier. The original AdaBoost paper was authored by Yoav Freund and Robert Schapire.
A single classifier may not be able to accurately predict the class of an object, but when we group multiple weak classifiers with each one progressively learning from the others' wrongly classified objects, we can build one such strong model. The classifier mentioned here could be any of your basic classifiers, from Decision Trees (often the default) to Logistic Regression, etc.
Now we may ask, what is a ""weak"" classifier? A weak classifier is one that performs better than random guessing, but still performs poorly at designating classes to objects. For example, a weak classifier may predict that everyone above the age of 40 could not run a marathon but people falling below that age could. Now, you might get above 60% accuracy, but you would still be misclassifying a lot of data points!
Rather than being a model in itself, AdaBoost can be applied on top of any classifier to learn from its shortcomings and propose a more accurate model. It is usually called the “best out-of-the-box classifier” for this reason.
Let's try to understand how AdaBoost works with Decision Stumps. Decision Stumps are like trees in a Random Forest, but not ""fully grown."" They have one node and two leaves. AdaBoost uses a forest of such stumps rather than trees.
Stumps alone are not a good way to make decisions. A full-grown tree combines the decisions from all variables to predict the target value. A stump, on the other hand, can only use one variable to make a decision. Let's try and understand the behind-the-scenes of the AdaBoost algorithm step-by-step by looking at several variables to determine whether a person is ""fit"" (in good health) or not.
An Example of How AdaBoost Works
Step 1: A weak classifier (e.g. a decision stump) is made on top of the training data based on the weighted samples. Here, the weights of each sample indicate how important it is to be correctly classified. Initially, for the first stump, we give all the samples equal weights.
Step 2: We create a decision stump for each variable and see how well each stump classifies samples to their target classes. For example, in the diagram below we check for Age, Eating Junk Food, and Exercise. We'd look at how many samples are correctly or incorrectly classified as Fit or Unfit for each individual stump.
Step 3: More weight is assigned to the incorrectly classified samples so that they're classified correctly in the next decision stump. Weight is also assigned to each classifier based on the accuracy of the classifier, which means high accuracy = high weight!
Step 4: Reiterate from Step 2 until all the data points have been correctly classified, or the maximum iteration level has been reached.
"
Classical models,Boosting,Gradient Boosting,"Gradient Boosting is a powerful boosting algorithm that combines several weak learners into strong learners, in which each new model is trained to minimize the loss function such as mean squared error or cross-entropy of the previous model using gradient descent. In each iteration, the algorithm computes the gradient of the loss function with respect to the predictions of the current ensemble and then trains a new weak model to minimize this gradient. The predictions of the new model are then added to the ensemble, and the process is repeated until a stopping criterion is met.
In contrast to AdaBoost, the weights of the training instances are not tweaked, instead, each predictor is trained using the residual errors of the predecessor as labels. There is a technique called the Gradient Boosted Trees whose base learner is CART (Classification and Regression Trees). The below diagram explains how gradient-boosted trees are trained for regression problems.
The ensemble consists of M trees. Tree1 is trained using the feature matrix X and the labels y. The predictions labeled y1(hat) are used to determine the training set residual errors r1. Tree2 is then trained using the feature matrix X and the residual errors r1 of Tree1 as labels. The predicted results r1(hat) are then used to determine the residual r2. The process is repeated until all the M trees forming the ensemble are trained. There is an important parameter used in this technique known as Shrinkage. Shrinkage refers to the fact that the prediction of each tree in the ensemble is shrunk after it is multiplied by the learning rate (eta) which ranges between 0 to 1. There is a trade-off between eta and the number of estimators, decreasing learning rate needs to be compensated with increasing estimators in order to reach certain model performance. Since all trees are trained now, predictions can be made. Each tree predicts a label and the final prediction is given by the formula,
y(pred) = y1 + (eta *  r1) + (eta * r2) + ....... + (eta * rN)
"
Classical models,Boosting,GBRT Mathematical formulation,"GBRT regressors are additive models whose prediction yi for a given input x is of the following form:
where the hm are estimators called weak learners in the context of boosting. Gradient Tree Boosting uses decision tree regressors of fixed size as weak learners. The constant M corresponds to the n_estimators parameter.
Similar to other boosting algorithms, a GBRT is built in a greedy fashion:
where the newly added tree  is fitted in order to minimize a sum of losses Lm, given the previous ensemble Fm-1:
where l(yi,F(xi)) is defined by the loss parameter.
By default, the initial model F0 is chosen as the constant that minimizes the loss: for a least-squares loss, this is the empirical mean of the target values.
Using a first-order Taylor approximation, the value of l can be approximated as follows:
The quantity [∂l(yi,F(xi))/∂F(xi)] is the derivative of the loss with respect to its second parameter, evaluated at Fm−1(x). It is easy to compute for any given Fm−1(xi) in a closed form since the loss is differentiable. We will denote it by gi.
Removing the constant terms, we have:
This is minimized if h(xi) is fitted to predict a value that is proportional to the negative gradient −gi. Therefore, at each iteration, the estimator hm is fitted to predict the negative gradients of the samples. The gradients are updated at each iteration. This can be considered as some kind of gradient descent in a functional space.
"
Classical models,Boosting,Friedman’s classic GBM algorithm,"We can now define the classic GBM algorithm suggested by Jerome Friedman in 1999. It is a supervised algorithm that has the following components:
dataset {(xi, yi)}
number of iterations M
choice of loss function L(y, f) with a defined gradient;
choice of function family of base algorithms h with the training procedure;
additional hyperparameters h (for example, in decision trees, the tree depth);
The only thing left is the initial approximation f0(x). For simplicity, for an initial approximation, a constant value is used. The constant value, as well as the optimal coefficient, are identified via binary search or another line search algorithm over the initial loss function (not a gradient). So, we have our GBM algorithm described as follows:
Initialize GBM with constant value    
For each iteration t=1...M, repeat:
Calculate pseudo-residuals rt 
Build new base algorithm ht(x) as regression on pseudo-residuals 
Find optimal coefficient  at ht(x) regarding initial loss function  
Save 
Update current approximation 
Compose final GBM model 
"
Classical models,Boosting,Difference between AdaBoost and XGBoost.,"Both methods combine weak learners into one strong learner. For example, one decision tree is a weak learner, and an emsemble of them would be a random forest model, which is a strong learner.
Both methods in the learning process will increase the ensemble of weak-trainers, adding new weak learners to the ensemble at each training iteration, i.e. in the case of the forest, the forest will grow with new trees. The only difference between AdaBoost and XGBoost is how the ensemble is replenished.
AdaBoost works by weighting the observations, putting more weight on difficult to classify instances and less on those already handled well. New weak learners are added sequentially that focus their training on the more difficult patterns. AdaBoost at each iteration changes the sample weights in the sample. It raises the weight of the samples in which more mistakes were made. The sample weights vary in proportion to the ensemble error. We thereby change the probabilistic distribution of samples - those that have more weight will be selected more often in the future. It is as if we had accumulated samples on which more mistakes were made and would use them instead of the original sample. In addition, in AdaBoost, each weak learner has its own weight in the ensemble (alpha weight) - this weight is higher, the “smarter” this weak learner is, i.e. than the learner least likely to make mistakes.
XGBoost does not change the selection or the distribution of observations at all. XGBoost builds the first tree (weak learner), which will fit the observations with some prediction error. A second tree (weak learner) is then added to correct the errors made by the existing model. Errors are minimized using a gradient descent algorithm. Regularization can also be used to penalize more complex models through both Lasso and Ridge regularization.
In short, AdaBoost- reweighting examples. Gradient boosting - predicting the loss function of trees. Xgboost - the regularization term was added to the loss function (depth + values in leaves).
"
Classical models,Dimensionality reduction,What reduction techniques do you know? ,"Singular Value Decomposition (SVD)
Principal Component Analysis (PCA)
Linear Discriminant Analysis (LDA)
T-distributed Stochastic Neighbor Embedding (t-SNE)
Autoencoders
Fourier and Wavelet Transforms
UMAP (Uniform Manifold Approximation and Projection)
"
Classical models,Dimensionality reduction,What’s singular value decomposition? ,"Singular value decomposition assumes a matrix (for example, a matrix) is decomposed as
M = U Σ VT
where U is a m x m matrix, Σ is a diagonal matrix of m x n, and VT is a n x n matrix. The matrices U and VT are orthonormal matrices. Meaning the columns of U or rows of V are (1) orthogonal to each other and are (2) unit vectors. Vectors are orthogonal to each other if any two vectors’ dot product is zero. A vector is unit vector if its L2-norm is 1. Orthonormal matrix has the property that its transpose is its inverse. 
Singular value decomposition gets its name from the diagonal entries on Σ, which are called the singular values of matrix . They are in fact, the square root of the eigenvalues of matrix M MT. Just like a number factorized into primes, the singular value decomposition of a matrix reveals a lot about the structure of that matrix.
"
Classical models,Dimensionality reduction,How is SVD typically used for machine learning? ,"For machine learning, Principal Component Analysis (PCA) is typically used. It is a special type of SVD where the singular values correspond to the eigenvectors and the values of the diagonal matrix are the squares of the eigenvalues. We use these features as they are statistically descriptive.
Having calculated the eigenvectors and eigenvalues, we can use the Kaiser-Guttman criterion, a scree plot or the proportion of explained variance to determine the principal components (i.e. the final dimensionality) that are useful for dimensionality reduction
"
Classical models,Dimensionality reduction,T-distributed Stochastic Neighbor Embedding,"t-SNE is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.
It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples. For more tips see Laurens van der Maaten’s FAQ [2].
"
Classical models,Dimensionality reduction,Linear Discriminant Analysis,"Linear discriminant analysis is used as a tool for classification, dimension reduction, and data visualization. It has been around for quite some time now. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the benchmarking method before other more complicated and flexible ones are employed.
"
Classical models,Dimensionality reduction,Principal Component Analysis (PCA),"Principal Component Analysis is one of the easiest, most intuitive, and most frequently used methods for dimensionality reduction, projecting data onto its orthogonal feature subspace.
More generally speaking, all observations can be considered as an ellipsoid in a subspace of an initial feature space, and the new basis set in this subspace is aligned with the ellipsoid axes. This assumption lets us remove highly correlated features since basis set vectors are orthogonal. In the general case, the resulting ellipsoid dimensionality matches the initial space dimensionality, but the assumption that our data lies in a subspace with a smaller dimension allows us to cut off the “excessive” space with the new projection (subspace). We accomplish this in a ‘greedy’ fashion, sequentially selecting each of the ellipsoid axes by identifying where the variance is maximal.
In order to decrease the dimensionality of our data from n to k with k≤n, we sort our list of axes in order of decreasing variance and take the top-k of them.
We begin by computing the variance and the covariance of the initial features. This is usually done with the covariance matrix. According to the covariance definition, the covariance of two features is computed as follows:
where μi is the expected value of the ith feature. It is worth noting that the covariance is symmetric, and the covariance of a vector with itself is equal to its variance.
Therefore the covariance matrix is symmetric with the variance of the corresponding features on the diagonal. Non-diagonal values are the covariances of the corresponding pair of features. In terms of matrices where X is the matrix of observations, the covariance matrix is as follows:
Quick recap: matrices, as linear operators, have eigenvalues and eigenvectors. They are very convenient because they describe parts of our space that do not rotate and only stretch when we apply linear operators on them; eigenvectors remain in the same direction but are stretched by a corresponding eigenvalue. Formally, a matrix M with eigenvector wi and eigenvalue λi satisfy this equation: Mwi=λiwi.
The covariance matrix for a sample X can be written as a product of XTX. According to the Rayleigh quotient, the maximum variation of our sample lies along the eigenvector of this matrix and is consistent with the maximum eigenvalue. Therefore, the principal components we aim to retain from the data are just the eigenvectors corresponding to the top-k largest eigenvalues of the matrix.
The next steps are easier to digest. We multiply the matrix of our data X by these components to get the projection of our data onto the orthogonal basis of the chosen components. If the number of components was smaller than the initial space dimensionality, remember that we will lose some information upon applying this transformation.
"
Classical models,Dimensionality reduction,UMAP,"UMAP (Uniform Manifold Approximation and Projection) is a dimensionality reduction technique that preserves both local and global structure of high-dimensional data. It works by constructing a high-dimensional graph of data points and optimizing a low-dimensional representation to maintain the structure.
🔹 Key Features:
More efficient and scalable than t-SNE
Preserves global and local structure well
Works well for visualization and clustering
Can handle large datasets efficiently
UMAP is widely used in NLP, CV, and bioinformatics for exploring embeddings and reducing complexity while maintaining meaningful relationships in data.
"
Classical models,Time series,Time Series,"Time series is a sequence of observations of categorical or numeric variables indexed by a date, or timestamp. A clear example of time series data is the time series of a stock price. In the following table, we can see the basic structure of time series data. In this case the observations are recorded every hour.
Normally, the first step in time series analysis is to plot the series, this is normally done with a line chart.
The most common application of time series analysis is forecasting future values of a numeric value using the temporal structure of the data. This means, the available observations are used to predict values from the future.
The temporal ordering of the data, implies that traditional regression methods are not useful. In order to build robust forecast, we need models that take into account the temporal ordering of the data.
The most widely used model for Time Series Analysis is called Autoregressive Moving Average (ARMA). The model consists of two parts, an autoregressive (AR) part and a moving average (MA) part. The model is usually then referred to as the ARMA(p, q) model where p is the order of the autoregressive part and q is the order of the moving average part.
"
Classical models,Time series,How are the time series problems different from other regression problems?,"Time series data can be thought of as an extension to linear regression which uses terms like autocorrelation, movement of averages for summarizing historical data of y-axis variables for predicting a better future.
Forecasting and prediction is the main goal of time series problems where accurate predictions can be made but sometimes the underlying reasons might not be known.
Having Time in the problem does not necessarily mean it becomes a time series problem. There should be a relationship between target and time for a problem to become a time series problem.
The observations close to one another in time are expected to be similar to the ones far away which provide accountability for seasonality. For instance, today’s weather would be similar to tomorrow’s weather but not similar to weather from 4 months from today. Hence, weather prediction based on past data becomes a time series problem.
Instead of adding fully connected layers on top of the feature maps, it takes the average of each feature map, and the resulting vector is fed directly into the softmax layer. One advantage of global average pooling over the fully connected layers is that it is more native to the convolution structure by enforcing correspondences between feature maps and categories.
Another advantage is that there is no parameter to optimize in the global average pooling thus overfitting is avoided at this layer. Global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input. We can see global average pooling as a structural regularizer that explicitly enforces feature maps to be confidence maps of concepts (categories).
"
Classical models,Time series,Which models do you know for solving time series problems? ,"Simple Exponential Smoothing: approximate the time series with an exponential function
Trend-Corrected Exponential Smoothing (Holt‘s Method): exponential smoothing that also models the trend
Trend- and Seasonality-Corrected Exponential Smoothing (Holt-Winter‘s Method): exponential smoothing that also models trend and seasonality
Time Series Decomposition: decomposed a time series into the four components trend, seasonal variation, cycling variation and irregular component
Autoregressive models: similar to multiple linear regression, except that the dependent variable y_t depends on its own previous values rather than other independent variables.
Deep learning approaches (RNN, LSTM, etc.)
"
Classical models,Time series,Components of Time Series Analysis,"Trend
Seasonality
Cyclical
Irregularity
Trend: In which there is no fixed interval and any divergence within the given dataset is a continuous timeline. The trend would be Negative or Positive or Null Trend
Seasonality: In which regular or fixed interval shifts within the dataset in a continuous timeline. Would be bell curve or saw tooth
Cyclical: In which there is no fixed interval, uncertainty in movement and its pattern
Irregularity: Unexpected situations/events/scenarios and spikes in a short time span.
"
Classical models,Time series,What are the limitations of Time Series Analysis?,"Time series has the below-mentioned limitations, we have to take care of those during our analysis,
Similar to other models, the missing values are not supported by TSA
The data points must be linear in their relationship.
Data transformations are mandatory, so a little expensive.
Models mostly work on Uni-variate data.
"
Classical models,Time series,Data Types of Time Series,"Let’s discuss the time series’ data types and their influence. While discussing TS data-types, there are two major types.
Stationary
Non- Stationary
Stationary: A dataset should follow the below thumb rules, without having Trend, Seasonality, Cyclical, and Irregularity component of time series
The MEAN value of them should be completely constant in the data during the analysis
The VARIANCE should be constant with respect to the time-frame
The COVARIANCE measures the relationship between two variables.
Non- Stationary: This is just the opposite of Stationary.
"
Classical models,Time series,Methods to check Stationarity ,"During the TSA model preparation workflow, we must access if the given dataset is Stationary or NOT. Using Statistical and Plots test.
Statistical Test: There are two tests available to test if the dataset is Stationary or NOT.
Augmented Dickey-Fuller (ADF) Test
Kwiatkowski-Phillips-Schmidt-Shin (KPSS) Test
Augmented Dickey-Fuller (ADF) Test or Unit Root Test: The ADF test is the most popular statistical test and with the following assumptions.
Null Hypothesis (H0): Series is non-stationary
Alternate Hypothesis (HA): Series is stationary
p-value >0.05 Fail to reject (H0)
p-value <= 0.05 Accept (H1)
Kwiatkowski–Phillips–Schmidt–Shin (KPSS): these tests are used for testing a NULL Hypothesis (HO), that will perceive the time-series, as stationary around a deterministic trend against the alternative of a unit root. Since TSA looking for Stationary Data for its further analysis, we have to make sure that the dataset should be stationary.
"
Classical models,Time series,Converting Non-stationary into stationary,"Let’s discuss quickly how to convert Non- stationary into stationary for effective time series modeling. There are two major methods available for this conversion.
Detrending
Differencing
Detrending: It involves removing the trend effects from the given dataset and showing only the differences in values from the trend. it always allows the cyclical patterns to be identified.
Differencing: This is a simple transformation of the series into a new time series, which we use to remove the series dependence on time and stabilize the mean of the time series, so trend and seasonality are reduced during this transformation.
Yt= Yt – Yt-1
Yt=Value with time
Transformation: This includes three different methods they are Power Transform, Square Root, and Log Transfer., most commonly used one is Log Transfer.
"
Classical models,Time series,Moving Average Methodology,"The commonly used time series method is Moving Average. This method is slick with random short-term variations. Relatively associated with the components of time series.
The Moving Average (MA) (Or) Rolling Mean: In which MA has calculated by taking averaging data of the time-series, within k periods.
Let’s see the types of moving averages:
Simple Moving Average (SMA),
Cumulative Moving Average (CMA)
Exponential Moving Average (EMA)
Simple Moving Average (SMA)
The SMA is the unweighted mean of the previous M or N points. The selection of sliding window data points, depending on the amount of smoothing is preferred since increasing the value of M or N, improves the smoothing at the expense of accuracy.
Cumulative Moving Average (CMA)
The CMA is the unweighted mean of past values, till the current time.
Exponential Moving Average (EMA)
EMA is mainly used to identify trends and to filter out noise. The weight of elements is decreased gradually over time. This means It gives weight to recent data points, not historical ones. Compared with SMA, the EMA is faster to change and more sensitive.
α –>Smoothing Factor.
It has a value between 0,1.
Represents the weighting applied to the very recent period. 
Rather than using past values of the forecast variable in a regression, a moving average model uses past forecast errors in a regression-like model.
yt=c+εt+θ1εt−1+θ2εt−2+⋯+θqεt−q,
where εtεt is white noise. We refer to this as an MA(qq) model, a moving average model of order qq. Of course, we do not observe the values of εtεt, so it is not really a regression in the usual sense.
"
Classical models,Time series,Time Series Analysis in Data Science and Machine Learning,"When dealing with TSA in Data Science and Machine Learning, there are multiple model options are available. In which the Autoregressive–Moving-Average (ARMA) models with [p, d, and q].
P==> autoregressive lags
q== moving average lags
d==> difference in the order
Before we get to know about Arima, first you should understand the below terms better.
Auto-Correlation Function (ACF)
Partial Auto-Correlation Function (PACF)
Auto-Correlation Function (ACF): ACF is used to indicate and how similar a value is within a given time series and the previous value. (OR) It measures the degree of the similarity between a given time series and the lagged version of that time series at different intervals that we observed.
Python Statsmodels library calculates autocorrelation. This is used to identify a set of trends in the given dataset and the influence of former observed values on the currently observed values.
Partial Auto-Correlation (PACF): PACF is similar to Auto-Correlation Function and is a little challenging to understand. It always shows the correlation of the sequence with itself with some number of time units per sequence order in which only the direct effect has been shown, and all other intermediary effects are removed from the given time series.
A partial autocorrelation is the amount of correlation between a variable and a lag of itself that is not explained by correlations at all lower-order-lags. The autocorrelation of a time series Y at lag 1 is the coefficient of correlation between Yt and Yt-1, which is presumably also the correlation between Yt-1 and Yt-2. But if Yt is correlated with Yt-1, and Yt-1 is equally correlated with Yt-2, then we should also expect to find correlation between Yt and Yt-2. In fact, the amount of correlation we should expect at lag 2 is precisely the square of the lag-1 correlation. Thus, the correlation at lag 1 ""propagates"" to lag 2 and presumably to higher-order lags. The partial autocorrelation at lag 2 is therefore the difference between the actual correlation at lag 2 and the expected correlation due to the propagation of correlation at lag 1.

The autocorrelations are significant for a large number of lags--but perhaps the autocorrelations at lags 2 and above are merely due to the propagation of the autocorrelation at lag 1. This is confirmed by the PACF plot:
Interpret ACF and PACF plots
Remember that both ACF and PACF require stationary time series for analysis.
"
Classical models,Time series,Understanding ARMA and ARIMA ,"ARMA This is a combination of the Auto-Regressive and Moving Average model for forecasting. This model provides a weakly stationary stochastic process in terms of two polynomials, one for the Auto-Regressive and the second for the Moving Average.
ARMA is best for predicting stationary series. So ARIMA came in since it supports stationary as well as non-stationary.
AR ==> Uses the past values to predict the future
MA ==> Uses the past error terms in the given series to predict the future
I==> uses the differencing of observation and makes the stationary data
AR+I+MA= ARIMA
Understand the Signature of ARIMA
p==> log order => No of lag observations.
d==> degree of differencing => No of times that the raw observations are differenced.
q==>order of moving average => the size of the moving average window
Implementation steps for ARIMA
Step 1: Plot a time series format
Step 2: Difference to make stationary on mean by removing the trend
Step 3: Make stationary by applying log transform.
Step 4: Difference log transform to make as stationary on both statistic mean and variance
Step 5: Plot ACF & PACF, and identify the potential AR and MA model
Step 6: Discovery of best fit ARIMA model
Step 7: Forecast/Predict the value, using the best fit ARIMA model
Step 8: Plot ACF & PACF for residuals of the ARIMA model, and ensure no more information is left.
"
Classical models,Time series,What are the problems with using trees for solving time series problems? ,"Random Forest models are not able to extrapolate time series data and understand increasing/decreasing trends. It will provide us with average data points if the validation data has values greater than the training data points.
"
Classical models,Clustering,Clustering,"The main idea behind clustering is pretty straightforward. Basically, we say to ourselves, “I have these points here, and I can see that they organize into groups. It would be nice to describe these things more concretely, and, when a new point comes in, assign it to the correct group.” This general idea encourages exploration and opens up a variety of algorithms for clustering.
"
Classical models,Clustering,What clustering algorithms do you know? ,"k-medoids: Takes the most central point instead of the mean value as the center of the cluster. This makes it more robust to noise.
Agglomerative Hierarchical Clustering (AHC): hierarchical clusters combining the nearest clusters starting with each point as its own cluster.
DIvisive ANAlysis Clustering (DIANA): hierarchical clustering starting with one cluster containing all points and splitting the clusters until each point describes its own cluster.
Density-Based Spatial Clustering of Applications with Noise (DBSCAN): Cluster defined as maximum set of density-connected points.
"
Classical models,Clustering,K-means,"K-means algorithm is the most popular and yet simplest of all the clustering algorithms. Here is how it works:
Select the number of clusters k that you think is the optimal number.
Initialize k points as “centroids” randomly within the space of our data.
Attribute each observation to its closest centroid.
Update the centroids to the center of all the attributed set of observations.
Repeat steps 3 and 4 a fixed number of times or until all of the centroids are stable (i.e. no longer change in step 4).
Inherently, K-means is NP-hard. For d dimensions, k clusters, and n observations, we will find a solution in O(n^dk+1) time. There are some heuristics to deal with this; an example is MiniBatch K-means, which takes portions (batches) of data instead of fitting the whole dataset and then moves centroids by taking the average of the previous steps. Compare the implementation of K-means and MiniBatch K-means in the scikit-learn documentation.
The implementation of the algorithm using scikit-learn has its benefits such as the possibility to state the number of initializations with the n_init function parameter, which enables us to identify more robust centroids. Moreover, these runs can be done in parallel to decrease the computation time.
"
Classical models,Clustering,How can you select k for k-means? ,"We use the elbow method to select k for k-means clustering. The idea of the elbow method is to run k-means clustering on the data set where 'k' is the number of clusters.
Within the sum of squares (WSS), it is defined as the sum of the squared distance between each member of the cluster and its centroid. 
Siloutte score.
For each point compute the score. 
Score = (b - a)/ max(a,b)
a = intra cluster distance 
b = inter cluster distance for nearest cluster for that point. 
Do this for all points and average. 
Pick the one with max. average siloutte score
In contrast to the supervised learning tasks such as classification and regression, clustering requires more effort to choose the optimization criterion. Usually, when working with k-means, we optimize the sum of squared distances between the observations and their centroids.
This definition seems reasonable – we want our observations to be as close to their centroids as possible. But, there is a problem – the optimum is reached when the number of centroids is equal to the number of observations, so you would end up with every single observation as its own separate cluster.
In order to avoid that case, we should choose a number of clusters after which a function J(Ck) is decreasing less rapidly. More formally,
"
Classical models,Clustering,How does DBScan work?,"Two input parameters epsilon (neighborhood radius) and minPts (minimum number of points in an epsilon-neighborhood)
Cluster defined as maximum set of density-connected points.
Points p_j and p_i are density-connected w.r.t. epsilon and minPts if there is a point o such that both, i and j are density-reachable from o w.r.t. epsilon and minPts.
p_j is density-reachable from p_i w.r.t. epsilon, minPts if there is a chain of points p_i -> p_i+1 -> p_i+x = p_j such that p_i+x is directly density-reachable from p_i+x-1.
p_j is a directly density-reachable point of the neighborhood of p_i if dist(p_i,p_j) <= epsilon.
"
Classical models,Clustering,When would you choose K-means and when DBScan? ,"DBScan is more robust to noise.
DBScan is better when the amount of clusters is difficult to guess.
K-means has a lower complexity, i.e. it will be much faster, especially with a larger amount of points.
"
Classical models,Clustering,Mean-Shift Clustering,"Meanshift is falling under the category of a clustering algorithm in contrast of Unsupervised learning that assigns the data points to the clusters iteratively by shifting points towards the mode (mode is the highest density of data points in the region, in the context of the Meanshift). As such, it is also known as the Mode-seeking algorithm. Mean-shift algorithm has applications in the field of image processing and computer vision.
Given a set of data points, the algorithm iteratively assigns each data point towards the closest cluster centroid and direction to the closest cluster centroid is determined by where most of the points nearby are at. So each iteration each data point will move closer to where the most points are at, which is or will lead to the cluster center. When the algorithm stops, each point is assigned to a cluster.
Unlike the popular K-Means cluster algorithm, mean-shift does not require specifying the number of clusters in advance. The number of clusters is determined by the algorithm with respect to the data. 
Note: The downside to Mean Shift is that it is computationally expensive O(n²).
Mean-shift clustering is a non-parametric, density-based clustering algorithm that can be used to identify clusters in a dataset. It is particularly useful for datasets where the clusters have arbitrary shapes and are not well-separated by linear boundaries.
The basic idea behind mean-shift clustering is to shift each data point towards the mode (i.e., the highest density) of the distribution of points within a certain radius. The algorithm iteratively performs these shifts until the points converge to a local maximum of the density function. These local maxima represent the clusters in the data.
The process of mean-shift clustering algorithm can be summarized as follows:
Initialize the data points as cluster centroids.
Repeat the following steps until convergence or a maximum number of iterations is reached:
For each data point, calculate the mean of all points within a certain radius (i.e., the “kernel”) centered at the data point.
Shift the data point to the mean.
Identify the cluster centroids as the points that have not moved after convergence.
Return the final cluster centroids and the assignments of data points to clusters.
One of the main advantages of mean-shift clustering is that it does not require the number of clusters to be specified beforehand. It also does not make any assumptions about the distribution of the data, and can handle arbitrary shapes and sizes of clusters. However, it can be sensitive to the choice of kernel and the radius of the kernel.
Mean-Shift clustering can be applied to various types of data, including image and video processing, object tracking and bioinformatics.
"
Classical models,Clustering,Affinity Propagation,"Affinity propagation is another example of a clustering algorithm. As opposed to K-means, this approach does not require us to set the number of clusters beforehand. The main idea here is that we would like to cluster our data based on the similarity of the observations (or how they “correspond” to each other).
Let’s define a similarity metric such that s(xi,xj)>s(xi,xk) if an observation xi is more similar to observation xj and less similar to observation xk. A simple example of such a similarity metric is a negative square of distance s(xi,xj)=−||xi−xj||2.
Now, let’s describe “correspondence” by making two zero matrices. One of them, ri,k, determines how well the kth observation is as a “role model” for the ith observation with respect to all other possible “role models”. Another matrix, ai,k determines how appropriate it would be for ith observation to take the kth observation as a “role model”. This may sound confusing, but it becomes more understandable with some hands-on practice.
The matrices are updated sequentially with the following rules:
"
Classical models,Clustering,Spectral clustering,"Spectral clustering combines some of the approaches described above to create a stronger clustering method.
First of all, this algorithm requires us to define the similarity matrix for observations called the adjacency matrix. This can be done in a similar fashion as in the Affinity Propagation algorithm: Ai,j=−||xi−xj||2. This matrix describes a full graph with the observations as vertices and the estimated similarity value between a pair of observations as edge weights for that pair of vertices. For the metric defined above and two-dimensional observations, this is pretty intuitive - two observations are similar if the edge between them is shorter. We’d like to split up the graph into two subgraphs in such a way that each observation in each subgraph would be similar to another observation in that subgraph. Formally, this is a Normalized cuts problem.
"
Classical models,Clustering,Agglomerative clustering,"The following algorithm is the simplest and easiest to understand among all the the clustering algorithms without a fixed number of clusters.
The algorithm is fairly simple:
We start by assigning each observation to its own cluster
Then sort the pairwise distances between the centers of clusters in descending order
Take the nearest two neigbor clusters and merge them together, and recompute the centers
Repeat steps 2 and 3 until all the data is merged into one cluster
The process of searching for the nearest cluster can be conducted with different methods of bounding the observations:
Single linkage d(Ci,Cj)=min xi∈Ci,xj∈Cj||xi−xj||
Complete linkage d(Ci,Cj)=max xi∈Ci,xj∈Cj||xi−xj||
Average linkage d(Ci,Cj)=1ninj∑xi∈Ci∑xj∈Cj||xi−xj||
Centroid linkage d(Ci,Cj)=||μi−μj||
The 3rd one is the most effective in computation time since it does not require recomputing the distances every time the clusters are merged.
The results can be visualized as a beautiful cluster tree (dendrogram) to help recognize the moment the algorithm should be stopped to get optimal results. There are plenty of Python tools to build these dendrograms for agglomerative clustering.
"
Classical models,Clustering,What’s the difference between Gaussian Mixture Model and K-Means?,"Let's says we are aiming to break them into three clusters. K-means will start with the assumption that a given data point belongs to one cluster.
Choose a data point. At a given point in the algorithm, we are certain that a point belongs to a red cluster. In the next iteration, we might revise that belief, and be certain that it belongs to the green cluster. However, remember, in each iteration, we are absolutely certain as to which cluster the point belongs to. This is the ""hard assignment"".
What if we are uncertain? What if we think, well, I can't be sure, but there is 70% chance it belongs to the red cluster, but also 10% chance its in green, 20% chance it might be blue. That's a soft assignment. The Mixture of Gaussian model helps us to express this uncertainty. It starts with some prior belief about how certain we are about each point's cluster assignments. As it goes on, it revises those beliefs. But it incorporates the degree of uncertainty we have about our assignment.
Kmeans: find kk to minimize (x−μk)^2
Gaussian Mixture (EM clustering) : find kk to minimize (x−μk)^2/σ^2
The difference (mathematically) is the denominator “σ^2”, which means GM takes variance into consideration when it calculates the measurement. Kmeans only calculates conventional Euclidean distance. In other words, Kmeans calculate distance, while GM calculates “weighted” distance.
K means:
Hard assign a data point to one particular cluster on convergence.
It makes use of the L2 norm when optimizing (Min {Theta} L2 norm point and its centroid coordinates).
EM:
Soft assigns a point to clusters (so it give a probability of any point belonging to any centroid).
It doesn't depend on the L2 norm, but is based on the Expectation, i.e., the probability of the point belonging to a particular cluster. This makes K-means biased towards spherical clusters.
"
Classical models,Ranking,Ranking,"In this post, by “ranking” we mean sorting documents by relevance to find contents of interest with respect to a query. This is a fundamental problem of Information Retrieval, but this task also arises in many other applications:
Search Engines — Given a user profile (location, age, sex, …) a textual query, sort web pages results by relevance.
Recommender Systems — Given a user profile and purchase history, sort the other items to find new potentially interesting products for the user.
Ranking models typically work by predicting a relevance score s = f(x) for each input x = (q, d) where q is a query and d is a document. Once we have the relevance of each document, we can sort (i.e. rank) the documents according to those scores.
The scoring model can be implemented using various approaches.
Vector Space Models – Compute a vector embedding (e.g. using Tf-Idf or BERT) for each query and document, and then compute the relevance score f(x) = f(q, d) as the cosine similarity between the vectors embeddings of q and d.
Learning to Rank – The scoring model is a Machine Learning model that learns to predict a score s given an input x = (q, d) during a training phase where some sort of ranking loss is minimized.
In this article we focus on the latter approach, and we show how to implement Machine Learning models for Learning to Rank.
"
Classical models,Ranking,Ranking Evaluation Metrics,"Before analyzing various ML models for Learning to Rank, we need to define which metrics are used to evaluate ranking models. These metrics are computed on the predicted documents ranking, i.e. the k-th top retrieved document is the k-th document with highest predicted score s.
All Learning to Rank models use a base Machine Learning model (e.g. Decision Tree or Neural Network) to compute s = f(x). The choice of the loss function is the distinctive element for Learning to Rank models. In general, we have 3 approaches, depending on how the loss is computed.
Pointwise Methods – The total loss is computed as the sum of loss terms defined on each document dᵢ (hence pointwise) as the distance between the predicted score sᵢ and the ground truth yᵢ, for i=1…n. By doing this, we transform our task into a regression problem, where we train a model to predict y.
Pairwise Methods – The total loss is computed as the sum of loss terms defined on each pair of documents dᵢ, dⱼ (hence pairwise), for i, j=1…n. The objective on which the model is trained is to predict whether yᵢ > yⱼ or not, i.e. which of two documents is more relevant. By doing this, we transform our task into a binary classification problem.
Listwise Methods – The loss is directly computed on the whole list of documents (hence listwise) with corresponding predicted ranks. In this way, ranking metrics can be more directly incorporated into the loss.
"
Classical models,Recommender Systems,Recommender System,"In a typical recommender system people provide recommendations as inputs, which the system then aggregates and directs to appropriate recipients. In some cases the primary transformation is in the aggregation; in others the system's value lies in its ability to make good matches between the recommenders and those seeking recommendations.
More formally, the recommendation problem can be formulated as follows: Let C be the set of all users and let S be the set of all possible items that can be recommended. Let u be a utility function that measures the usefulness of item s to user c, that is, u: C x S ⇒ R, where R is a totally ordered set (for example, nonnegative integers or real numbers within a certain range). Then, for each user c ∈ C, we want to choose such item s' ∈ S that maximizes the user's utility.
"
Classical models,Recommender Systems,Recommender Systems Approaches,"Like many machine learning techniques, a recommender system makes prediction based on users’ historical behaviors. Specifically, it’s to predict user preference for a set of items based on past experience. To build a recommender system, the most two popular approaches are Content-based and Collaborative Filtering.
Content-based approach requires a good amount of information of items’ own features, rather than using users’ interactions and feedbacks. For example, it can be movie attributes such as genre, year, director, actor etc., or textual content of articles that can extracted by applying Natural Language Processing.
Unlike collaborative methods that only rely on the user-item interactions, content based approaches use additional information about users and/or items. If we consider the example of a movies recommender system, this additional information can be, for example, the age, the sex, the job or any other personal information for users as well as the category, the main actors, the duration or other characteristics for the movies (items).
Collaborative Filtering, on the other hand, doesn’t need anything else except users’ historical preference on a set of items. Because it’s based on historical data, the core assumption here is that the users who have agreed in the past tend to also agree in the future. In terms of user preference, it usually expressed by two categories. Explicit Rating, is a rate given by a user to an item on a sliding scale, like 5 stars for Titanic. This is the most direct feedback from users to show how much they like an item. Implicit Rating, suggests users preference indirectly, such as page views, clicks, purchase records, whether or not listen to a music track, and so on. In this article, I will take a close look at collaborative filtering that is a traditional and powerful tool for recommender systems.
"
Classical models,Recommender Systems,Content Based Recommender System,"A Content-Based Recommender works by the data that we take from the user, either explicitly (rating) or implicitly (clicking on a link). By the data we create a user profile, which is then used to suggest to the user, as the user provides more input or take more actions on the recommendation, the engine becomes more accurate.
User Profile:
In the User Profile, we create vectors that describe the user’s preference. In the creation of a user profile, we use the utility matrix which describes the relationship between user and item. With this information, the best estimate we can make regarding which item user likes, is some aggregation of the profiles of those items.
Item Profile:
In Content-Based Recommender, we must build a profile for each item, which will represent the important characteristics of that item.
For example, if we make a movie as an item then its actors, director, release year and genre are the most significant features of the movie. We can also add its rating from the IMDB (Internet Movie Database) in the Item Profile.
Utility Matrix:
Utility Matrix signifies the user’s preference with certain items. In the data gathered from the user, we have to find some relation between the items which are liked by the user and those which are disliked, for this purpose we use the utility matrix. In it we assign a particular value to each user-item pair, this value is known as the degree of preference. Then we draw a matrix of a user with the respective items to identify their preference relationship.
Some of the columns are blank in the matrix that is because we don’t get the whole input from the user every time, and the goal of a recommendation system is not to fill all the columns but to recommend a movie to the user which he/she will prefer. Through this table, our recommender system won’t suggest Movie 3 to User 2, because in Movie 1 they have given approximately the same ratings, and in Movie 3 User 1 has given the low rating, so it is highly possible that User 2 also won’t like it.
Recommending Items to User Based on Content:
Method 1:
We can use the cosine distance between the vectors of the item and the user to determine its preference to the user. For explaining this, let us consider an example:
We observe that the vector for a user will have a positive number for actors that tend to appear in movies the user likes and negative numbers for actors user doesn’t like, Consider a movie with actors which user likes and only a few actors which user doesn’t like, then the cosine angle between the user’s and movie’s vectors will be a large positive fraction. Thus, the angle will be close to 0, therefore a small cosine distance between the vectors.
It represents that the user tends to like the movie, if the cosine distance is large, then we tend to avoid the item from the recommendation.
Method 2:
We can use a classification approach in the recommendation systems too, like we can use the Decision Tree for finding out whether a user wants to watch a movie or not, like at each level we can apply a certain condition to refine our recommendation. 
"
Classical models,Recommender Systems,The Meaning of Singular Value Decomposition in Recommender System,"If the matrix  M has rank r, then we can prove that the matrices MMT and MTM are both rank r. In singular value decomposition (the reduced SVD), the columns of matrix U are eigenvectors of MMT and the rows of matrix VT are eigenvectors of MTM. What’s interesting is that they are potentially in different size (because matrix can be non-square shape), but they have the same set of eigenvalues, which are the square of values on the diagonal of ∑.
This is why the result of singular value decomposition can reveal a lot about the matrix.
Imagine we collected some book reviews such that books are columns and people are rows, and the entries are the ratings that a person gave to a book. In that case, MMT would be a table of person-to-person which the entries would mean the sum of the ratings one person gave match with another one. Similarly MTM would be a table of book-to-book which entries are the sum of the ratings received match with that received by another book. What can be the hidden connection between people and books? That could be the genre, or the author, or something of similar nature.
"
Classical models,Recommender Systems,Matrix Factorization in Collaborative Filtering,"What is matrix factorization? Matrix factorization is simply a family of mathematical operations for matrices in linear algebra. To be specific, a matrix factorization is a factorization of a matrix into a product of matrices. In the case of collaborative filtering, matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices. One matrix can be seen as the user matrix where rows represent users and columns are latent factors. The other matrix is the item matrix where rows are latent factors and columns represent items.
How does matrix factorization solve our problems?
Model learns to factorize rating matrix into user and movie representations, which allows model to predict better personalized movie ratings for users
With matrix factorization, less-known movies can have rich latent representations as much as popular movies have, which improves recommender’s ability to recommend less-known movies
In the sparse user-item interaction matrix, the predicted rating user u will give item i is computed as:
where H is user matrix, W is item matrix
Rating of item i given by user u can be expressed as a dot product of the user latent vector and the item latent vector.
Notice in above formula, the number of latent factors can be tuned via cross-validation. Latent factors are the features in the lower dimension latent space projected from user-item interaction matrix. The idea behind matrix factorization is to use latent factors to represent user preferences or movie topics in a much lower dimension space. Matrix factorization is one of very effective dimension reduction techniques in machine learning.
Very much like the concept of components in PCA, the number of latent factors determines the amount of abstract information that we want to store in a lower dimension space. A matrix factorization with one latent factor is equivalent to a most popular or top popular recommender (e.g. recommends the items with the most interactions without any personalization). Increasing the number of latent factors will improve personalization, until the number of factors becomes too high, at which point the model starts to overfit. A common strategy to avoid overfitting is to add regularization terms to the objective function.
The objective of matrix factorization is to minimize the error between true rating and predicted rating:
where H is user matrix, W is item matrix.
"
Classical models,Recommender Systems,Alternating Least Square (ALS),"Alternating Least Square (ALS) is a matrix factorization algorithm and it runs itself in a parallel fashion. ALS is implemented in Apache Spark ML and built for a larges-scale collaborative filtering problems. ALS is doing a pretty good job at solving scalability and sparseness of the Ratings data, and it’s simple and scales well to very large datasets.
"
Classical models,Recommender Systems,Two-Stage Recommender Systems,"In large scale recommender systems pipelines, the size of the item catalog (number of unique items) might be in the order of millions. At such scale, a typical setup is having two-stage pipeline, where a faster candidate retrieval model quickly extracts thousands of relevant items and a then a more powerful ranking model (i.e. with more features and more powerful architecture) ranks the top-k items that are going to be displayed to the user. For ML-based candidate retrieval model, as it needs to quickly score millions of items for a given user, a popular choices are models that can produce recommendation scores by just computing the dot product the user embeddings and item embeddings. Popular choices of such models are Matrix Factorization, which learns low-rank user and item embeddings, and the Two-Tower architecture, which is a neural network with two MLP towers where both user and item features are fed to generate user and item embeddings in the output.
"
Classical models,Recommender Systems,Biases in Recommender Systems,"clickbait bias,
duration bias,
Longer videos always have a tendency to be watched for a longer time,
position bias,
Position bias simply means that users engage with the first thing they see, whether it is particularly relevant to them or not
popularity bias
Popularity bias refers to the tendency of the model to favor items that are more popular overall (due to the fact that they’ve been rated by more users), rather than being based on their actual quality or relevance for a particular user. This can lead to a distorted ranking, where less popular or niche items that could be a better fit for the user’s preferences are not given adequate consideration.
single-interest bias.
Suppose you watch mostly drama movies, but sometimes you like to watch a comedy, and from time to time a documentary. You have multiple interests, yet a ranking model trained with the single objective of maximizing watch time would over-emphasize drama movies because that’s what you’re most likely to engage with. This is single-interest bias, the failure of a model to understand that users inherently have multiple interests and preferences.
"
Classical models,Recommender Systems,Evaluation of Recommender Systems,"Because of the difficulties of running large-scale user studies, recommender systems have conventionally been evaluated on one or both of the following measures:
Prediction accuracy. How well do the system's predicted ratings compare with those that are known, but withheld?
Precision of recommendation lists. Given a short list of recommendations produced by the system (typically all a user would have patience to examine), how many of the entries match known “liked” items?
Both of these conventional measures are deficient in some key respects and many of the new areas of exploration in recommender systems have led to experimentation with new evaluation metrics to supplement these common ones.
One of the most significant problems occurs because of the long-tailed nature of the ratings distribution in many data sets. A recommendation technique that optimizes for high accuracy over the entire data set therefore contains an implicit bias toward well-known items, and therefore may fail to capture aspects of utility related to novelty. An accurate prediction on an item that the user already knows is inherently less useful than a prediction on an obscure item. To address this issue, some researchers are looking at the balance between accuracy and diversity in a set of recommendations and are working on algorithms that are sensitive to item distributions.
Another problem with conventional recommender systems evaluation is that it is essentially static. A fixed database of ratings is divided into training and test sets and used to demonstrate the effectiveness of an algorithm. However, the user experience of recommendation is quite different. In an application like movie recommendation, the field of items is always expanding; a user's tastes are evolving; new users are coming to the system. Some recommendation applications require that we take the dynamic nature of the recommendation environment into account and evaluate our algorithms accordingly.
Another area of evaluation that is relatively underexamined is the interaction between the utility functions of the store owner and the user, which necessarily look quite different. Owners implement recommender systems in order to achieve business goals, typically increased profit. The owner therefore may prefer an imperfect match with a high profit margin to a perfect match with limited profit. On the other hand, a user who is presented with low utility recommendations may cease to trust the recommendation function or the entire site. Owners with high volume sites can field algorithms side by side in randomized trials and observe sales and profit differentials, but such results rarely filter out into the research community.
"
Classical models,Recommender Systems,Name and define techniques used to find similarities in the recommendation system. ,"Pearson correlation and Cosine correlation are techniques used to find similarities in recommendation systems. 
Cross-Validation Through Time 
Random Split randomly splits per-user interactions into train and test folds. The main disadvantage of these methods is that they can hardly be reproduced unless the authors release the used data splits.
User Split randomly split users, rather than their interactions, into train and test groups. In this case, particular users and all their interactions are reserved for training, while a different user set and all their interactions are applied for testing.
Leave One Out Split selects one last final transaction per user for testing while keeping all remaining interactions for training. In the case of next-item recommendations, the last interaction corresponds to the last user-item pair per user. In the case of next-basket recommendations, the last interaction is defined as a basket
Temporal User Split splits per-user interactions into train and test sets based on interaction timestamps (e.g., the last 20% of interactions are used for testing). While this scenario is actively used in the sequential RecSys domain, it could lead to a data leakage.
Temporal Global Split splits all available interactions into train and test folds based on a fixed timestamp. Interactions before it are used for training, and those that come after are reserved for testing. Compared to Leave One Out Split or Temporal User Split, this method could sample fewer interactions for testing since having the same amount of users or items in train and test sets is not guaranteed. Nevertheless, according to recent studies, this is the only strategy that prevents data leakage.
"
Classical models,Hyperparameter optimization,Grid Search,"The simplest algorithms that you can use for hyperparameter optimization is a Grid Search. The idea is simple and straightforward. You just need to define a set of parameter values, train model for all possible parameter combinations and select the best one. This method is a good choice only when model can train quickly, which is not the case for typical neural networks.
"
Classical models,Hyperparameter optimization,Random Search,"The idea is similar to Grid Search, but instead of trying all possible combinations we will just use randomly selected subset of the parameters. Instead of trying to check 100,000 samples we can check only 1,000 of parameters. Now it should take a week to run hyperparameter optimization instead of 2 years.
"
Classical models,Hyperparameter optimization,Bayesian Optimization,"Bayesian optimization is a sequential search framework that incorporates both exploration and exploitation and can be considerably more efficient than either grid search or random search. The goal is to build a probabilistic model of the underlying function that will know both (i) that x1 is a good place to sample because the function will probably return a high value here and (ii) that x2 is a good place to sample because the uncertainty here is very large. 
A Bayesian optimization algorithm has two main components:
A probabilistic model of the function: Bayesian optimization starts with an initial probability distribution (the prior) over the function f[∙] to be optimized. Usually this just reflects the fact that we are extremely uncertain about what the function is. With each observation of the function (xt,f[xt]), we learn more and the distribution over possible functions (now called the posterior) becomes narrower.
An acquisition function: This is computed from the posterior distribution over the function and is defined on the same domain. The acquisition indicates the desirability of sampling each point next and depending on how it is defined it can favor exploration or exploitation.
"
Classical models,Hyperparameter optimization,Acquisition functions,"The acquisition function takes the mean and variance at each point x on the function and computes a value that indicates how desirable it is to sample next at this position. A good acquisition function should trade off exploration and exploitation.
Popular acquisition functions: 
the upper confidence bound 
expected improvement 
probability of improvement
Thompson sampling
"
Classical models,Hyperparameter optimization,Tree-structured Parzen Estimators (TPE),"Tree-structured Parzen Estimators (TPE) fixes disadvantages of the Gaussian Process. Each iteration TPE collects new observation and at the end of the iteration, the algorithm decides which set of parameters it should try next. The main idea is similar, but an algorithm is completely different
At the very beginning, we need to define a prior distribution for out hyperparameters. By default, they can be all uniformly distributed, but it’s possible to associate any hyperparameter with some random unimodal distribution.
For the first few iterations, we need to warm up TPE algorithm. It means that we need to collect some data at first before we can apply TPE. The best and simplest way to do it is just to perform a few iterations of Random Search. A number of iterations for Random Search is a parameter defined by the user for the TPE algorithm.
When we collected some data we can finally apply TPE. The next step is to divide collected observations into two groups. The first group contains observations that gave best scores after evaluation and the second one - all other observations. And the goal is to find a set of parameters that more likely to be in the first group and less likely to be in the second group. The fraction of the best observations is defined by the user as a parameter for the TPE algorithm. Typically, it’s 10-25% of observations.
The next part of the TPE is to model likelihood probability for each of the two groups. This is the next big difference between Gaussian Process and TPE. For Gaussian Process we’ve modeled posterior probability instead of likelihood probability. Using the likelihood probability from the first group (the one that contains best observations) we sample the bunch of candidates. From the sampled candidates we try to find a candidate that more likely to be in the first group and less likely to be in the second one. 
"
Classical models,"
Bias-Variance Tradeoff",Bias-Variance Tradeoff,"Error due to Bias: The error due to bias is taken as the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict. Of course you only have one model so talking about expected or average prediction values might seem a little strange. However, imagine you could repeat the whole model building process more than once: each time you gather new data and run a new analysis creating a new model. Due to randomness in the underlying data sets, the resulting models will have a range of predictions. Bias measures how far off in general these models' predictions are from the correct value.
Error due to Variance: The error due to variance is taken as the variability of a model prediction for a given data point. Again, imagine you can repeat the entire model building process multiple times. The variance is how much the predictions for a given point vary between different realizations of the model
This error of model may be decomposed into bias and variance components:
Err(x)=Bias^2+Variance+Irreducible Error
At its root, dealing with bias and variance is really about dealing with over- and under-fitting. Bias is reduced and variance is increased in relation to model complexity. As more and more parameters are added to a model, the complexity of the model rises and variance becomes our primary concern while bias steadily falls. For example, as more polynomial terms are added to a linear regression, the greater the resulting model's complexity will be. In other words, bias has a negative first-order derivative in response to model complexity while variance has a positive slope.
The sweet spot for any model is the level of complexity at which the increase in bias is equivalent to the reduction in variance. 
"
Classical models,Model Calibration,What is Model Calibration?,"Model calibration refers to the process of adjusting the predicted probabilities of a model so that they reflect the true likelihood of an event. For example, if a model predicts a 70% probability of rain tomorrow, it should actually rain 70% of the times when such a prediction is made over a large number of instances.
The model outputs two important pieces of information in a typical classification ML model. One is the predicted class label (for example, classification as spam or not spam emails), and the other is the predicted probability. In binary classification, the sci-kit learn library gives a method called the model.predict_proba(test_data) that gives us the probability for the target to be 0 and 1 in an array form. A model predicting rain can give us a 40% probability of rain and a 60% probability of no rain. We are interested in the uncertainty in the estimate of a classifier. There are typical use cases where the predicted probability of the model is very much of interest to us, such as weather models, fraud detection models, customer churn models, etc. For example, we may be interested in answering the question, what is the probability of this customer repaying the loan?
Why is Calibration Important?
Properly calibrated models are essential for decision-making processes where the predicted probabilities are directly used. Here are a few reasons why calibration is important:
Trustworthiness: A well-calibrated model provides more reliable predictions, which can be crucial in fields like healthcare, finance, and autonomous systems.
Decision Making: Accurate probability estimates allow for better risk assessment and decision making.
Comparative Performance: Calibration can help compare the performance of different models or classifiers in a meaningful way.
"
Classical models,Model Calibration,How to Measure Calibration,"Several metrics and tools can help assess the calibration of your model:
Calibration Plots (Reliability Diagrams): These plots compare predicted probabilities with actual outcomes. A perfectly calibrated model will have points lying on the diagonal line.
Brier Score: The Brier score is a metric used to measure the accuracy of probabilistic predictions. It is particularly useful for evaluating the calibration of binary classifiers. The Brier score quantifies how close the predicted probabilities are to the actual outcomes, combining both calibration and refinement.
"
Classical models,Model Calibration,Techniques for Model Calibration,"There are several methods to calibrate a machine learning model:
Histogram Binning
Platt Scaling
It is a post-processing method used to calibrate the probability estimates of a binary classifier. It involves fitting a logistic regression model to the scores (e.g., raw outputs or decision function values) of an existing classifier. The logistic regression model transforms these scores into well-calibrated probabilities.
How Does Platt Scaling Work?
The method involves the following steps:
Train the Classifier: Train your binary classifier as usual.
Get Raw Scores: Obtain the raw scores or decision function values from the classifier.
Fit Logistic Regression: Fit a logistic regression model to these scores using the true binary labels.
Predict Probabilities: Use the logistic regression model to convert raw scores into calibrated probabilities.
Isotonic Regression
Isotonic Regression is a non-parametric technique used to fit a non-decreasing (isotonic) function to data. It is particularly useful for calibrating the probabilities of a classifier by ensuring that the predicted probabilities are monotonically increasing with respect to the true outcomes.
How Does Isotonic Regression Work?
The method works by fitting a piecewise constant non-decreasing function to the data. Here’s the step-by-step process:
Train the Classifier: Train your binary classifier as usual.
Get Raw Scores: Obtain the raw scores or decision function values from the classifier.
Fit Isotonic Regression: Fit an isotonic regression model to these scores using the true binary labels.
Predict Probabilities: Use the isotonic regression model to convert raw scores into calibrated probabilities.
"
Classical models,Generative Models for Data Classification,Discriminative vs. Generative Models,"Discriminative Models
Discriminative models are designed to make clear, effective decisions about which class a data point belongs to. They do this by focusing directly on the boundary between classes, which means they concentrate on the differences between classes rather than modeling each class itself.
You’re asked to separate people into two groups: adults and children, based on height. Instead of trying to analyze everything about each group (like average age, weight, interests), you could simply focus on height, looking for a “cutoff” point that separates the two groups. Here, you’re looking for the height boundary, like “anyone taller than 5 feet is likely an adult.”
In this analogy:
The height cutoff is the decision boundary.
Focusing on the cutoff rather than understanding all the characteristics of adults and children represents the discriminative approach.
A common example of a discriminative model is logistic regression. This model tries to estimate the probability that a data point belongs to a specific class, say class k, based on the input features X. In mathematical terms, it models the conditional probability: P(Y = k ∣ X = x)
where:
Y is the class label, which tells us the category each data point belongs to.
X represents the input features or characteristics of the data point, like height, weight, or age.
k is a specific class we’re interested in.
By focusing on this probability, discriminative models don’t try to understand the individual classes themselves. Instead, they focus only on how to distinguish between classes. This makes them highly effective for tasks like classification, where the main goal is to decide whether a data point belongs to one class or another.
Generative Models
In contrast to discriminative models, generative models take a more comprehensive approach. They attempt to understand the data by modeling each class separately. Generative models estimate how the data in each class is distributed, aiming to capture the characteristics of each class itself. This is achieved by modeling the joint probability: P(X =x, Y =k)
where:
P(X =x, Y =k) is the probability of both the features X and the class label Y occurring together.
Generative models go a step further than discriminative models. They can calculate P(X∣Y = k), the probability of the input features given the class, and also P(Y=k), the overall probability of the class. These models offer a different perspective by modeling the distribution of the input features for each category. With this information, they can then compute P(Y = k ∣ X = x) using Bayes’ theorem.
This approach allows generative models not only to classify data points but also to generate new data that resembles the data in each class, which is a key advantage in applications like image generation or speech synthesis.
Generative classifiers
Assume some functional form for P(Y), P(X|Y)
Estimate parameters of P(X|Y), P(Y) directly from training data
Use Bayes rule to calculate P(Y |X)
Discriminative Classifiers
Assume some functional form for P(Y|X)
Estimate parameters of P(Y|X) directly from training data
"
Classical models,Generative Models for Data Classification,Which models are discriminative and which generative?,"Generative classifiers
‌Naïve Bayes
Bayesian networks
Markov random fields
‌Hidden Markov Models (HMM)
Discriminative Classifiers
‌Logistic regression
Scalar Vector Machine
‌Traditional neural networks
‌Nearest neighbour
Conditional Random Fields (CRF)s
"
Classical models,Generative Models for Data Classification,Why Generative Models Are Useful,"Generative models have several strengths:
Robustness: When classes are clearly distinct, generative models can be more stable and accurate. Since they model each class in detail, they are good at handling situations where classes are well-separated.
Effective with Small Sample Sizes: Generative models are often useful when there’s limited data. If the data within each class follows a known pattern (like a normal distribution), generative models can still perform well, even with fewer examples.
Flexibility with Multiple Classes: Generative models can handle scenarios with more than two classes naturally, as Bayes’ theorem can easily extend to any number of classes by summing across them.
Ability to Generate New Data: Since generative models learn the distribution of each class, they can create new data points that resemble those in the original dataset. This makes them popular in applications like image generation, text generation, and more, where creating realistic new examples is important.
"
Classical models,Generative Models for Data Classification,Gaussian Discriminant Analysis,"Gaussian Discriminant Analysis (GDA) is a supervised learning algorithm used for classification tasks in machine learning. It is a variant of the Linear Discriminant Analysis (LDA) algorithm that relaxes the assumption that the covariance matrices of the different classes are equal.
GDA works by assuming that the data in each class follows a Gaussian (normal) distribution, and then estimating the mean and covariance matrix for each class. It then uses Bayes’ theorem to compute the probability that a new data point belongs to each class, and chooses the class with the highest probability as the predicted class.
GDA can handle data with arbitrary covariance matrices for each class, unlike LDA, which assumes that the covariance matrices are equal. This makes GDA more flexible and able to handle more complex datasets. However, the downside is that GDA requires estimating more parameters, as it needs to estimate a separate covariance matrix for each class.
One disadvantage of GDA is that it can be sensitive to outliers and may overfit the data if the number of training examples is small relative to the number of parameters being estimated. Additionally, GDA may not perform well when the decision boundary between classes is highly nonlinear.
Overall, GDA is a powerful classification algorithm that can handle more complex datasets than LDA, but it requires more parameters to estimate and may not perform well in all situations.
"
Classical models,Generative Models for Data Classification,Linear Discriminant Analysis (LDA),"Linear Discriminant Analysis (LDA) is a popular classification technique in machine learning and statistics. Unlike logistic regression, which is a discriminative model, LDA is a generative model. It assumes that each class follows a normal (Gaussian) distribution, sharing the same variance but differing in mean. This setup allows this model to use Bayes’ theorem to determine the class of a new data point.
LDA assumes that the data in each class can be modeled by a normal distribution. In a normal distribution, most data points cluster around a central mean, forming a bell-shaped curve:
Each class k has its own mean μ_k for the distribution of the features.
All classes share a common variance σ², meaning the spread or “width” of the bell curve is the same across classes.
For example, if we’re classifying flowers by species, LDA assumes that the feature distributions (like petal length or width) for each species are normally distributed with unique means but the same variance.
LDA uses the training data to estimate three key components:
Prior probabilities P(Y=k): This is the probability of each class occurring in the dataset, calculated by the proportion of data points belonging to each class.
Class means u_k: For each class k, the mean of the feature values is computed to represent the “center” of the distribution for that class.
Shared variance σ²: LDA calculates a single variance across all classes to represent how the data is spread out around the means.
With these parameters estimated, LDA applies Bayes’ theorem to compute the posterior probability P(Y=k ∣ X=x), which is the probability that a data point with features X=x belongs to class k.
LDA assigns a new data point to the class with the highest posterior probability P(Y=k ∣ X=x). However, instead of calculating these probabilities directly, it’s often more convenient to work with the log of the posterior probabilities to avoid very small numbers and make computation easier, so LDA simplifies the problem by using the logarithm of the joint probability P(Y=k)⋅P(X=x ∣ Y=k).
For two classes, LDA finds a linear decision boundary where the discriminant functions for both classes are equal. This boundary is a straight line (or hyperplane in higher dimensions) that separates the classes in feature space.
"
Classical models,Generative Models for Data Classification,Quadratic Discriminant Analysis (QDA),"Assumption: Assumes that the data from each class follows a multivariate normal distribution with its own covariance matrix.
Decision Boundaries: The decision boundaries between classes are quadratic (curved).
Usage: Suitable for situations where the classes have different covariance structures (i.e., different variances or correlations between the features).
Flexibility: More flexible than Linear Discriminant Analysis (LDA), as it allows each class to have its own covariance matrix, which can lead to more complex decision boundaries.
"
Classical models,Miscellaneous ,Why might it be preferable to include fewer predictors over many?,"When we add irrelevant features, it increases model's tendency to overfit because those features introduce more noise. When two variables are correlated, they might be harder to interpret in case of regression, etc.
curse of dimensionality
adding random noise makes the model more complicated but useless
computational cost
"
Classical models,Miscellaneous ,What are some ways I can make my model more robust to outliers?,"We can have regularization such as L1 or L2 to reduce variance (increase bias).
Changes to the algorithm:
Use tree-based methods instead of regression methods as they are more resistant to outliers. For statistical tests, use non parametric tests instead of parametric ones.
Use robust error metrics such as MAE or Huber Loss instead of MSE.
Changes to the data:
Winsorizing the data
Transforming the data (e.g. log)
Remove them only if you’re certain they’re anomalies not worth predicting
"
Classical models,Miscellaneous ,Kernel function,"Kernel functions are generalized dot product functions used for the computing dot product of vectors x and y in high dimensional feature space. Kernal trick method is used for solving a non-linear problem by using a linear classifier by transforming linearly inseparable data into separable ones in higher dimensions.
If the data is not linearly separable in the original, or input, space then we apply transformations to the data, which map the data from the original space into a higher dimensional feature space. The goal is that after the transformation to the higher dimensional space, the classes are now linearly separable in this higher dimensional feature space. We can then fit a decision boundary to separate the classes and make predictions. The decision boundary will be a hyperplane in this higher dimensional space.
"
Classical models,Miscellaneous ,Describe Markov chains?,"Markov Chains defines that a state’s future probability depends only on its current state. 
Markov chains belong to the Stochastic process type category.
A perfect example of the Markov Chains is the system of word recommendation. In this system, the model recognizes and recommends the next word based on the immediately previous word and not anything before that. The Markov Chains take the previous paragraphs that were similar to training data-sets and generates the recommendations for the current paragraphs accordingly based on the previous word.
"
Classical models,Miscellaneous ,Difference between an error and a residual error,"The error of an observation is the deviation of the observed value from the true value of a quantity of interest (for example, a population mean). The residual is the difference between the observed value and the estimated value of the quantity of interest (for example, a sample mean).
"
Classical models,Miscellaneous ,What are the differences between Supervised and Unsupervised Learning?,"Supervised learning is a type of machine learning where a function is inferred from labeled training data. The training data contains a set of training examples.
Unsupervised learning, on the other hand, is when inferences are drawn from datasets containing input data without labeled responses.
The following are the various other differences between the two types of machine learning:
"
Classical models,Miscellaneous ,What is the Computational Graph?,"A computational graph is a graphical presentation that is based on TensorFlow. It has a wide network of different kinds of nodes wherein each node represents a particular mathematical operation. The edges in these nodes are called tensors. This is the reason the computational graph is called a TensorFlow of inputs. The computational graph is characterized by data flows in the form of a graph; therefore, it is also called the DataFlow Graph.
Forward pass is the procedure for evaluating the value of the mathematical expression represented by computational graphs. Doing forward pass means we are passing the value from variables in forward direction from the left (input) to the right where the output is.
In the backward pass, our intention is to compute the gradients for each input with respect to the final output. These gradients are essential for training the neural network using gradient descent.
"
Classical models,Miscellaneous ,What are parametric models? Provide an example.,"Parametric models have a finite number of parameters. You only need to know the parameters of the model to make a data prediction. Common examples are as follows: 
Logistic Regression
Linear Discriminant Analysis
Perceptron
Naive Bayes
Simple Neural Networks
A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) is called a parametric model. No matter how much data you throw at a parametric model, it won’t change its mind about how many parameters it needs.
Non-parametric models have an unbounded number of parameters to offer flexibility. For data predictions, you need the parameters of the model and the state of the observed data. Common examples are as follows: 
k-Nearest Neighbors
Decision Trees like CART and C4.5
Support Vector Machines
Nonparametric methods are good when you have a lot of data and no prior knowledge, and when you don’t want to worry too much about choosing just the right features.
An easy to understand nonparametric model is the k-nearest neighbors algorithm that makes predictions based on the k most similar training patterns for a new data instance. The method does not assume anything about the form of the mapping function other than patterns that are close are likely to have a similar output variable.
"
Classical models,Miscellaneous ,Hidden Markov Model vs Recurrent Neural Network,"Hidden Markov Models (HMMs) are much simpler than Recurrent Neural Networks (RNNs), and rely on strong assumptions which may not always be true. If the assumptions are true then you may see better performance from an HMM since it is less finicky to get working.
An RNN may perform better if you have a very large dataset, since the extra complexity can take better advantage of the information in your data. This can be true even if the HMMs assumptions are true in your case.
Finally, don't be restricted to only these two models for your sequence task, sometimes simpler regressions (e.g. ARIMA) can win out, and sometimes other complicated approaches such as Convolutional Neural Networks might be the best. (Yes, CNNs can be applied to some kinds of sequence data just like RNNs.)
As always, the best way to know which model is best is to make the models and measure performance on a held out test set.
Strong Assumptions of HMMs
State transitions only depend on the current state, not on anything in the past.
"
Classical models,Miscellaneous ,What is difference between Inference and Prediction?,"Prediction:
- Evaluate a variety of models
- Select the best-performing model
- Empirically determine loss on test set
- Predict the outcome for new samples
- Model interpretability suffers
- Model validity shown for the test set
- Model may overfit if the test data are similar to the training data
Inference:
- Reason about the data generation process
- Select model whose assumptions seem most reasonable
- Use goodneess-of-fit tests
- Use the model to explain the data generation process
- High model interpretability
- Model validity is uncertain since predictive accuracy was not considered
- Overfitting prevented through simplified assumptions
"
Classical models,Miscellaneous ,State the limitations of Fixed Basis Function,"Linear separability in feature space doesn’t imply linear separability in input space. So, Inputs are non-linearly transformed using vectors of basic functions with increased dimensionality. Limitations of Fixed basis functions are:
Non-Linear transformations cannot remove overlap between two classes but they can increase overlap.
Often it is not clear which basis functions are the best fit for a given task. So, learning the basic functions can be useful over using fixed basis functions.
If we want to use only fixed ones, we can use a lot of them and let the model figure out the best fit but that would lead to overfitting the model thereby making it unstable. 
"
Classical models,Miscellaneous ,Explain the term instance-based learning.,"Instance Based Learning is a set of procedures for regression and classification which produce a class label prediction based on resemblance to its nearest neighbors in the training data set. These algorithms just collects all the data and get an answer when required or queried. In simple words they are a set of procedures for solving new problems based on the solutions of already solved problems in the past which are similar to the current problem.
"
Classical models,Miscellaneous ,Define Perceptron,"An artificial neuron is a mathematical function conceived as a model of biological neurons, that is, a neural network.
A Perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data. It is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).
”Perceptron Learning Rule states that the algorithm would automatically learn the optimal weight coefficients.
Single layer Perceptrons can learn only linearly separable patterns.
Multilayer Perceptron or feedforward neural network with two or more layers have the greater processing power and can process non-linear patterns as well.
Perceptrons can implement Logic Gates like AND, OR, or XOR.
"
Classical models,Miscellaneous ,Estimating the Noise Variance,"The maximum likelihood estimate of the noise variance is the empirical mean of the squared distances between the noise-free function values ϕ ⊤ (xn)θ and the corresponding noisy observations yn at input locations xn:
σ^2 = 1 / N * sum_n=1..N(yn − ϕ⊤(xn)θ)^2
"
Classical NLP,NLP Tasks,Part of Speech Tagging,"Each word has its own role in a sentence. For example,’Geeta is dancing’. Geeta is the person or ‘Noun’ and dancing is the action performed by her, so it is a ‘Verb’.Likewise, each word can be classified. This is referred as POS or Part of Speech Tagging.
Rule-based POS tagging: The rule-based POS tagging models apply a set of handwritten rules and use contextual information to assign POS tags to words. These rules are often known as context frame rules. One such rule might be: “If an ambiguous/unknown word ends with the suffix ‘ing’ and is preceded by a Verb, label it as a Verb”.
Transformation Based Tagging:  The transformation-based approaches use a pre-defined set of handcrafted rules as well as automatically induced rules that are generated during training.
Deep learning models: Various Deep learning models have been used for POS tagging such as Meta-BiLSTM which have shown an impressive accuracy of around 97 percent.
Stochastic (Probabilistic) tagging: A stochastic approach includes frequency, probability or statistics. The simplest stochastic approach finds out the most frequently used tag for a specific word in the annotated training data and uses this information to tag that word in the unannotated text. But sometimes this approach comes up with sequences of tags for sentences that are not acceptable according to the grammar rules of a language. One such approach is to calculate the probabilities of various tag sequences that are possible for a sentence and assign the POS tags from the sequence with the highest probability. Hidden Markov Models (HMMs) are probabilistic approaches to assign a POS Tag.
"
Classical NLP,NLP Tasks,Dependency Parsing,"In a sentence, the words have a relationship with each other. The one word in a sentence which is independent of others, is called as Head /Root word. All the other word are dependent on the root word, they are termed as dependents.
Dependency Parsing is the method of analyzing the relationship/ dependency between different words of a sentence.
Usually, in a sentence, the verb is the head word.
You can access the dependency of a token through token.dep_ attribute.
token.dep_ prints dependency tags for each token
What is the meaning of these dependency tags ?
nsubj :Subject of the sentence
ROOT: The headword or independent word,(generally the verb)
prep: prepositional modifier, it modifies the meaning of a noun, verb, adjective, or preposition.
cc and conj: Linkages between words. For example: is, and, etc…
pobj : Denotes the object of the preposition
aux : Denotes it is an auxiliary word
dobj : direct object of the verb
det : Not specific, but not an independent word.
"
Classical NLP,NLP Tasks,Named Entity Recognition,"Suppose you have a collection of news articles text data. What if you want to know what companies/organizations have been in the news? How will you do that?
Take another case of text dataset of information about influential people. What if you want to know the names of influencers in this dataset ?
NER is the technique of identifying named entities in the text corpus and assigning them pre-defined categories such as ‘ person names’ , ‘ locations’ ,’organizations’,etc..
It is a very useful method especially in the field of claasification problems and search egine optimizations.
NER can be implemented through both nltk and spacy`.
"
Classical NLP,NLP Tasks,What is Extractive Text Summarization and Generative Text Summarization,"Extractive Text Summarization is the traditional method, in which the process is to identify significant phrases/sentences of the text corpus and include them in the summary.
The summary obtained from this method will contain the key-sentences of the original text corpus.
You can notice that in the extractive method, the sentences of the summary are all taken from the original text. There is no change in structure of any sentence.
Generative text summarization methods overcome this shortcoming. The concept is based on capturing the meaning of the text and generating entirely new sentences to best represent them in the summary.
"
Classical NLP,NLP Tasks,Define the term parsing concerning NLP,"Parsing refers to the task of generating a linguistic structure for a given input. For example, parsing the word ‘helping’ will result in verb-pass + gerund-ing.
Simply speaking, parsing in NLP is the process of determining the syntactic structure of a text by analyzing its constituent words based on an underlying grammar (of the language).
See this example grammar below, where each line indicates a rule of the grammar to be applied to an example sentence “Tom ate an apple”.
"
Classical NLP,NLP Tasks,What Is Semantic Analysis?,"Simply put, semantic analysis is the process of drawing meaning from text. It allows computers to understand and interpret sentences, paragraphs, or whole documents, by analyzing their grammatical structure, and identifying relationships between individual words in a particular context.
It’s an essential sub-task of Natural Language Processing (NLP) and the driving force behind machine learning tools like chatbots, search engines, and text analysis.
"
Classical NLP,NLP Tasks,What is NLU?," NLU stands for Natural Language Understanding. It is a subdomain of NLP that concerns making a machine learn the skills of reading comprehension. A few applications of NLU include Machine translation (MT), Newsgathering, and Text categorization. It often goes by the name Natural Language Interpretation (NLI) as well.
"
Classical NLP,Preprocessing,Hashing Vecrtorizer,"Hashing Vectorizer converts text to a matrix of occurrences using the “hashing trick” Each word is mapped to a feature and using the hash function converts it to a hash. If the word occurs again in the body of the text it is converted to that same feature which allows us to count it in the same feature without retaining a dictionary in memory.
"
Classical NLP,Preprocessing,Lemmatization,"Lemmatization is a process by which inflected forms of words are grouped together to be analyzed as a single aspect. It is a way of using the intended meaning of a word to determine the “lemma”. It is largely depending on correctly finding “intended parts of speech” and the true meaning of a word in a sentence, paragraph, or larger documents. Examples of Lemmatization are that “run” is a base form for words like “running” or “ran” or that the word “better” and “good” are in the same lemma so they are considered the same.
"
Classical NLP,Preprocessing,Stemming,"Stemming is quite similar to Lemmatization in that it groups words together, but unlike lemmatization it takes a word and refers it back to its base or root form. In fact, on the best examples I’ve come across to describe it involves refereeing Stemming back to its base form. “Stems”, “Stemming”, “Stemmed”, “and Stemtization” are all based on the single word “stem”.
"
Classical NLP,Preprocessing,N-Grams,"An N-Gram is a sequence of N-words in a sentence. Here, N is an integer which stands for the number of words in the sequence.
For example, if we put N=1, then it is referred to as a uni-gram. If you put N=2, then it is a bi-gram. If we substitute N=3, then it is a tri-gram.
The bag of words does not take into consideration the order of the words in which they appear in a document, and only individual words are counted.
In some cases, the order of the words might be important.
N-grams captures the context in which the words are used together. For example, it might be a good idea to consider bigrams like “New York” instead of breaking it into individual words like “New” and “York”
Consider the sentence “I like dancing in the rain”
See the Uni-Gram, Bi-Gram, and Tri-Gram cases below.
UNIGRAM: ‘I’, ‘like’, ‘dancing’, ‘in’, ‘the’, ‘rain’
BIGRAM: ‘I like’, ‘like dancing’, ‘dancing in’, ‘in the’, ‘the rain’
TRIGRAM: ‘I like dancing’, ‘like dancing in’, ‘dancing in the’, ‘in the rain’
"
Classical NLP,Preprocessing,Byte-Pair Encoding (BPE),"Natural Language Processing (NLP) is a subfield of artificial intelligence that gives the ability to machine understand and process human languages. Tokenization is the process of dividing the text into a collection of tokens from a string of text. Tokenization is often the first step in natural language processing tasks such as text classification, named entity recognition, and sentiment analysis. The resulting tokens are typically used as input to further processing steps, such as vectorization, where the tokens are converted into numerical representations for machine learning models to use.
Byte-Pair Encoding (BPE) is a compression algorithm used in Natural Language Processing (NLP) to represent large vocabulary with a small set of subword units. It was introduced by Sennrich et al. in 2016 and has been widely used in various NLP tasks such as machine translation, text classification, and text generation. The basic idea of BPE is to iteratively merge the most frequent pair of consecutive bytes or characters in a text corpus until a predefined vocabulary size is reached. The resulting subword units can be used to represent the original text in a more compact and efficient way.
Concepts related to BPE:
Vocabulary: A set of subword units that can be used to represent a text corpus.
Byte: A unit of digital information that typically consists of eight bits.
Character: A symbol that represents a written or printed letter or numeral.
Frequency: The number of times a byte or character occurs in a text corpus.
Merge: The process of combining two consecutive bytes or characters to create a new subword unit.
Steps involved in BPE:
Initialize the vocabulary with all the bytes or characters in the text corpus
Calculate the frequency of each byte or character in the text corpus.
Repeat the following steps until the desired vocabulary size is reached:
Find the most frequent pair of consecutive bytes or characters in the text corpus
Merge the pair to create a new subword unit.
Update the frequency counts of all the bytes or characters that contain the merged pair.
Add the new subword unit to the vocabulary.
Represent the text corpus using the subword units in the vocabulary.
Byte-level BPE
A base vocabulary that includes all possible base characters can be quite large if e.g. all unicode characters are considered as base characters. To have a better base vocabulary, GPT-2 uses bytes as the base vocabulary, which is a clever trick to force the base vocabulary to be of size 256 while ensuring that every base character is included in the vocabulary. With some additional rules to deal with punctuation, the GPT2’s tokenizer can tokenize every text without the need for the <unk> symbol. GPT-2 has a vocabulary size of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned with 50,000 merges.
"
Classical NLP,Preprocessing,WordPiece,"WordPiece is the subword tokenization algorithm used for BERT, DistilBERT, and Electra. The algorithm was outlined in Japanese and Korean Voice Search (Schuster et al., 2012) and is very similar to BPE. WordPiece first initializes the vocabulary to include every character present in the training data and progressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent symbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.
So what does this mean exactly? Referring to the previous example, maximizing the likelihood of the training data is equivalent to finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by its second symbol is the greatest among all symbol pairs. E.g. ""u"", followed by ""g"" would have only been merged if the probability of ""ug"" divided by ""u"", ""g"" would have been greater than for any other symbol pair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it loses by merging two symbols to ensure it’s worth it.
"
Classical NLP,Preprocessing,Unigram,"Unigram is a subword tokenization algorithm introduced in Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo, 2018). In contrast to BPE or WordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each symbol to obtain a smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and the most common substrings. Unigram is not used directly for any of the models in the transformers, but it’s used in conjunction with SentencePiece.
At each training step, the Unigram algorithm defines a loss (often defined as the log-likelihood) over the training data given the current vocabulary and a unigram language model. Then, for each symbol in the vocabulary, the algorithm computes how much the overall loss would increase if the symbol was to be removed from the vocabulary. Unigram then removes p (with p usually being 10% or 20%) percent of the symbols whose loss increase is the lowest, i.e. those symbols that least affect the overall loss over the training data. This process is repeated until the vocabulary has reached the desired size. The Unigram algorithm always keeps the base characters so that any word can be tokenized.
Because Unigram is not based on merge rules (in contrast to BPE and WordPiece), the algorithm has several ways of tokenizing new text after training. As an example, if a trained Unigram tokenizer exhibits the vocabulary:
[""b"", ""g"", ""h"", ""n"", ""p"", ""s"", ""u"", ""ug"", ""un"", ""hug""],
""hugs"" could be tokenized both as [""hug"", ""s""], [""h"", ""ug"", ""s""] or [""h"", ""u"", ""g"", ""s""]. So which one to choose? Unigram saves the probability of each token in the training corpus on top of saving the vocabulary so that the probability of each possible tokenization can be computed after training. The algorithm simply picks the most likely tokenization in practice, but also offers the possibility to sample a possible tokenization according to their probabilities.
Those probabilities are defined by the loss the tokenizer is trained on. Assuming that the training data consists of the wordsx1,…,xN  and that the set of all possible tokenizations for a wordxixi is defined as S(xi), then the overall loss is defined as L=−∑i=1Nlog⁡(∑x∈S(xi)p(x))
"
Classical NLP,Preprocessing,SentencePiece,"All tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to separate words. However, not all languages use spaces to separate words. One possible solution is to use language specific pre-tokenizers, e.g. XLM uses a specific Chinese, Japanese, and Thai pre-tokenizer. To solve this problem more generally, SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo et al., 2018) treats the input as a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram algorithm to construct the appropriate vocabulary.
The XLNetTokenizer uses SentencePiece for example, which is also why in the example earlier the ""▁"" character was included in the vocabulary. Decoding with SentencePiece is very easy since all tokens can just be concatenated and ""▁"" is replaced by a space.
All transformers models in the library that use SentencePiece use it in combination with unigram. Examples of models using SentencePiece are ALBERT, XLNet, Marian, and T5.
"
Classical NLP,Word Embeddings,What do you understand by word embedding?,"In NLP, word embedding is the process of representing textual data through a real-numbered vector. This method allows words having similar meanings to have a similar representation.
"
Classical NLP,Word Embeddings,What is an embedding matrix?,"A word embedding matrix is a matrix that contains embedding vectors of all the words in a given text.
"
Classical NLP,Word Embeddings,List a few popular methods used for word embedding.,"Following are a few methods of word embedding.
One-hot
BOW
TF-IDF
Word2Vec:
CBOW
Skip-gram
Glove
"
Classical NLP,Word Embeddings,Contextualized Word Embeddings vs. Static Embeddings,"Contextualized word embeddings are representations of words that change depending on the context in which they appear, whereas static embeddings assign a single fixed vector to each word regardless of its usage.
1. Static Word Embeddings (Word2Vec, GloVe, FastText)
Each word has one fixed vector representation, learned from large corpora.
Doesn't account for polysemy (words with multiple meanings) or contextual nuance.
2. Contextualized Word Embeddings (ELMo, BERT, GPT, T5)
Word embeddings are generated dynamically based on the surrounding words.
Uses deep neural networks, often transformers (like in BERT, GPT).
Can distinguish multiple meanings of the same word (polysemy).
"
Classical NLP,Word Embeddings,Semantic Similarity in Word Embeddings,"Semantic similarity refers to how closely the meanings of two words are related. In the context of word embeddings, words with similar meanings have vector representations that are close together in the embedding space. This is because embeddings capture contextual and semantic relationships based on how words are used in a language.
1. Cosine Similarity (Most Common)
2. Euclidean Distance
3. Pearson Correlation
"
Classical NLP,Word Embeddings,Bag of Words (BoW):,"The BoW model captures the frequencies of the word occurrences in a text corpus.
Bag of words is not concerned about the order in which words appear in the text; instead, it only cares about which words appear in the text.
Let’s understand how BoW works with an example. Consider the following phrases:
Document 1: Cats and dogs are not allowed
Document 2: Cats and dogs are antagonistic
Bag of words will first create a unique list of all the words based on the two documents. If we consider the two documents, we will have seven unique words.
‘cats’, ‘and’, ‘dogs’, ‘are’, ‘not’, ‘allowed’, ‘antagonistic’
Each unique word is a feature or dimension.
Now for each document, a feature vector will be created. Each feature vector will be seven-dimensional since we have seven unique words.
Document1 vector: [1 1 1 1 1 1 0]
Document2 vector: [1 1 1 1 0 0 1]
"
Classical NLP,Word Embeddings,"Term Frequency, Inverse Document Frequency(TF-IDF):","This is the most popular way to represent documents as feature vectors. TF-IDF stands for Term Frequency, Inverse Document Frequency.
TF-IDF measures how important a particular word is with respect to a document and the entire corpus.
Term Frequency:
Term frequency is the measure of the counts of each word in a document out of all the words in the same document. 
TF(w) = (number of times word w appears in a document) / (total number of words in the document)
For example, if we want to find the TF of the word cat which occurs 50 times in a document of 1000 words, then 
TF(cat) = 50 / 1000 = 0.05
Inverse Document Frequency:
IDF is a measure of the importance of a word, taking into consideration the frequency of the word throughout the corpus.
It measures how important a word is for the corpus.
IDF(w) = log(total number of documents / number of documents with w in it)
For example, if the word cat occurs in 100 documents out of 3000, then the IDF is calculated as
IDF(cat) = log(3000 / 100) = 1.47
Finally, to calculate TF-IDF, we multiply these two factors – TF and IDF.
TF-IDF(w) = TF(w) x IDF(w)
TF-IDF(cat) = 0.05 * 1.47 = 0.073
"
Classical NLP,Word Embeddings,Word2Vec,"Word2Vec is a popular word embedding technique that aims to represent words as continuous vectors in a high-dimensional space. It introduces two models: Continuous Bag of Words (CBOW) and Skip-gram, each contributing to the learning of vector representations.
1. Model Architecture:
Continuous Bag of Words (CBOW): In CBOW, the model predicts a target word based on its context. The context words are used as input, and the target word is the output. The model is trained to minimize the difference between the predicted and actual target words.
Skip-gram: Conversely, the Skip-gram model predicts context words given a target word. The target word serves as input, and the model aims to predict the words that are likely to appear in its context. Like CBOW, the goal is to minimize the difference between the predicted and actual context words.
2. Neural Network Training:
Both CBOW and Skip-gram models leverage neural networks to learn vector representations. The neural network is trained on a large text corpus, adjusting the weights of connections to minimize the prediction error. This process places similar words closer together in the resulting vector space.
3. Vector Representations:
Once trained, Word2Vec assigns each word a unique vector in the high-dimensional space. These vectors capture semantic relationships between words. Words with similar meanings or those that often appear in similar contexts have vectors that are close to each other, indicating their semantic similarity.
4. Advantages and Disadvantages:
Advantages:
Captures semantic relationships effectively.
Efficient for large datasets.
Provides meaningful word representations.
Disadvantages:
May struggle with rare words.
Ignores word order.
"
Classical NLP,Word Embeddings,Doc2Vec Model,"It is an unsupervised algorithm that learns fixed-length feature vector representation from variable-length pieces of texts. Then these vectors can be used in any machine learning classifier to predict the classes label.
It is similar to Word2Vec model except, it uses all words in each text file to create a unique column in a matrix (called it Paragraph Matrix). Then a single layer NN, like the one seen in Skip-Gram model, will be trained where the input data are all surrounding words of the current word along with the current paragraph column to predict the current word. The rest is same as the Skip-Gram or CBOW models.
"
Classical NLP,Word Embeddings,FastText,"FastText is an advanced word embedding technique developed by Facebook AI Research (FAIR) that extends the Word2Vec model. Unlike Word2Vec, FastText not only considers whole words but also incorporates subword information — parts of words like n-grams. This approach enables the handling of morphologically rich languages and captures information about word structure more effectively.
1. Subword Information:
FastText represents each word as a bag of character n-grams in addition to the whole word itself. This means that the word “apple” is represented by the word itself and its constituent n-grams like “ap”, “pp”, “pl”, “le”, etc. This approach helps capture the meanings of shorter words and affords a better understanding of suffixes and prefixes.
2. Model Training:
Similar to Word2Vec, FastText can use either the CBOW or Skip-gram architecture. However, it incorporates the subword information during training. The neural network in FastText is trained to predict words (in CBOW) or context (in Skip-gram) not just based on the target words but also based on these n-grams.
3. Handling Rare and Unknown Words:
A significant advantage of FastText is its ability to generate better word representations for rare words or even words not seen during training. By breaking down words into n-grams, FastText can construct meaningful representations for these words based on their subword units.
4. Advantages and Disadvantages:
Advantages:
Better representation of rare words.
Capable of handling out-of-vocabulary words.
Richer word representations due to subword information.
Disadvantages:
Increased model size due to n-gram information.
Longer training times compared to Word2Vec.
"
Classical NLP,Word Embeddings,How does FastText differ from Word2Vec in handling out-of-vocabulary words?,"FastText differs from Word2Vec in handling out-of-vocabulary (OOV) words primarily due to its subword-based approach. Here’s how:
Subword Representation:
FastText represents words as a combination of character n-grams (e.g., ""apple"" might be broken into <ap, app, ppl, ple>, etc.).
This allows it to generate word vectors even for words not seen during training by composing their embeddings from known subword units.
Word2Vec's Limitation:
Word2Vec treats words as atomic units, meaning that if a word wasn’t seen during training, it has no embedding.
OOV words are not assigned any vector and are essentially ignored.
Impact on Performance:
FastText performs better on morphologically rich languages and domain-specific vocabularies.
It can generalize to unseen words, such as new names, typos, or rare words, which is especially useful for NLP tasks like spell correction and low-resource languages.
"
Classical NLP,Word Embeddings,GloVe (Global Vectors for Word Representation),"Global Vectors for Word Representation (GloVe) is a powerful word embedding technique that captures the semantic relationships between words by considering their co-occurrence probabilities within a corpus. The key to GloVe’s effectiveness lies in the construction of a word-context matrix and the subsequent factorization process.
1. Word-Context Matrix Formation:
The first step in GloVe’s mechanics involves creating a word-context matrix. This matrix is designed to represent the likelihood of a given word appearing near another across the entire corpus. Each cell in the matrix holds the co-occurrence count of how often words appear together in a certain context window.
2. Factorization for Word Vectors:
With the word-context matrix in place, GloVe turns to matrix factorization. The objective here is to decompose this high-dimensional matrix into two smaller matrices — one representing words and the other contexts. Let’s denote these as W for words and C for contexts. The ideal scenario is when the dot product of W and CT (transpose of C) approximates the original matrix:
X≈W⋅CT
Through iterative optimization, GloVe adjusts W and C to minimize the difference between X and W⋅CT. This process yields refined vector representations for each word, capturing the nuances of their co-occurrence patterns.
3. Vector Representations:
Once trained, GloVe provides each word with a dense vector that captures not just local context but global word usage patterns. These vectors encode semantic and syntactic information, revealing similarities and differences between words based on their overall usage in the corpus.
4. Advantages and Disadvantages:
Advantages:
Efficiently captures global statistics of the corpus.
Good at representing both semantic and syntactic relationships.
Effective in capturing word analogies.
Disadvantages:
Requires more memory for storing co-occurrence matrices.
Less effective with very small corpora.
"
Classical NLP,Word Embeddings,Skip-gram and negative sampling,"While a bag-of-words model predicts a word given the neighboring context, a skip-gram model predicts the context (or neighbors) of a word, given the word itself. The model is trained on skip-grams, which are n-grams that allow tokens to be skipped (see the diagram below for an example). The context of a word can be represented through a set of skip-gram pairs of (target_word, context_word) where context_word appears in the neighboring context of target_word.
Consider the following sentence of eight words:
The wide road shimmered in the hot sun.
The context words for each of the 8 words of this sentence are defined by a window size. The window size determines the span of words on either side of a target_word that can be considered a context word. Below is a table of skip-grams for target words based on different window sizes.
The training objective of the skip-gram model is to maximize the probability of predicting context words given the target word. For a sequence of words w1, w2, ... wT, the objective can be written as the average log probability
where c is the size of the training context. The basic skip-gram formulation defines this probability using the softmax function.
where v and v' are target and context vector representations of words and W is vocabulary size.
Computing the denominator of this formulation involves performing a full softmax over the entire vocabulary words, which are often large (105-107) terms.
The noise contrastive estimation (NCE) loss function is an efficient approximation for a full softmax. With an objective to learn word embeddings instead of modeling the word distribution, the NCE loss can be simplified to use negative sampling.
The simplified negative sampling objective for a target word is to distinguish the context word from num_ns negative samples drawn from noise distribution Pn(w) of words. More precisely, an efficient approximation of full softmax over the vocabulary is, for a skip-gram pair, to pose the loss for a target word as a classification problem between the context word and num_ns negative samples.
A negative sample is defined as a (target_word, context_word) pair such that the context_word does not appear in the window_size neighborhood of the target_word. For the example sentence, these are a few potential negative samples (when window_size is 2).
"
Classical NLP,Word Embeddings,Choosing the Right Embedding Model,"Word2Vec: Use when semantic relationships are crucial, and you have a large dataset.
GloVe: Suitable for diverse datasets and when capturing global context is important.
FastText: Opt for morphologically rich languages or when handling out-of-vocabulary words is vital.
"
Classical NLP,Word Embeddings,How the Curse of Dimensionality Affects Word Embeddings,"Word embeddings (e.g., Word2Vec, GloVe, FastText) represent words as high-dimensional vectors (e.g., 300D). While this helps capture semantics, increasing dimensions excessively can lead to:
A) Sparse Representations
High-dimensional word vectors can become sparse, reducing their ability to capture meaningful patterns.
This sparsity makes it difficult for models to generalize well across unseen words.
B) Computational Complexity
High-dimensional operations (e.g., matrix multiplications, similarity calculations) require more memory and computation time.
Training and querying embeddings in high-dimensional space slow down significantly.
C) Decreased Similarity Discrimination
In very high dimensions, cosine similarity values tend to be close together, making it harder to differentiate truly similar words.
Solutions to Combat the Curse of Dimensionality in Word Embeddings
A) Dimensionality Reduction Techniques
Principal Component Analysis (PCA): Reduces dimensions while retaining key variance.
t-SNE / UMAP: Helps visualize embeddings in 2D/3D while preserving local structure.
B) Optimal Embedding Size
Studies show 300-dimensional embeddings often perform just as well as 1000D embeddings while being more efficient.
Too many dimensions can lead to overfitting, while too few can cause information loss.
C) Subword & Contextual Embeddings
FastText uses subword information to make embeddings denser and more informative.
BERT & Transformer-based embeddings create contextualized representations that adapt to different meanings of a word in various contexts.
"
Classical NLP,Metrics,What is perplexity in NLP?,"In general, perplexity is a measurement of how well a probability model predicts a sample. In the context of Natural Language Processing, perplexity is one way to evaluate language models.
A language model is a probability distribution over sentences: it’s both able to generate plausible human-written sentences (if it’s a good language model) and to evaluate the goodness of already written sentences. Presented with a well-written document, a good language model should be able to give it a higher probability than a badly written document, i.e. it should not be “perplexed” when presented with a well-written document.
Thus, the perplexity metric in NLP is a way to capture the degree of ‘uncertainty’ a model has in predicting (i.e. assigning probabilities to) text.
The probability of a generic sentence W, made of the words w1, w2, up to wn, can be expressed as the following:
P(W) = P(w1, w2, …, wn)
This can be done by normalizing the sentence probability by the number of words in the sentence. Since the probability of a sentence is obtained by multiplying many factors, we can average them using the geometric mean.
Let’s call Pnorm(W) the normalized probability of the sentence W. Let n be the number of words in W. Then, applying the geometric mean:
Pnorm(W) = P(W) ^ (1 / n)
Perplexity can be computed also starting from the concept of Shannon entropy. Let’s call H(W) the entropy of the language model when predicting a sentence W. Then, it turns out that:
PP(W) = 2 ^ (H(W))
"
Classical NLP,Metrics,Evaluating the Quality of Word Embeddings,"Evaluating word embeddings is essential to ensure they capture meaningful semantic and syntactic relationships. There are three main approaches: intrinsic evaluation, extrinsic evaluation, and visualization techniques.
1. Intrinsic Evaluation (Direct Assessment of Embeddings)
Intrinsic evaluation measures how well embeddings capture semantic and syntactic relationships without using a specific task.
A) Word Similarity Tasks
Idea: Compare cosine similarity between word pairs with human judgments.
Example Datasets:
WordSim-353 (semantic similarity)
MEN dataset (semantic relatedness)
SimLex-999 (focuses on fine-grained word meanings)
Metric: Spearman’s correlation between model predictions and human scores.
Example:
Similarity(“dog”, “puppy”) = 0.85 (expected: high similarity)
Similarity(“car”, “banana”) = 0.10 (expected: low similarity)
B) Analogy Tasks (Word Relationship Evaluation)
Idea: Evaluate embeddings by solving word analogies, e.g.,
""king"" - ""man"" + ""woman"" ≈ ""queen""
""Paris"" - ""France"" + ""Germany"" ≈ ""Berlin""
Example Datasets:
Google Analogy Dataset (semantic & syntactic relations)
MSR Syntactic Analogies
Metric: Accuracy (% of correct analogies).
C) Clustering & Nearest Neighbors
Idea: Check if embeddings group similar words together in vector space.
Example:
""apple"", ""banana"", ""grape"" should be clustered in the ""fruit"" category.
""doctor"", ""nurse"", ""surgeon"" should be close in a healthcare domain.
2. Extrinsic Evaluation (Task-Specific Performance)
Extrinsic evaluation tests how well embeddings improve downstream NLP tasks.
A) Text Classification
Idea: Use embeddings as features in models like Logistic Regression, LSTMs, or Transformers.
Example Tasks:
Sentiment Analysis (IMDB, SST-2)
Spam Detection (Enron Email Dataset)
Metric: Accuracy, F1-score.
B) Named Entity Recognition (NER)
Idea: Test embeddings in a sequence tagging task.
Example Dataset: CoNLL-2003 NER
Metric: Precision, Recall, F1-score.
C) Machine Translation
Idea: Evaluate embeddings by using them in an encoder-decoder model.
Metric: BLEU Score.
D) Question Answering
Idea: Use embeddings to improve information retrieval.
Example Dataset: SQuAD
Metric: F1-score, Exact Match (EM).
3. Visualization Techniques
A) t-SNE / PCA for Dimensionality Reduction
Idea: Reduce high-dimensional embeddings (e.g., 300D) into 2D or 3D to visualize clustering.
t-SNE Example:
Words like “dog, cat, lion, tiger” should cluster in the “animals” region.
Words like “Paris, London, Tokyo” should group as cities.
B) Heatmaps for Similarity
Idea: Show cosine similarity between selected words.
"
Classical NLP,Metrics,What is BLEU?,"BLEU is the abbreviation for bilingual evaluation understudy.
BLEU is used for evaluating the quality of machine translated text.
A text is considered higher quality the similar it is to a professional human translator.
BLEU's is always between 0 and 1. The value of 1 is for the most similar text to the target text.
A BLEU score of 0.5 is considered high quality, while a score of >0.6 is considered better than humans. Anything less than 0.2 is not understandable and does not give the gist of the translation well.
"
Classical NLP,Metrics,Briefly describe the N-gram model in NLP," N-gram model is a model in NLP that predicts the probability of a word in a given sentence using the conditional probability of n-1 previous words in the sentence. The basic intuition behind this algorithm is that instead of using all the previous words to predict the next word, we use only a few previous words.
"
Classical NLP,Metrics,What is the Markov assumption for the bigram model?," The Markov assumption assumes for the bigram model that the probability of a word in a sentence depends only on the previous word in that sentence and not on all the previous words.
"
Classical NLP,NLP Libraries ,Natural Language Toolkit (NLTK),"NLTK is the main library for building Python projects to work with human language data. It gives simple to-utilize interfaces to more than 50 corpora and lexical assets like WordNet, alongside a set-up of text preprocessing libraries for tagging, parsing, classification, stemming, tokenization and semantic reasoning wrappers for NLP libraries and an active conversation discussion. NLTK is accessible for Windows, Mac OS, and Linux. The best part is that NLTK is a free, open-source, local area-driven venture. It has some disadvantages as well. It is slow and difficult to match the demands of production usage. The learning curve is somehow steep. Some of the features provided by NLTK are;
Entity Extraction
Part-of-speech tagging
Tokenization
Parsing
Semantic reasoning
Stemming
Text classification
"
Classical NLP,NLP Libraries ,GenSim,"Gensim is a famous python library for natural language processing tasks. It provides a special feature to identify semantic similarity between two documents by the use of vector space modelling and the topic modelling toolkit. All algorithms in GenSim are memory-independent concerning corpus size it means we can process input larger than RAM. It provides a set of algorithms that are very useful in natural language tasks such as Hierarchical Dirichlet Process(HDP), Random Projections(RP), Latent Dirichlet Allocation(LDA), Latent Semantic Analysis(LSA/SVD/LSI) or word2vec deep learning. The most advanced feature of GenSim is its processing speed and fantastic memory usage optimization. The main uses of GenSim include Data Analysis, Text generation applications (chatbots) and Semantic search applications.GenSim highly depends on SciPy and NumPy for scientific computing.
"
Classical NLP,NLP Libraries ,SpaCy,"SpaCy is an open-source python Natural language processing library. It is mainly designed for production usage- to build real-world projects and it helps to handle a large number of text data. This toolkit is written in python in Cython which’s why it much faster and efficient to handle a large amount of text data. Some of the features of SpaCy are shown below:
It provides multi trained transformers like BERT
It is way faster than other libraries
Provides tokenization that is motivated linguistically In more than 49 languages
Provides functionalities such as text classification, sentence segmentation, lemmatization, part-of-speech tagging, named entity recognition and many more
It has 55 trained pipelines in more than 17 languages.
"
Classical NLP,Miscellaneous,What Is a Knowledge Graph?,"A knowledge graph is an organized representation of real-world entities and their relationships. It is typically stored in a graph database, which natively stores the relationships between data entities. Entities in a knowledge graph can represent objects, events, situations, or concepts. The relationships between these entities capture the context and meaning of how they are connected.
Now that you understand how knowledge graphs organize and access data with context, let’s look at the building blocks of a knowledge graph data model. The definition of knowledge graphs varies depending on whom you ask, but we can distill the essence into three key components: nodes, relationships, and organizing principles. 
Nodes denote and store details about entities, such as people, places, objects, or institutions. Each node has a (or sometimes several) label to identify the node type and may optionally have one or more properties (attributes). Nodes are also sometimes called vertices.
Relationships link two nodes together: they show how the entities are related. Like nodes, each relationship has a label identifying the relationship type and may optionally have one or more properties. Relationships are also sometimes called edges. 
Organizing Principles are a framework, or schema, that organizes nodes and relationships according to fundamental concepts essential to the use cases at hand. Unlike many data designs, knowledge graphs easily incorporate multiple organizing principles.
In generative AI applications, knowledge graphs capture and organize key domain-specific or proprietary company information. Knowledge graphs are not limited to structured data; they can handle less organized data as well. 
In Fraud Detection and Analytics, the knowledge graph represents a network of transactions, their participants, and relevant information about them.
"
Classical NLP,Miscellaneous,What do you know about Latent Semantic Indexing (LSI)?,"LSI is a technique that analyzes a set of documents to find the statistical coexistence of words that appear together. It gives an insight into the topics of those documents.
LSI is also known as Latent Semantic Analysis.
"
Classical NLP,Miscellaneous,NLP Pipeline,"NLP Pipeline is a set of steps followed to build an end to end NLP software.
Before we started we have to remember this things pipeline is not universal, Deep Learning Pipelines are slightly different, and Pipeline is non-linear.
1.  Data Acquisition
In the data acquisition step, these three possible situations happen.
A. Public Dataset – If a public dataset is available for our problem statement.
B. Web Scrapping –  Scrapping competitor data using beautiful soup or other libraries
C. API – Using different APIs. eg. Rapid API
2. Text Preprocessing
So Our data collection step is done but we can not use this data for model building. we have to do text preprocessing. 
Steps –
1. Text Cleaning – In-text cleaning we do HTML tag removing, emoji handling, Spelling checker, etc
2. Basic Preprocessing — In basic preprocessing we do tokenization(word or sent tokenization, stop word removal, removing digit, lower casing.
3. Advance Preprocessing — In this step we do POS tagging, Parsing, and Coreference resolution.
3. Featured Engineering
Feature Engineering means converting text data to numerical data. but why it is required to convert text data to numerical data, because our machine learning model doesn’t understand text data then we have to do feature engineering. This step is also called Feature extraction from text.
"
Classical NLP,Miscellaneous,"For correcting spelling errors in a corpus, which one is a better choice: a giant dictionary or a smaller dictionary, and why?","Initially, a smaller dictionary is a better choice because most NLP researchers feared that a giant dictionary would contain rare words that may be similar to misspelled words. However, later it was found (Damerau and Mays (1989)) that in practice, a more extensive dictionary is better at marking rare words as errors.
"
Classical NLP,Miscellaneous,Do you always recommend removing punctuation marks from the corpus you’re dealing with? Why/Why not?,"No, it is not always a good idea to remove punctuation marks from the corpus as they are necessary for certain NLP applications that require the marks to be counted along with words.
For example: Part-of-speech tagging, parsing, speech synthesis.
"
Classical NLP,Miscellaneous,"
What is a hapax/hapax legomenon?","The rare words that only occur once in a sample text or corpus are called hapaxes. Each one of them is called an hapax or hapax legomenon (greek for ‘read-only once’). It is also called a singleton.
"
Classical NLP,Miscellaneous,What is a collocation?,"A collocation is a group of two or more words that possess a relationship and provide a classic alternative of saying something. For example, ‘strong breeze’, ‘the rich and powerful’, ‘weapons of mass destruction.
"
Classical NLP,Miscellaneous,What do you understand by regular expressions in NLP?,"Regular expressions in natural language processing are algebraic notations representing a set of strings. They are mainly used to find or replace strings in a text and can also be used to define a language in a formal way.
"
Data,Bias,List Sampling Techniques,"It is the practice of selecting an individual group from a population in order to study the whole population.
Probability Sampling Techniques is one of the important types of sampling techniques. Probability sampling allows every member of the population a chance to get selected. It is mainly used in quantitative research when you want to produce results representative of the whole population.
1. Simple Random Sampling
2. Systematic Sampling
In systematic sampling, every population is given a number as well like in simple random sampling. However, instead of randomly generating numbers, the samples are chosen at regular intervals.
3. Stratified Sampling
In stratified sampling, the population is subdivided into subgroups, called strata, based on some characteristics (age, gender, income, etc.). After forming a subgroup, you can then use random or systematic sampling to select a sample for each subgroup. This method allows you to draw more precise conclusions because it ensures that every subgroup is properly represented.
4. Cluster Sampling
In cluster sampling, the population is divided into subgroups, but each subgroup has similar characteristics to the whole sample. Instead of selecting a sample from each subgroup, you randomly select an entire subgroup. This method is helpful when dealing with large and diverse populations.
Non-Probability Sampling Techniques is one of the important types of Sampling techniques. In non-probability sampling, not every individual has a chance of being included in the sample. This sampling method is easier and cheaper but also has high risks of sampling bias. It is often used in exploratory and qualitative research with the aim to develop an initial understanding of the population.
1. Convenience Sampling
In this sampling method, the researcher simply selects the individuals which are most easily accessible to them. This is an easy way to gather data, but there is no way to tell if the sample is representative of the entire population. The only criteria involved is that people are available and willing to participate.
2. Voluntary Response Sampling
Voluntary response sampling is similar to convenience sampling, in the sense that the only criterion is people are willing to participate. However, instead of the researcher choosing the participants, the participants volunteer themselves. 
3. Purposive Sampling
In purposive sampling, the researcher uses their expertise and judgment to select a sample that they think is the best fit. It is often used when the population is very small and the researcher only wants to gain knowledge about a specific phenomenon rather than make statistical inferences.
4. Snowball Sampling
In snowball sampling, the research participants recruit other participants for the study. It is used when participants required for the research are hard to find. It is called snowball sampling because like a snowball, it picks up more participants along the way and gets larger and larger.
"
Data,Bias,Define and explain selection bias,"The selection bias occurs in the case when the researcher has to make a decision on which participant to study. The selection bias is associated with those researches when the participant selection is not random. The selection bias is also called the selection effect. The selection bias is caused by as a result of the method of sample collection.
Four types of selection bias are explained below:
Sampling Bias: As a result of a population that is not random at all, some members of a population have fewer chances of getting included than others, resulting in a biased sample. This causes a systematic error known as sampling bias.
Time interval: 
Trials may be stopped early if we reach any extreme value but if all variables are similar invariance, the variables with the highest variance have a higher chance of achieving the extreme value. Early termination of a trial at a time when its results support the desired conclusion.
Data: It is when specific data is selected arbitrarily and the generally agreed criteria are not followed. Cherry picking, which actually is not selection bias, but confirmation bias, when specific subsets of data are chosen to support a conclusion (e.g. citing examples of plane crashes as evidence of airline flight being unsafe, while ignoring the far more common example of flights that complete safely. See: Availability heuristic)
Attrition: Attrition in this context means the loss of the participants. It is the discounting of those subjects that did not complete the trial.
"
Data,Bias,Types of sampling bias,"Self-selection
Non-response 
Undercoverage
Survivorship
Pre-screening or advertising
Healthy user
"
Data,Bias,Imbalanced Training Data,"We now understand what class imbalance is and why it provides misleading classification accuracy.
1) Collect More Data
2) Try Changing Performance Metric
Accuracy is not the metric to use when working with an imbalanced dataset. We have seen that it is misleading.
There are metrics that have been designed to tell you a more truthful story when working with imbalanced classes.
I give more advice on selecting different performance measures in my post “Classification Accuracy is Not Enough: More Performance Measures You Can Use“.
In that post I look at an imbalanced dataset that characterizes the recurrence of breast cancer in patients.
From that post, I recommend looking at the following performance measures that can give more insight into the accuracy of the model than traditional classification accuracy:
Confusion Matrix: A breakdown of predictions into a table showing correct predictions (the diagonal) and the types of incorrect predictions made (what classes incorrect predictions were assigned).
Precision: A measure of a classifiers exactness.
Recall: A measure of a classifiers completeness
F1 Score (or F-score): A weighted average of precision and recall.
I would also advice you to take a look at the following:
Kappa (or Cohen’s kappa): Classification accuracy normalized by the imbalance of the classes in the data.
ROC Curves: Like precision and recall, accuracy is divided into sensitivity and specificity and models can be chosen based on the balance thresholds of these values.
You can learn a lot more about using ROC Curves to compare classification accuracy in our post “Assessing and Comparing Classifier Performance with ROC Curves“.
Still not sure? Start with kappa, it will give you a better idea of what is going on than classification accuracy.
3) Resampling Dataset
You can change the dataset that you use to build your predictive model to have more balanced data.
This change is called sampling your dataset and there are two main methods that you can use to even-up the classes:
You can add copies of instances from the under-represented class called over-sampling (or more formally sampling with replacement), or
You can delete instances from the over-represented class, called under-sampling.
These approaches are often very easy to implement and fast to run. They are an excellent starting point.
In fact, I would advise you to always try both approaches on all of your imbalanced datasets, just to see if it gives you a boost in your preferred accuracy measures.
You can learn a little more in the the Wikipedia article titled “Oversampling and undersampling in data analysis“.
Some Rules of Thumb
Consider testing under-sampling when you have an a lot data (tens- or hundreds of thousands of instances or more)
Consider testing over-sampling when you don’t have a lot of data (tens of thousands of records or less)
Consider testing random and non-random (e.g. stratified) sampling schemes.
Consider testing different resampled ratios (e.g. you don’t have to target a 1:1 ratio in a binary classification problem, try other ratios)
4) Try Generate Synthetic Samples
A simple way to generate synthetic samples is to randomly sample the attributes from instances in the minority class.
You could sample them empirically within your dataset or you could use a method like Naive Bayes that can sample each attribute independently when run in reverse. You will have more and different data, but the non-linear relationships between the attributes may not be preserved.
There are systematic algorithms that you can use to generate synthetic samples. The most popular of such algorithms is called SMOTE or the Synthetic Minority Over-sampling Technique.
5) Try Different Algorithms
As always, I strongly advice you to not use your favorite algorithm on every problem. You should at least be spot-checking a variety of different types of algorithms on a given problem.
6) Try Penalized Models
You can use the same algorithms but give them a different perspective on the problem.
Penalized classification imposes an additional cost on the model for making classification mistakes on the minority class during training. These penalties can bias the model to pay more attention to the minority class.
Now for handling class imbalance, you can use weighted Sigmoid Cross-Entropy loss. So you will penalize for wrong prediction based on the number/ratio of positive examples.
"
Data,Bias,Define and explain the concept of Inductive Bias with some examples,"Inductive Bias is a set of assumptions that humans use to predict outputs given inputs that the learning algorithm has not encountered yet. When we are trying to learn Y from X and the hypothesis space for Y is infinite, we need to reduce the scope by our beliefs/assumptions about the hypothesis space which is also called inductive bias. Through these assumptions, we constrain our hypothesis space and also get the capability to incrementally test and improve on the data using hyper-parameters. Examples:
We assume that Y varies linearly with X while applying Linear regression.
We assume that there exists a hyperplane separating negative and positive examples.
"
Data,Bias,What could be some issues if the distribution of the test data is significantly different than the distribution of the training data?,"The model that has high training accuracy might have low test accuracy. Without further knowledge, it is hard to know which dataset represents the population data and thus the generalizability of the algorithm is hard to measure. This should be mitigated by repeated splitting of train vs test dataset (as in cross validation).
When there is a change in data distribution, this is called the dataset shift. If the train and test data has a different distribution, then the classifier would likely overfit to the train data.
This issue can be overcome by using a more general learning method.
This can occur when:
P(y|x) are the same but P(x) are different. (covariate shift)
P(y|x) are different. (concept shift)
The causes can be:
Training samples are obtained in a biased way. (sample selection bias)
Train is different from test because of temporal, spatial changes. (non-stationary environments)
Solution to covariate shift
importance weighted cv
"
Data,Cross-Validation,What is Cross-Validation?,"Cross-Validation is a Statistical technique used for improving a model’s performance. Here, the model will be trained and tested with rotation using different samples of the training dataset to ensure that the model performs well for unknown data. The training data will be split into various groups and the model is run and validated against these groups in rotation.
The most commonly used techniques are:
K-Fold method
Leave p-out method
Leave-one-out method
Holdout method
"
Data,Cross-Validation,Hold-out cross-validation,"Hold-out cross-validation is the simplest and most common technique. You might not know that it is a hold-out method but you certainly use it every day.
The algorithm of hold-out technique:
Divide the dataset into two parts: the training set and the test set. Usually, 80% of the dataset goes to the training set and 20% to the test set but you may choose any splitting that suits you better
Train the model on the training set
Validate on the test set
Save the result of the validation
"
Data,Cross-Validation,k-Fold cross-validation,"k-Fold cross-validation is a technique that minimizes the disadvantages of the hold-out method. k-Fold introduces a new way of splitting the dataset which helps to overcome the “test only once bottleneck”.
The algorithm of the k-Fold technique:
Pick a number of folds – k. Usually, k is 5 or 10 but you can choose any number which is less than the dataset’s length.
Split the dataset into k equal (if possible) parts (they are called folds)
Choose k – 1 folds as the training set. The remaining fold will be the test set
Train the model on the training set. On each iteration of cross-validation, you must train a new model independently of the model trained on the previous iteration
Validate on the test set
Save the result of the validation
Repeat steps 3 – 6 k times. Each time use the remaining  fold as the test set. In the end, you should have validated the model on every fold that you have.
To get the final score average the results that you got on step 6.
To perform k-Fold cross-validation you can use sklearn.model_selection.KFold.
In general, it is always better to use k-Fold technique instead of hold-out. In a head to head, comparison k-Fold gives a more stable and trustworthy result since training and testing is performed on several different parts of the dataset. We can make the overall score even more robust if we increase the number of folds to test the model on many different sub-datasets.
Still, k-Fold method has a disadvantage. Increasing k results in training more models and the training process might be really expensive and time-consuming.
"
Data,Cross-Validation,Leave-one-out cross-validation,"Leave-one-out сross-validation (LOOCV) is an extreme case of k-Fold CV. Imagine if k is equal to n where n is the number of samples in the dataset. Such k-Fold case is equivalent to Leave-one-out technique.

The algorithm of LOOCV technique:
Choose one sample from the dataset which will be the test set
The remaining n – 1 samples will be the training set
Train the model on the training set. On each iteration, a new model must be trained
Validate on the test set
Save the result of the validation
Repeat steps 1 – 5 n times as for n samples we have n different training and test sets
To get the final score average the results that you got on step 5.
For LOOCV sklearn also has a built-in method. It can be found in the model_selection library – sklearn.model_selection.LeaveOneOut.
The greatest advantage of Leave-one-out cross-validation is that it doesn’t waste much data. We use only one sample from the whole dataset as a test set, whereas the rest is the training set. But when compared with k-Fold CV, LOOCV requires building n models instead of k models, when we know that n which stands for the number of samples in the dataset is much higher than k. It means LOOCV is more computationally expensive than k-Fold, it may take plenty of time to cross-validate the model using LOOCV.
Thus, the Data Science community has a general rule based on empirical evidence and different researches, which suggests that 5- or 10-fold cross-validation should be preferred over LOOCV.
"
Data,Cross-Validation,Leave-p-out cross-validation,"Leave-p-out cross-validation (LpOC) is similar to Leave-one-out CV as it creates all the possible training and test sets by using p samples as the test set. All mentioned about LOOCV is true and for LpOC.
Still, it is worth mentioning that unlike LOOCV and k-Fold test sets will overlap for LpOC if p is higher than 1.
The algorithm of LpOC technique:
Choose p samples from the dataset which will be the test set
The remaining n – p samples will be the training set
Train the model on the training set. On each iteration, a new model must be trained
Validate on the test set
Save the result of the validation
Repeat steps 2 – 5 Cpn times 
To get the final score average the results that you got on step 5
You can perform Leave-p-out CV using sklearn – sklearn.model_selection.LeavePOut.
"
Data,Cross-Validation,Stratified k-Fold cross-validation,"Sometimes we may face a large imbalance of the target value in the dataset. For example, in a dataset concerning wristwatch prices, there might be a larger number of wristwatch having a high price. In the case of classification, in cats and dogs dataset there might be a large shift towards the dog class.
Stratified k-Fold is a variation of the standard k-Fold CV technique which is designed to be effective in such cases of target imbalance. 
It works as follows. Stratified k-Fold splits the dataset on k folds such that each fold contains approximately the same percentage of samples of each target class as the complete set. In the case of regression, Stratified k-Fold makes sure that the mean target value is approximately equal in all the folds.
The algorithm of Stratified k-Fold technique:
Pick a number of folds – k
Split the dataset into k folds. Each fold must contain approximately the same percentage of samples of each target class as the complete set 
Choose k – 1 folds which will be the training set. The remaining fold will be the test set
Train the model on the training set. On each iteration a new model must be trained
Validate on the test set
Save the result of the validation
Repeat steps 3 – 6 k times. Each time use the remaining  fold as the test set. In the end, you should have validated the model on every fold that you have.
To get the final score average the results that you got on step 6.
As you may have noticed, the algorithm for Stratified k-Fold technique is similar to the standard k-Folds. You don’t need to code something additionally as the method will do everything necessary for you.
Stratified k-Fold also has a built-in method in sklearn – sklearn.model_selection.StratifiedKFold.
All mentioned above about k-Fold CV is true for Stratified k-Fold technique. When choosing between different CV methods, make sure you are using the proper one. For example, you might think that your model performs badly simply because you are using k-Fold CV to validate the model which was trained on the dataset with a class imbalance. To avoid that you should always do a proper exploratory data analysis on your data.
"
Data,Cross-Validation,Repeated k-Fold cross-validation,"Repeated k-Fold cross-validation or Repeated random sub-sampling CV is probably the most robust of all CV techniques in this paper. It is a variation of k-Fold but in the case of Repeated k-Folds k is not the number of folds. It is the number of times we will train the model.
The general idea is that on every iteration we will randomly select samples all over the dataset as our test set. For example, if we decide that 20% of the dataset will be our test set, 20% of samples will be randomly selected and the rest 80% will become the training set. 
The algorithm of Repeated k-Fold technique:
Pick k – number of times the model will be trained
Pick a number of samples which will be the test set
Split the dataset
Train on the training set. On each iteration of cross-validation, a new model must be trained
Validate on the test set
Save the result of the validation
Repeat steps 3-6 k times
To get the final score average the results that you got on step 6.
Repeated k-Fold has clear advantages over standard k-Fold CV. Firstly, the proportion of train/test split is not dependent on the number of iterations. Secondly, we can even set unique proportions for every iteration. Thirdly, random selection of samples from the dataset makes Repeated k-Fold even more robust to selection bias.
Still, there are some disadvantages. k-Fold CV guarantees that the model will be tested on all samples, whereas Repeated k-Fold is based on randomization which means that some samples may never be selected to be in the test set at all. At the same time, some samples might be selected multiple times. Thus making it a bad choice for imbalanced datasets.
Sklearn will help you to implement a Repeated k-Fold CV. Just use sklearn.model_selection.RepeatedKFold. 
"
Data,Cross-Validation,Nested k-Fold,"Unlike the other CV techniques, which are designed to evaluate the quality of an algorithm, Nested k-fold CV is used to train a model in which hyperparameters also need to be optimized. It estimates the generalization error of the underlying model and its (hyper)parameter search.
The algorithm of Nested k-Fold technique:
Define set of hyper-parameter combinations, C, for current model. If model has no hyper-parameters, C is the empty set.
Divide data into K folds with approximately equal distribution of cases and controls.
(outer loop) For fold k, in the K folds:
Set fold k, as the test set.
Perform automated feature selection on the remaining K-1 folds.
For parameter combination c in C:
(inner loop) For fold k, in the remaining K-1 folds:
Set fold k, as the validation set.
Train model on remaining K-2 folds.
Evaluate model performance on fold k.
Calculate average performance over K-2 folds for parameter combination c.
Train model on K-1 folds using hyper-parameter combination that yielded best average performance over all steps of the inner loop.
 Evaluate model performance on fold k.
Calculate average performance over K folds.
The inner loop performs cross-validation to identify the best features and model hyper-parameters using the k-1 data folds available at each iteration of the outer loop. The model is trained once for each outer loop step and evaluated on the held-out data fold. This process yields k evaluations of the model performance, one for each data fold, and allows the model to be tested on every sample.
It is to be noted that this technique is computationally expensive because plenty of models is trained and evaluated. Unfortunately, there is no built-in method in sklearn that would perform Nested k-Fold CV for you.
"
Data,Cross-Validation,Time-series cross-validation,"Traditional cross-validation techniques don’t work on sequential data such as time-series because we cannot choose random data points and assign them to either the test set or the train set as it makes no sense to use the values from the future to forecast values in the past. There are mainly two ways to go about this:
Rolling cross-validation
Cross-validation is done on a rolling basis i.e. starting with a small subset of data for training purposes, predicting the future values, and then checking the accuracy on the forecasted data points. The following image can help you get the intuition behind this approach.
Blocked cross-validation
The first technique may introduce leakage from future data to the model. The model will observe future patterns to forecast and try to memorize them. That’s why blocked cross-validation was introduced. 
It works by adding margins at two positions. The first is between the training and validation folds in order to prevent the model from observing lag values which are used twice, once as a regressor and another as a response. The second is between the folds used at each iteration in order to prevent the model from memorizing patterns from one iteration to the next.
"
Data,Cross-Validation,How do we choose K in K-fold cross-validation? What’s your favorite K? ,"There are two things to consider while deciding K: the number of models we get and the size of validation set. We do not want the number of models to be too less, like 2 or 3. At least 4 models give a less biased decision on the metrics. On the other hand, we would want the dataset to be at least 20-25% of the entire data. So that at least a ratio of 3:1 between training and validation set is maintained.
I tend to use 4 for small datasets and 5 for large ones as K.
"
Data,Cross-Validation,"Is it possible to test for the probability of improving model accuracy without cross-validation techniques? If yes, please explain.","Yes, it is possible to test for the probability of improving model accuracy without cross-validation techniques. We can do so by running the ML model for say n number of iterations, recording the accuracy. Plot all the accuracies and remove the 5% of low probability values. Measure the left [low] cut off and right [high] cut off. With the remaining 95% confidence, we can say that the model can go as low or as high [as mentioned within cut off points]. 
"
Data,Outlier Detection,Outlier Detection Techniques ,"Outliers are those observations that differ strongly(different properties) from the other data points in the sample of a population.
Most popular outlier detection techniques are:
1. Z-Score
2. Local Outlier Factor (LOF)
3. Isolation Forest
4. DBSCAN
5. Coresets
"
Data,Outlier Detection,What are the possible sources of outliers in a dataset?,"There are multiple reasons why there can be outliers in the dataset, like Human errors (Wrong data entry), Measurement errors(System/Tool error), Data manipulation error(Faulty data preprocessing error), Sampling errors(creating samples from heterogeneous sources), etc. Importantly, detecting and treating these Outliers is important for learning a robust and generalizable machine learning system.
"
Data,Outlier Detection,Z-Score for Outlier Detection,"The Z-score(also called the standard score) is an important concept in statistics that indicates how far away a certain point is from the mean. By applying Z-transformation we shift the distribution and make it 0 mean with unit standard deviation. For example — A Z-score of 2 would mean the data point is 2 standard deviation away from the mean.
Z-score(i) = (x(i) -mean) / standard deviation
It assumes that the data is normally distributed and hence the % of data points that lie between -/+1 stdev. is ~68%, -/+2 stdev. is ~95% and -/+3 stdev. is ~99.7%. Hence, if the Z-score is >3 we can safely mark that point to be an outlier. Refer to below fig.
"
Data,Outlier Detection,Interquartile Range (IQR): ,"IQR, the concept used to build boxplots, can also be used to identify outliers. The IQR is equal to the difference between the 3rd quartile and the 1st quartile. You can then identify if a point is an outlier if it is less than Q1–1.5*IRQ or greater than Q3 + 1.5*IQR. This comes to approximately 2.698 standard deviations.
"
Data,Outlier Detection,Local Outlier Factor  (LOF),"In Local Outlier Factor (LOF), the idea revolves around the concept of local regions. Here, we calculate and compare the local density of the focus point with the local density of its neighbours. If we find that the local density of the focus point is very low compared to its neighbours, that would kind of hint that the focus point is isolated in that space and is a potential outlier. The algorithm depends on the hyperparameter K, which decides upon the number of neighbours to consider when calculating the local density. This value is bounded between 0 (no neighbour) and the total points (all points being neighbour) in the space.
The local density function is defined as the reciprocal of average reachability distance, where, average reachability distance is defined as the average distance from the focus point to all points in the neighbour.
LOF = average local density of neighbors / local density of focus point
If,
LOF ≈ 1 similar density as neighbors
LOF < 1 higher density than neighbors (normal point)
LOF > 1 lower density than neighbors (anomaly)
"
Data,Outlier Detection,Isolation Forest,"Isolation Forest is a tree-based algorithm that tries to find out outliers based on the concept of decision boundaries(just like we have for decision trees). The idea over here is to keep splitting the data at random thresholds and feature till every point gets isolated(it’s like overfitting a decision tree on a dataset). Once the isolation is achieved we chunk out points that got isolated pretty early during this process. And we mark these points as potential outliers. If you see this intuitively, the farther a point is from the majority, the easier it gets to isolate, whereas, isolating the points that are part of a group would require more cuts to isolate every point.
"
Data,Outlier Detection,Autoencoder for Outlier Detection,"Autoencoders are Neural network architectures that are trained to reproduce the input itself. It consists of two trainable components namely — Encoder and Decoder. Where the goal of the encoder is to learn a latent representation of the input(original dimension to low dimension) and the goal of the decoder is to learn to reconstruct the input from this latent representation(low dimension to original dimension). So for the autoencoder to work well, both of these components should optimize on their respective tasks.
Autoencoders are widely used for detecting anomalies. A typical intuition behind how this works is that if a point in feature space lies far away from the majority of the points(meaning it holds different properties, for example — dog images clustered around a certain part of the feature space and cow image lies pretty far from that cluster), in such cases, the autoencoder learns the dog distribution (because of the count of dog images would be very high compared to cows — that’s why it’s an anomaly, hence model would majorly focus on learning the dog cluster). This means, the model would be able to more or less correctly re-generate the dog images leading to low loss values, whereas, for the cow image it would generate high loss(because that’s something odd it saw for the first time and the weights it has learnt is mostly to reconstruct dog images). We use these reconstruction loss values as the anomaly scores, so higher the scores, the higher the chances of input being an anomaly.
"
Data,Outlier Detection,Outlier Detection using In-degree Number (ODIN),"In Outlier Detection using In-degree Number (ODIN), we calculate the in-degree for each of the data points. Here, in-degree is defined as the number of nearest neighbour sets to this point belongs. Higher this value, the more the confidence of this point belonging to some dense region in the space. Whereas, on the other side, a lesser value of this would mean that it’s not part of many nearest neighbour sets and is kind of isolated in the space. You can think of this method to be the reverse of KNN.
"
Data,Outlier Detection,Dealing with outliers,"Univariate method: This method looks for data points with extreme values on one variable.
Multivariate method: Here, we look for unusual combinations of all the variables.
Minkowski error: This method reduces the contribution of potential outliers in the training process.
"
Data,Dimensionality Reduction,The Curse of Dimensionality,"If we have more features than observations than we run the risk  of massively overfitting our model — this would generally result in terrible out of sample performance.
When we have too many features, observations become harder to cluster — believe it or not, too many dimensions causes every observation in your dataset to appear equidistant from all the others. And because clustering uses a distance measure such as Euclidean distance to quantify the similarity between observations, this is a big problem. If the distances are all approximately equal, then all the observations appear equally alike (as well as equally different), and no meaningful clusters can be formed.
All samples are close to the edge of the sample. And this is a bad news because prediction is much more difficult near the edges of the training sample.
The sampling density decreases exponentially as p increases and hence the data becomes much more sparse without significantly more data. 
We should conduct PCA to reduce dimensionality
The curse of dimensionality, first introduced by Bellman [1], indicates that the number of samples needed to estimate an arbitrary function with a given level of accuracy grows exponentially with respect to the number of input variables (i.e., dimensionality) of the function.
"
Data,Dimensionality Reduction,What is the importance of dimensionality reduction?,"The process of dimensionality reduction constitutes reducing the number of features in a dataset to avoid overfitting and reduce the variance. There are mostly 4 advantages of this process:
This reduces the storage space and time for model execution.
Removes the issue of multi-collinearity thereby improving the parameter interpretation of the ML model.
Makes it easier for visualizing data when the dimensions are reduced.
Avoids the curse of increased dimensionality. 
"
Data,Dimensionality Reduction,Methods of dimensionality reduction,"Feature Selection Methods
Matrix Factorization
Manifold Learning
Autoencoder Methods
Linear Discriminant Analysis (LDA)
Principal component analysis (PCA)
"
Data,Dimensionality Reduction,If a weight for one variable is higher than for another  —  can we say that this variable is more important? ,"Yes - if your predictor variables are normalized.
Without normalization, the weight represents the change in the output per unit change in the predictor. If you have a predictor with a huge range and scale that is used to predict an output with a very small range - for example, using each nation's GDP to predict maternal mortality rates - your coefficient should be very small. That does not necessarily mean that this predictor variable is not important compared to the others.
"
Data,Dimensionality Reduction,Feature selection methods ,"The main benefits of performing feature selection in advance, rather than letting the machine learning model figure out which features are most important, include:
simpler models: simple models are easy to explain - a model that is too complex and unexplainable is not valuable
shorter training times: a more precise subset of features decreases the amount of time needed to train a model
variance reduction: increase the precision of the estimates that can be obtained for a given simulation 
avoid the curse of high dimensionality: dimensionally cursed phenomena states that, as dimensionality and the number of features increases, the volume of space increases so fast that the available data become limited - PCA feature selection may be used to reduce dimensionality ,
Filter Methods:
Filter methods select features based on statistics rather than feature selection cross-validation performance. A selected metric is applied to identify irrelevant attributes and perform recursive feature selection. Filter methods are either univariate, in which an ordered ranking list of features is established to inform the final selection of feature subset; or multivariate, which evaluates the relevance of the features as a whole, identifying redundant and irrelevant features.
There are various filter methods such as the Chi-Square test, Fisher’s Score method, Correlation Coefficient, Variance Threshold, Mean Absolute Difference (MAD) method, Dispersion Ratios, etc.
Wrapper Methods:
Wrapper feature selection methods consider the selection of a set of features as a search problem, whereby their quality is assessed with the preparation, evaluation, and comparison of a combination of features to other combinations of features. This method facilitates the detection of possible interactions amongst variables. Wrapper methods focus on feature subsets that will help improve the quality of the results of the clustering algorithm used for the selection. 
There are three types of wrapper methods, they are:
Forward Selection: Here, one feature is tested at a time and new features are added until a good fit is obtained.
Backward Selection: Here, all the features are tested and the non-fitting ones are eliminated one by one to see while checking which works better.
Recursive Feature Elimination: The features are recursively checked and evaluated how well they perform.
These methods are generally computationally intensive and require high-end resources for analysis. But these methods usually lead to better predictive models having higher accuracy than filter methods.
Embedded Methods:
Embedded feature selection methods integrate the feature selection machine learning algorithm as part of the learning algorithm, in which classification and feature selection are performed simultaneously. The features that will contribute the most to each iteration of the model training process are carefully extracted. Random forest feature selection, decision tree feature selection, and LASSO feature selection are common embedded methods.
"
Data,Normalization,Data Normalization,"There are different types of data normalization. Assume you have a dataset X, which has N rows(entries) and D columns(features). X[:,i] represent feature i and X[j,:] represent entry j. We have:
Z Normalization(Standardization):
I used to falsely think this method somehow yields a standard Gaussian result. In fact, standardization does not change the type of distribution:
This transformation sets the mean of data to 0 and the standard deviation to 1. In most cases, standardization is used feature-wise
Min-Max Normalization:
This method rescales the range of the data to [0,1]. In most cases, standardization is used feature-wise as well
StandardScaling and MinMax Scaling have similar applications and are often more or less interchangeable. However, if the algorithm involves the calculation of distances between points or vectors, the default choice is StandardScaling. But MinMax Scaling is useful for visualization by bringing features within the interval (0, 255).
Unit Vector Normalization:
Scaling to unit length shrinks/stretches a vector (a row of data can be viewed as a D-dimensional vector) to a unit sphere. When used on the entire dataset, the transformed data can be visualized as a bunch of vectors with different directions on the D-dimensional unit sphere.
"
Data,Normalization,Why do we need Normalization?,"Monotonic feature transformation is critical for some algorithms and has no effect on others. This is one of the reasons for the increased popularity of decision trees and all its derivative algorithms (random forest, gradient boosting). Not everyone can or want to tinker with transformations, and these algorithms are robust to unusual distributions.
There are also purely engineering reasons: np.log is a way of dealing with large numbers that do not fit in np.float64. This is an exception rather than a rule; often it’s driven by the desire to adapt the dataset to the requirements of the algorithm. Parametric methods usually require the data distribution to be at least symmetric and unimodal, which is not always the case.
However, data requirements are imposed not only by parametric methods; K nearest neighbors will predict complete nonsense if features are not normalized e.g. when one distribution is located in the vicinity of zero and does not go beyond (-1, 1) while the other’s range is on the order of hundreds of thousands.
a). Standardization improves the numerical stability of your model
b). Standardization may speed up the training process
A corollary to the first ‘theorem’ is that if different features have drastically different ranges, the learning rate is determined by the feature with the largest range. This leads to another advantage of standardization: speeds up the training process.
Standardization gives ‘equal’ considerations for each feature.
Standardization is beneficial in many cases. It improves the numerical stability of the model and often reduces training time. However, standardization isn’t always great. It can harm the performance of distance-based clustering algorithms by assuming equal importance of features. If there are inherent importance differences between features, it’s generally not a good idea to do standardization.
"
Data,Normalization,Log-normal distribution,"In probability theory, a log-normal (or lognormal) distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed. 
If we assume that some data is not normally distributed but is described by the log-normal distribution, it can easily be transformed to a normal distribution. The lognormal distribution is suitable for describing salaries, price of securities, urban population, number of comments on articles on the internet, etc. However, to apply this procedure, the underlying distribution does not necessarily have to be lognormal; you can try to apply this transformation to any distribution with a heavy right tail. Furthermore, one can try to use other similar transformations, formulating their own hypotheses on how to approximate the available distribution to a normal. Examples of such transformations are Box-Cox transformation (logarithm is a special case of the Box-Cox transformation) or Yeo-Johnson transformation (extends the range of applicability to negative numbers). In addition, you can also try adding a constant to the feature — np.log (x + const).
"
Data,Normalization,What is a Box-Cox Transformation?,"The dependent variable for a regression analysis might not satisfy one or more assumptions of an ordinary least squares regression. The residuals could either curve as the prediction increases or follow the skewed distribution. In such scenarios, it is necessary to transform the response variable so that the data meets the required assumptions. A Box cox transformation is a statistical technique to transform non-normal dependent variables into a normal shape. If the given data is not normal then most of the statistical techniques assume normality. Applying a box cox transformation means that you can run a broader number of tests.
"
Data,Normalization,How to Test for Normality,"Q Q plot compares two different distributions. If the two sets of data came from the same distribution, the points will fall on a 45 degree reference line. To use this type of graph for the assumption of normality, compare your data to data from a distribution with known normality.
Boxplot.
Draw a boxplot of your data. If your data comes from a normal distribution, the box will be symmetrical with the mean and median in the center. If the data meets the assumption of normality, there should also be few outliers.
Normal Probability Plot.
The normal probability plot was designed specifically to test for the assumption of normality. If your data comes from a normal distribution, the points on the graph will form a line.
Histogram.
The popular histogram can give you a good idea about whether your data meets the assumption. If your data looks like a bell curve: then it’s probably normal.
Check Skewness and Kurtosis of the sampled data. Skewness = 0 and kurtosis = 3 are typical for a normal distribution, so the farther away they are from these values, the more non-normal the distribution.
More in Statistics
"
Data,Miscellaneous,Ways to Compensate for Missing Values In a Dataset (Data Imputation),"Many real-world datasets may contain missing values for various reasons. They are often encoded as NaNs, blanks or any other placeholders. Training a model with a dataset that has a lot of missing values can drastically impact the machine learning model’s quality. Some algorithms such as scikit-learn estimators assume that all values are numerical and have and hold meaningful value.
One way to handle this problem is to get rid of the observations that have missing data. However, you will risk losing data points with valuable information. A better strategy would be to impute the missing values. In other words, we need to infer those missing values from the existing part of the data. There are three main types of missing data:
Missing completely at random (MCAR)
Missing at random (MAR)
Not missing at random (NMAR)
However, in this article, I will focus on 6 popular ways for data imputation for cross-sectional datasets ( Time-series dataset is a different story ).
1- Do Nothing:
That’s an easy one. You just let the algorithm handle the missing data. Some algorithms can factor in the missing values and learn the best imputation values for the missing data based on the training loss reduction (ie. XGBoost). Some others have the option to just ignore them (ie. LightGBM — use_missing=false). However, other algorithms will panic and throw an error complaining about the missing values (ie. Scikit learn — LinearRegression). In that case, you will need to handle the missing data and clean it before feeding it to the algorithm.
Let’s see some other ways to impute the missing values before training:
2- Imputation Using (Mean/Median) Values:
This works by calculating the mean/median of the non-missing values in a column and then replacing the missing values within each column separately and independently from the others. It can only be used with numeric data.
Pros:
Easy and fast.
Works well with small numerical datasets.
Cons:
Doesn’t factor the correlations between features. It only works on the column level.
Will give poor results on encoded categorical features (do NOT use it on categorical features).
Not very accurate.
Doesn’t account for the uncertainty in the imputations.
3- Imputation Using (Most Frequent) or (Zero/Constant) Values:
Most Frequent is another statistical strategy to impute missing values and YES!! It works with categorical features (strings or numerical representations) by replacing missing data with the most frequent values within each column.
Pros:
Works well with categorical features.
Cons:
It also doesn’t factor the correlations between features.
It can introduce bias in the data.Zero or Constant imputation — as the name suggests — it replaces the missing values with either zero or any constant value you specify
4- Imputation Using k-NN:
The k nearest neighbours is an algorithm that is used for simple classification. The algorithm uses ‘feature similarity’ to predict the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set. This can be very useful in making predictions about the missing values by finding the k’s closest neighbours to the observation with missing data and then imputing them based on the non-missing values in the neighbourhood. Let’s see some example code using Impyute library which provides a simple and easy way to use KNN for imputation:
How does it work?
It creates a basic mean impute then uses the resulting complete list to construct a KDTree. Then, it uses the resulting KDTree to compute nearest neighbours (NN). After it finds the k-NNs, it takes the weighted average of them.
Pros:
Can be much more accurate than the mean, median or most frequent imputation methods (It depends on the dataset).
Cons:
Computationally expensive. KNN works by storing the whole training dataset in memory.
K-NN is quite sensitive to outliers in the data (unlike SVM)
5- Imputation Using Multivariate Imputation by Chained Equation (MICE)
This type of imputation works by filling the missing data multiple times. Multiple Imputations (MIs) are much better than a single imputation as it measures the uncertainty of the missing values in a better way. The chained equations approach is also very flexible and can handle different variables of different data types (ie., continuous or binary) as well as complexities such as bounds or survey skip patterns. For more information on the algorithm mechanics, you can refer to the Research Paper
6- Imputation Using Deep Learning (Datawig):
This method works very well with categorical and non-numerical features. It is a library that learns Machine Learning models using Deep Neural Networks to impute missing values in a dataframe. It also supports both CPU and GPU for training.
Imputation using Datawig
Pros:
Quite accurate compared to other methods.
It has some functions that can handle categorical data (Feature Encoder).
It supports CPUs and GPUs.
Cons:
Single Column imputation.
Can be quite slow with large datasets.
You have to specify the columns that contain information about the target column that will be imputed.
Other Imputation Methods:
Stochastic regression imputation:
It is quite similar to regression imputation which tries to predict the missing values by regressing it from other related variables in the same dataset plus some random residual value.
Extrapolation and Interpolation:
It tries to estimate values from other observations within the range of a discrete set of known data points.
Hot-Deck imputation:
Works by randomly choosing the missing value from a set of related and similar variables.
In conclusion, there is no perfect way to compensate for the missing values in a dataset. Each strategy can perform better for certain datasets and missing data types but may perform much worse on other types of datasets. There are some set rules to decide which strategy to use for particular types of missing values, but beyond that, you should experiment and check which model works best for your dataset.
"
Data,Miscellaneous,"You run your regression on different subsets of your data, and find that in each subset, the beta value for a certain variable varies wildly. What could be the issue here?","The dataset might be heterogeneous. In which case, it is recommended to cluster datasets into different subsets wisely, and then draw different models for different subsets. Or, use models like non parametric models (trees) which can deal with heterogeneity quite nicely.
Heterogeneous data are any data with high variability of data types and formats. They are possibly ambiguous and low quality due to missing values, high data redundancy, and untruthfulness.

"
Data,Miscellaneous,Is more data always better?,"Statistically,
It depends on the quality of your data, for example, if your data is biased, just getting more data won’t help.
It depends on your model. If your model suffers from high bias, getting more data won’t improve your test results beyond a point. You’d need to add more features, etc.
Practically
Also there’s a tradeoff between having more data and the additional storage, computational power, memory it requires. Hence, always think about the cost of having more data.
"
Data,Miscellaneous,Ways to Deal with the Lack of Data in Machine Learning ,"Use simpler model
Use ensemble methods
In general, the simpler the machine learning algorithm, the better it will learn from small data sets. From an ML perspective, small data requires models that have low complexity (or high bias) to avoid overfitting the model to the data. I noticed that the Naive Bayes algorithm is among the simplest classifiers and as a result learns remarkably well from relatively small data sets.
Transfer learning
Data Augmentation
Synthetic Data
"
Data,Miscellaneous,What are advantages of plotting your data before performing analysis?,"Data sets have errors.  You won't find them all but you might find some. That 212 year old man. That 9 foot tall woman.
Variables can have skewness, outliers etc.  Then the arithmetic mean might not be useful. Which means the standard deviation isn't useful.
Variables can be multimodal!  If a variable is multimodal then anything based on its mean or median is going to be suspect. 
"
Data,Miscellaneous,How can you determine which features are the most important in your model?,"run the features though a Gradient Boosting Machine or Random Forest to generate plots of relative importance and information gain for each feature in the ensembles.
Look at the variables added in forward variable selection 
"
Data,Miscellaneous,Define confounding variables.,"Confounding variables are also known as confounders. These variables are a type of extraneous variables that influence both independent and dependent variables causing spurious association and mathematical relationships between those variables that are associated but are not casually related to each other
"
Data,Miscellaneous,What if we want to build a model for predicting prices? Are prices distributed normally? Do we need to do any pre-processing for prices? ,"Data is not normal. Specially, real-world datasets or uncleaned datasets always have certain skewness. Same goes for the price prediction. Price of houses or any other thing under consideration depends on a number of factors. So, there's a great chance of presence of some skewed values i.e outliers if we talk in data science terms.
Yes, you may need to do pre-processing. Most probably, you will need to remove the outliers to make your distribution near-to-normal.
"
Data,Miscellaneous,What do you mean by Associative Rule Mining (ARM)?,"Associative Rule Mining is one of the techniques to discover patterns in data like features (dimensions) which occur together and features (dimensions) which are correlated. It is mostly used in Market-based Analysis to find how frequently an itemset occurs in a transaction. Association rules have to satisfy minimum support and minimum confidence at the very same time. Association rule generation generally comprised of two different steps:
“A min support threshold is given to obtain all frequent item-sets in a database.”
“A min confidence constraint is given to these frequent item-sets in order to form the association rules.”
Support is a measure of how often the “item set” appears in the data set and Confidence is a measure of how often a particular rule has been found to be true.
"
Data,Miscellaneous,What is the significance of using the Fourier transform in Deep Learning tasks?,"The Fourier transform function efficiently analyzes, maintains, and manages large datasets. You can use it to generate real-time array data that is helpful for processing multiple signals.
If the matrices of the input and filters in the CNN can be converted into the frequency domain to perform the multiplication and the outcome matrices of the multiplication in the frequency domain can be converted into the time domain will not perform any harm to the accuracy of the model. The conversion of matrices from the time domain to the frequency domain can be done by the Fourier transform or fast Fourier transform and conversion from the frequency domain to the time domain can be done by the inverse Fourier transform or inverse fast Fourier transform. 
"
Deep Learning,Optimizers,What is an optimizer?,"Optimizers are algorithms or methods used to minimize an error function (loss function) or to maximize the efficiency of production. Optimizers are mathematical functions which are dependent on model’s learnable parameters i.e Weights & Biases. Optimizers help to know how to change weights and learning rate of neural network to reduce the losses.
"
Deep Learning,Optimizers,List Optimizer Types,"Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent, SGD with Momentum, Nesterov Accelerated Gradient, RMS-Prop, AdaGrad(Adaptive Gradient Descent), AdaDelta, Adam
"
Deep Learning,Optimizers,Gradient Descent,"We now consider the problem of solving for the minimum of a real-valued function min x f(x), where f : Rd → R is an objective function that captures the machine learning problem at hand. We assume that our function f is differentiable, and we are unable to analytically find a solution in closed form. Gradient descent is a first-order optimization algorithm. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point. Recall that the gradient points in the direction of steepest ascent.
Let us consider multivariate functions. Imagine a surface (described by the function f(x)) with a ball starting at a particular location x0. When the ball is released, it will move downhill in the direction of steepest descent. Gradient descent exploits the fact that f(x0) decreases fastest if one moves from x0 in the direction of the negative gradient −((∇f)(x0))⊤ of f at x0. We assume in this book that the functions are differentiable, and refer the reader to more general settings in Section 7.4. Then, if x1 = x0 − γ((∇f)(x0))⊤ (7.5) for a small step-size γ ⩾ 0, then f(x1) ⩽ f(x0). Note that we use the transpose for the gradient since otherwise the dimensions will not work out. This observation allows us to define a simple gradient descent algorithm: If we want to find a local optimum f(x∗) of a function f : Rn → R, x 7→ f(x), we start with an initial guess x0 of the parameters we wish to optimize and then iterate according to xi+1 = xi − γi((∇f)(xi))⊤ . (7.6) For suitable step-size γi , the sequence f(x0) ⩾ f(x1) ⩾ . . . converges to a local minimum.
In summary, Gradient Descent method’s steps are:
choose a starting point (initialisation)
calculate gradient at this point
make a scaled step in the opposite direction to the gradient (objective: minimise)
repeat points 2 and 3 until one of the criteria is met:
maximum number of iterations reached
step size is smaller than the tolerance.
Below there’s an exemplary implementation of the Gradient Descent algorithm (with steps tracking):
This function takes 5 parameters:
1. starting point - in our case, we define it manually but in practice, it is often a random initialisation
2. gradient function - has to be specified before-hand
3. learning rate - scaling factor for step sizes
4. maximum number of iterations
5. tolerance to conditionally stop the algorithm (in this case a default value is 0.01)
Advantages of Gradient Descent
Easy to understand
Easy to implement
Disadvantages of Gradient Descent
Because this method calculates the gradient for the entire data set in one update, the calculation is very slow.
It requires large memory and it is computationally expensive.
"
Deep Learning,Optimizers,Stochastic Gradient Descent,"It is a variant of Gradient Descent. If the model has 10K dataset SGD will update the model parameters 10k times.
Stochastic Gradient Descent
Advantages of Stochastic Gradient Descent
Frequent updates of model parameter
Requires less Memory.
Allows the use of large data sets as it has to update only one example at a time.
Disadvantages of Stochastic Gradient Descent
The frequent can also result in noisy gradients which may cause the error to increase instead of decreasing it.
High Variance.
Frequent updates are computationally expensive.
"
Deep Learning,Optimizers,Mini-Batch Gradient Descent,"It is a combination of the concepts of SGD and batch gradient descent. It simply splits the training dataset into small batches and performs an update for each of those batches. This creates a balance between the robustness of stochastic gradient descent and the efficiency of batch gradient descent. it can reduce the variance when the parameters are updated, and the convergence is more stable. It splits the data set in batches in between 50 to 256 examples, chosen at random.
Mini Batch Gradient Descent
Advantages of Mini Batch Gradient Descent:
It leads to more stable convergence.
more efficient gradient calculations.
Requires less amount of memory.
Disadvantages of Mini Batch Gradient Descent
Mini-batch gradient descent does not guarantee good convergence,
If the learning rate is too small, the convergence rate will be slow. If it is too large, the loss function will oscillate or even deviate at the minimum value.
"
Deep Learning,Optimizers,How does a batch size influence a model?,"Increasing batch size drops the learners' ability to generalize. The idea is that smaller batches are more likely to push out local minima and find the Global Minima.
large batch size means the model makes very large gradient updates and very small gradient updates. The size of the update depends heavily on which particular samples are drawn from the dataset. On the other hand using small batch size means the model makes updates that are all about the same size. The size of the update only weakly depends on which particular samples are drawn from the dataset
Large Batch Training methods tend to overfit compared to the same network trained with smaller batch size.
Large Batch Training methods tend to get trapped or even attracted to potential saddle points in the loss landscape.
Large Batch Training methods tend to zoom in on the closest relative minima that it finds, whereas networks trained with a smaller batch size tend to “explore” the loss landscape before settling on a promising minimum.
Large Batch Training methods tend to converge to completely “different” minima points than networks trained with smaller batch sizes.
Furthermore, the authors tackled the Generalization Gap from the perspective of how Neural Networks navigate the loss landscape during training. Training with a relatively large batch size tends to converge to sharp minimizers, while reducing the batch size usually leads to falling into flat minimizers. A sharp minimizer can be thought of as a narrow and steep ravine, whereas a flat minimizer is analogous to a valley in a vast landscape of low and mild hill terrains. To phrase it in more rigorous terms:
Sharp minimizers are characterized by a significant number of large positive eigenvalues of the Hessian Matrix of f(x), while flat minimizers are characterized by a considerable number of smaller positive eigenvalues of the Hessian Matrix of f(x).
“Falling” into a sharp minimizer may produce a seemingly better loss than a flat minimizer, but it’s more prone to generalizing poorly to unseen datasets. The diagram below illustrates a simple 2-dimensional loss landscape from Keskar et al.
A sharp minimum compared to a flat minimum. From Keskar et al.
We assume that the relationship between features and labels of unseen data points is similar to that of the data points that we used for training but not exactly the same. As the example shown above, the “difference” between train and test can be a slight horizontal shift. The parameter values that result in a sharp minimum become a relative maximum when applied to unseen data points due to its narrow accommodation of minimum values. With a flat minimum, though, as shown in the diagram above, a slight shift in the “Testing Function” would still put the model at a relatively minimum point in the loss landscape.
Typically, adopting a small batch size adds noise to training compared to using a bigger batch size. Since the gradients were estimated with a smaller number of samples, the estimation at each batch update will be rather “noisy” relative to the “loss landscape” of the entire dataset. Noisy training in the early stages is helpful to the model as it encourages exploration of the loss landscape. Keskar et al. also stated that…
“We have observed that the loss function landscape of deep Neural Networks is such that large-batch methods are attracted to regions with sharp minimizers and that, unlike small-batch methods, are unable to escape basins of attraction of these minimizers.”
Although larger batch sizes are considered to bring more stability to training, the noisiness that small batch training provides is actually beneficial to explore and avoiding sharp minimizers. We can effectively utilize this fact to design a “batch size scheduler” where we start with a small batch size to allow for exploration of the loss landscape. Once a general direction is decided, we hone in on the (hopefully) flat minimum and increase the batch size to stabilize training. The details of how one can increase the batch size during training to obtain faster and better results are described in the following article.
"
Deep Learning,Optimizers,SGD with Momentum,"SGD with Momentum is a stochastic optimization method that adds a momentum term to regular stochastic gradient descent. Momentum simulates the inertia of an object when it is moving, that is, the direction of the previous update is retained to a certain extent during the update, while the current update gradient is used to fine-tune the final update direction. In this way, you can increase the stability to a certain extent, so that you can learn faster, and also have the ability to get rid of local optimization.
Advantages of SGD with momentum
Momentum helps to reduce the noise.
Exponential Weighted Average is used to smoothen the curve.
Disadvantage of SGD with momentum
Extra hyperparameter is added.
"
Deep Learning,Optimizers,Nesterov Accelerated Gradient,"in Nesterov Accelerated Gradient, we apply the velocity vt to the parameters θ to compute interim parameters θ. We then compute the gradient using the interim parameters
We can view Nesterov Accelerated Gradients as the correction factor for Momentum method. Consider the case when the velocity added to the parameters gives you immediate unwanted high loss, e.g., exploding gradient case. In this case, the Momentum method can be very slow since the optimization path taken exhibits large oscillations. In Nesterov Accelerated Gradient case, you can view it like peeking through the interim parameters where the added velocity will lead the parameters. If the velocity update leads to bad loss, then the gradients will direct the update back towards θ𝑡. This help Nesterov Accelerated Gradient to avoid the oscillations.
"
Deep Learning,Optimizers,AdaGrad(Adaptive Gradient Descent),"In all the algorithms that we discussed previously the learning rate remains constant. The intuition behind AdaGrad is can we use different Learning Rates for each and every neuron for each and every hidden layer based on different iterations.
Advantages of AdaGrad
Learning Rate changes adaptively with iterations.
It is able to train sparse data as well.
Disadvantage of AdaGrad
If the neural network is deep the learning rate becomes very small number which will cause dead neuron problem.
"
Deep Learning,Optimizers,RMS-Prop (Root Mean Square Propagation),"RMS-Prop is a special version of Adagrad in which the learning rate is an exponential average of the gradients instead of the cumulative sum of squared gradients. RMS-Prop basically combines momentum with AdaGrad.
or
Advantages of RMS-Prop
In RMS-Prop learning rate gets adjusted automatically and it chooses a different learning rate for each parameter.
Disadvantages of RMS-Prop
Slow Learning
"
Deep Learning,Optimizers,AdaDelta,"Adadelta is an extension of Adagrad and it also tries to reduce Adagrad’s aggressive, monotonically reducing the learning rate and remove decaying learning rate problem. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size w.
In Adadelta we do not need to set the default learning rate as we take the ratio of the running average of the previous parameters to the current gradient.
E – running average 
Advantages of Adadelta
The main advantage of AdaDelta is that we do not need to set a default learning rate.
Disadvantages of Adadelta
Computationally expensive
"
Deep Learning,Optimizers,Adam (Adaptive Moment Estimation),"Adam optimizer is one of the most popular and famous gradient descent optimization algorithms. It is a method that computes adaptive learning rates for each parameter. It stores both the decaying average of the past gradients, similar to momentum, and also the decaying average of the past squared gradients, similar to RMS-Prop and Adadelta. Thus, it combines the advantages of both the methods.
We compute the decaying averages of past and past squared gradients mt and vt respectively as follows:
They counteract these biases by computing bias-corrected first and second moment estimates:
Advantages of Adam
Easy to implement
Computationally efficient.
Little memory requirements.
"
Deep Learning,Optimizers,How to choose optimizers?,"If the data is sparse, use the self-applicable methods, namely Adagrad, Adadelta, RMSprop, Adam.
RMSprop, Adadelta, Adam have similar effects in many cases.
Adam just added bias-correction and momentum on the basis of RMSprop,
As the gradient becomes sparse, Adam will perform better than RMSprop.
"
Deep Learning,Optimizers,Step size in Gradient Descent,"Choosing a good step-size, or learning rate, is important in gradient descent. If the step-size is too small, gradient descent can be slow. If the step-size is chosen too large, gradient descent can overshoot, fail to converge, or even diverge
There are two simple heuristics: 
When the function value increases after a gradient step, the step-size was too large. Undo the step and decrease the step-size. 
When the function value decreases the step could have been larger. Try to increase the step-size.
"
Deep Learning,Optimizers,Solving a Linear Equation System with Gradient Descent,"When we solve linear equations of the form Ax = b, in practice we solve Ax−b = 0 approximately by finding x∗ that minimizes the squared error 
∥Ax − b∥^2 = (Ax − b) ⊤(Ax − b)
 if we use the Euclidean norm. The gradient of (7.9) with respect to x is 
∇x = 2(Ax − b) ⊤A
We can use this gradient directly in a gradient descent algorithm. However, for this particular special case, it turns out that there is an analytic solution, which can be found by setting the gradient to zero.
When applied to the solution of linear systems of equations Ax = b, gradient descent may converge slowly. The speed of convergence of gradient descent is dependent on the condition number κ = σ(A)/max σ(A)min, which is the ratio of the maximum to the minimum singular value of A. The condition number essentially measures the ratio of the most curved direction versus the least curved direction, which corresponds to our imagery that poorly conditioned problems are long, thin valleys: They are very curved in one direction, but very flat in the other. Instead of directly solving Ax = b, one could instead solve P −1 (Ax − b) = 0, where P is called the preconditioner. The goal is to design P −1 such that P −1A has a better condition number, but at the same time P −1 is easy to compute.
"
Deep Learning,Optimizers,Stochastic Gradient Descent,"Stochastic gradient descent descent (often shortened as SGD) is a stochastic approximation of the gradient descent method for minimizing an objective function that is written as a sum of differentiable functions. The word stochastic here refers to the fact that we acknowledge that we do not know the gradient precisely, but instead only know a noisy approximation to it. By constraining the probability distribution of the approximate gradients, we can still theoretically guarantee that SGD will converge.
Why should one consider using an approximate gradient? A major reason is practical implementation constraints, such as the size of central processing unit (CPU)/graphics processing unit (GPU) memory or limits on computational time. We can think of the size of the subset used to estimate the gradient in the same way that we thought of the size of a sample when estimating empirical means. Large mini-batch sizes will provide accurate estimates of the gradient, reducing the variance in the parameter update. Furthermore, large mini-batches take advantage of highly optimized matrix operations in vectorized implementations of the cost and gradient. The reduction in variance leads to more stable convergence, but each gradient calculation will be more expensive.
In contrast, small mini-batches are quick to estimate. If we keep the mini-batch size small, the noise in our gradient estimate will allow us to get out of some bad local optima, which we may otherwise get stuck in. In machine learning, optimization methods are used for training by minimizing an objective function on the training data, but the overall goal is to improve generalization performance. Since the goal in machine learning does not necessarily need a precise estimate of the minimum of the objective function, approximate gradients using mini-batch approaches have been widely used. Stochastic gradient descent is very effective in large-scale machine learning problems.
"
Deep Learning,Schedulers,Learning Rate Schedule,"An easy start is to use a constant learning rate in gradient descent algorithm. But you can do better with a learning rate schedule. A schedule is to make learning rate adaptive to the gradient descent optimization procedure, so you can increase performance and reduce training time.
In the neural network training process, data is feed into the network in batches, with many batches in one epoch. Each batch triggers one training step, which the gradient descent algorithm updates the parameters once. However, usually the learning rate schedule is updated once for each training epoch only.
You can update the learning rate as frequent as each step but usually it is updated once per epoch because you want to know how the network performs in order to determine how the learning rate should update. Regularly, a model is evaluated with validation dataset once per epoch.
There are multiple ways of making learning rate adaptive. At the beginning of training, you may prefer a larger learning rate so you improve the network coarsely to speed up the progress. In a very complex neural network model, you may also prefer to gradually increase the learning rate at the beginning because you need the network to explore on the different dimensions of prediction. At the end of training, however, you always want to have the learning rate smaller. Since at that time, you are about to get the best performance from the model and it is easy to overshoot if the learning rate is large.
Therefore, the simplest and perhaps most used adaptation of the learning rate during training are techniques that reduce the learning rate over time. These have the benefit of making large changes at the beginning of the training procedure when larger learning rate values are used and decreasing the learning rate so that a smaller rate and, therefore, smaller training updates are made to weights later in the training procedure.
"
Deep Learning,Schedulers,Common Learning Rate Schedulers,"Learning rate schedulers come in various flavors, each with its own strategy for adjusting the learning rate.
Here are some common ones:
1. Step Learning Rate Scheduler
The step scheduler reduces the learning rate by a fixed factor after a fixed number of epochs or when a predefined condition is met. For example, it might reduce the learning rate by half every five epochs.
2. Exponential Learning Rate Scheduler
This scheduler exponentially decays the learning rate over time. The learning rate is reduced by a factor at regular intervals, leading to a smooth decrease.
3. ReduceLROnPlateau Scheduler
This dynamic scheduler monitors a specified metric, such as validation loss. If the metric plateaus (i.e., stops improving), the learning rate is reduced by a factor. It’s useful for fine-tuning when the model’s performance has stalled.
4. Cyclic Learning Rate Scheduler
Cyclic schedulers alternate between a low and high learning rate within a predefined range. This approach helps the model escape local minima and encourages exploration of the loss landscape.
5. One-Cycle Learning Rate Scheduler
An extension of the cyclic scheduler, the one-cycle scheduler starts with a low learning rate, increases it to a maximum, and then gradually reduces it again. This strategy combines the benefits of both high and low learning rates.
"
Deep Learning,Schedulers,Choosing the Right Scheduler,"Selecting the appropriate learning rate scheduler depends on your specific problem, model architecture, and dataset. Here are some tips to help you make the right choice:
Experiment: Try different schedulers and learning rate ranges to see which one works best for your problem. What works well for one task may not be ideal for another.
Monitor Metrics: Keep an eye on relevant metrics, such as training and validation loss, accuracy, or any custom metric specific to your problem. This will help you choose a scheduler that responds to your model’s needs.
Early Stopping: Combine learning rate scheduling with early stopping to prevent overfitting and ensure your model generalizes well.
Hyperparameter Tuning: Consider learning rate scheduling as one of the hyperparameters to tune during your model optimization process.
"
Deep Learning,Schedulers,Gradient Descent With Momentum,"Gradient descent with momentum (Rumelhart et al., 1986) is a method that introduces an additional term to remember what happened in the previous iteration. This memory dampens oscillations and smoothes out the gradient updates. Continuing the ball analogy, the momentum term emulates the phenomenon of a heavy ball that is reluctant to change directions. The idea is to have a gradient update with memory to implement a moving average. The momentum-based method remembers the update ∆xi at each iteration i and determines the next update as a linear combination of the current and previous gradients 
xi+1 = xi − γi((∇f)(xi))⊤ + α∆xi
∆xi = xi − xi−1 = α∆xi−1 − γi−1((∇f)(xi−1))⊤ ,
where α ∈ [0, 1]. Sometimes we will only know the gradient approximately. In such cases, the momentum term is useful since it averages out different noisy estimates of the gradient. One particularly useful way to obtain an approximate gradient is by using a stochastic approximation, which we discuss next
"
Deep Learning,Activation functions,What is an activation function?,"Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron. That is exactly what an activation function does in an ANN as well. It takes in the output signal from the previous cell and converts it into some form that can be taken as input to the next cell.
"
Deep Learning,Activation functions,Why do we need activation functions?,"They help in keeping the value of the output from the neuron restricted to a certain limit as per our requirement. This is important because input into the activation function is W*x + b where W is the weights of the cell and the x is the inputs and then there is the bias b added to that. This value if not restricted to a certain limit can go very high in magnitude especially in case of very deep neural networks that have millions of parameters. This will lead to computational issues. For example, there are some activation functions (like softmax) that out specific values for different values of input (0 or 1).
The most important feature in an activation function is its ability to add non-linearity into a neural network. 
What if we use an ANN with a single cell but without an activation function. So our output is basically W*x + b. But this is no good because W*x also has a degree of 1, hence linear and this is basically identical to a linear classifier.
What if we stack multiple layers. Let’s represent nᵗʰ layer as a function fₙ(x). So we have:
o(x) = fₙ(fₙ₋₁(….f₁(x))
However, this is also not complex enough especially for problems with very high patterns such as that faced in computer vision or natural language processing.
In order to make the model get the power (aka the higher degree complexity) to learn the non-linear patterns, specific non-linear layers (activation functions) are added in between.
"
Deep Learning,Activation functions,Desirable features of an activation function,"Vanishing Gradient problem: Neural Networks are trained using the process gradient descent. The gradient descent consists of the backward propagation step which is basically chain rule to get the change in weights in order to reduce the loss after every epoch. Consider a two-layer network and the first layer is represented as f₁(x) and the second layer is represented as f₂(x). The overall network is o(x) = f₂(f₁(x)). If we calculate weights during the backward pass, we get o`(x) = f₂(x)*f₁`(x). Here f₁(x) is itself a compound function consisting of Act(W₁*x₁ + b₁) where Act is the activation function after layer 1. Applying chain rule again, we clearly see that f₁`(x) = Act(W₁*x₁ + b₁)*x₁ which means it also depends directly on the activation value. Now imagine such a chain rule going through multiple layers while backpropagation. If the value of Act() is between 0 and 1, then several such values will get multiplied to calculate the gradient of the initial layers. This reduces the value of the gradient for the initial layers and those layers are not able to learn properly. In other words, their gradients tend to vanish because of the depth of the network and the activation shifting the value to zero. This is called the vanishing gradient problem. So we want our activation function to not shift the gradient towards zero.
Zero-Centered: Output of the activation function should be symmetrical at zero so that the gradients do not shift to a particular direction.
Computational Expense: Activation functions are applied after every layer and need to be calculated millions of times in deep networks. Hence, they should be computationally inexpensive to calculate.
Differentiable: As mentioned, neural networks are trained using the gradient descent process, hence the layers in the model need to differentiable or at least differentiable in parts. This is a necessary requirement for a function to work as activation function layer.
"
Deep Learning,Activation functions,List Activation functions,"Sigmoid
σ(x) = 1 / (1 + e^-x)
0 <= σ(x) <= 1
Softmax - The softmax is a more generalised form of the sigmoid. It is used in multi-class classification problems. Similar to sigmoid, it produces values in the range of 0–1 therefore it is used as the final layer in classification models.
Softmax(xi) = e^xi / sum_j(e^xj)
Tanh - If you compare it to sigmoid, it solves just one problem of being zero-centred.
ReLU: ReLU (Rectified Linear Unit) is defined as f(x) = max(0,x):
This is a widely used activation function, especially with Convolutional Neural networks. It is easy to compute and does not saturate and does not cause the Vanishing Gradient Problem. It has just one issue of not being zero centred. It suffers from “dying ReLU” problem. Since the output is zero for all negative inputs. It causes some nodes to completely die and not learn anything.
Another problem with ReLU is of exploding the activations since it higher limit is, well, inf. This sometimes leads to unusable nodes.
Leaky ReLU and Parametric ReLU: It is defined as f(x) = max(αx, x)
the figure is for α = 0.1
Here α is a hyperparameter generally set to 0.01. Clearly, Leaky ReLU solves the “dying ReLU” problem to some extent. Note that, if we set α as 1 then Leaky ReLU will become a linear function f(x) = x and will be of no use. Hence, the value of α is never set close to 1. If we set α as a hyperparameter for each neuron separately, we get parametric ReLU or PReLU.
ReLU6: It is basically ReLU restricted on the positive side and it is defined as f(x) = min(max(0,x),6)
This helps to stop blowing up the activation thereby stopping the gradients to explode(going to inf) as well another of the small issues that occur with normal ReLUs.
Notable non-linear activations coming out of latest research
Swish: This was proposed in 2017 by Ramachandran et.al. It is defined as f(x) = x*sigmoid(x).
It is slightly better in performance as compared to ReLU since its graph is quite similar to ReLU. However, because it does not change abruptly at a point as ReLU does at x = 0, this makes it easier to converge while training.
But, the drawback of Swish is that it is computationally expensive. To solve that we come to the next version of Swish.
Hard-Swish or H-Swish: This is defined as:
The best part is that it is almost similar to swish but it is less expensive computationally since it replaces sigmoid (exponential function) with a ReLU (linear type).
"
Deep Learning,Activation functions,Applications of Sigmoid ,"The sigmoid can be used simply as an activation function throughout a neural network, applying it to the outputs of each network layer. It isn’t used as much nowadays, however, because it has a couple of inefficiencies. 
The first is the problem of saturating gradients. Looking at its graph, we can see that the sigmoid has a strong slope in the middle, but at the ends, its slope is very shallow. This is a problem for learning. At a high level, when we run gradient descent, many of the neurons in our network will be outputting values in the shallow regions of the sigmoid. Changing the network weights will then have little effect on its overall output, and learning comes to a halt.
In a little more detail, to run backpropagation and learn, we must take the gradient of the loss function with respect to each parameter in our network. At first, some neurons may be outputting values in the middle of the sigmoid range, where the slope is strong. But as we make updates, we move up or down this slope and quickly end up in a shallow region. The magnitude of our gradient then becomes smaller and smaller, meaning we take smaller and smaller learning steps. Learning is not very efficient this way.
The other problem with the sigmoid is that it’s not symmetric about the origin. In the brain, neurons either fire or don’t, so we may have the intuition that neuron activations should be zero or one. Despite this, researchers have actually found that neural networks learn better when activations are centered around zero. This is one of the reasons it’s a good idea to standardize your data (i.e., shift it to have mean zero) before feeding it into a neural network. It’s also one of the reasons for batch normalization, a similar process where we standardize our network activations at intermediate layers rather than just at the start.
If you look at the beginning of the previous section, you’ll see that the tanh function ranges from -1 to one and is centered around zero. For this reason, it’s often preferable to the sigmoid. It also has the problem of saturating gradients, though. The most common activation function nowadays is the rectified linear unit (ReLU):
This function has a strong slope everywhere to the right of zero, although it’s obviously not symmetric around zero. So, tanh has saturating gradients, and ReLU is non-symmetric. In practice, the former is a bigger problem than the latter. The moral here, though, is that the sigmoid is the worst of both worlds on these fronts.
Despite all this, the sigmoid still has a place in modern machine learning: binary classification. In binary classification, we categorize inputs as one of two classes. If we’re using neural networks, the output of our network must be a number between zero and one, representing the probability that the input belongs to class one (with the probability for class two being immediately inferable).
The output layer of such a network consists of a single neuron. Consider the output value of this neuron. Before applying any activation function, it can be any real number, which is no good. If we apply a ReLU, it will be positive (or zero). If we use tanh, it will be between -1 and one. None of these work. We must apply a sigmoid to this last neuron. We need a number between zero and one, and we still need the activation function to be smooth for the purposes of training. The sigmoid is the right choice.
"
Deep Learning,Activation functions,Why can’t I use Softmax on the hidden layer?,"The following steps explain why using the softmax function on the hidden layer is not a good idea:
1. Variables independence: A lot of regularization and effort is required to keep your variables independent, uncorrelated and quite sparse. If you use the softmax layer as a hidden layer, then you will keep all your nodes linearly dependent which may result in many problems and poor generalization.
2. Training issues:  if your network is working better, you have to make a part of activations from your hidden layer a little bit lower. Here automatically you are making the rest of them have mean activation on a higher level which might, in fact, increase the error and harm your training phase.
3. Mathematical issues: If you create constraints on activations of your model you decrease the expressive power of your model without any logical explanation. 
4. Batch normalization does it better: You may consider the fact that mean output from a network may be useful for training. But on the other hand, a technique called Batch Normalization has been already proven to work better, but it was reported that setting softmax as the activation function in a hidden layer may decrease the accuracy and speed of learning.
"
Deep Learning,Activation functions,GELU (Gaussian Error Linear Unit),"The key advantage of GELU lies in its smoothness and differentiability across the entire real line. This smoothness facilitates training, as gradient-based optimization algorithms can easily navigate the function’s landscape.
The smoothness also mitigates the problem of vanishing gradients. It occurs at or near the saturation point of the activation function, where changes to the input cease to impact the derivative. A smoother, more gradual gradient transition provides a more consistently informative training signal.
The smoothness also mitigates abrupt gradient changes found in other activation functions, aiding convergence during training.
GELU has a significantly smoother gradient transition than the sharp and abrupt ReLU.
Further, GELU has been shown to improve performance in transformer architectures and on standard benchmarks such as cifar-10 and is easy to include in models.
"
Deep Learning,Weights Initialization,Xavier (Glorot) initialization ,"Xavier initialization sets a layer’s weights to values chosen from a random uniform distribution that’s bounded between +-sqrt(6/(ni+ni+1))
where nᵢ is the number of incoming network connections, or “fan-in,” to the layer, and nᵢ₊₁ is the number of outgoing network connections from that layer, also known as the “fan-out.”
The main idea is that the method used for randomization isn’t so important. It is the number of outputs in the following layer that matters. With the passing of each layer, the Xavier initialization maintains the variance in some bounds so that we can take full advantage of the activation functions.
There are two formulas for this strategy.
The Uniform Xavier initialization states we should draw each weight w from a random uniform distribution in the range from minus x to x, where x is equal to square root of 6, divided by the number of inputs, plus the number of outputs for the transformation.
For the normal Xavier initialization, we draw each weight w from a normal distribution with a mean of 0, and a standard deviation equal to 2, divided by the number of inputs, plus the number of outputs for the transformation.
The numerator values 2 and 6 vary across sources, but the main idea is the same.
Another detail we should highlight here is that the number of inputs and outputs matters.
Outputs are clear – that’s where the activation function goes. So, the higher the number of outputs, the higher the need to spread weights.
What about inputs? Well, since we achieve optimization through backpropagation, we would obviously have the same problem, but in the opposite direction.
"
Deep Learning,Convolutional Layer,What are convolutional neural networks?,"Neural networks are a subset of machine learning, and they are at the heart of deep learning algorithms. They are comprised of node layers, containing an input layer, one or more hidden layers, and an output layer. Each node connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.
While we primarily focused on feedforward networks in that article, there are various types of neural nets, which are used for different use cases and data types. For example, recurrent neural networks are commonly used for natural language processing and speech recognition whereas convolutional neural networks (ConvNets or CNNs) are more often utilized for classification and computer vision tasks. Prior to CNNs, manual, time-consuming feature extraction methods were used to identify objects in images. However, convolutional neural networks now provide a more scalable approach to image classification and object recognition tasks, leveraging principles from linear algebra, specifically matrix multiplication, to identify patterns within an image. That said, they can be computationally demanding, requiring graphical processing units (GPUs) to train models. 
"
Deep Learning,Convolutional Layer,How do convolutional neural networks work?,"Convolutional neural networks are distinguished from other neural networks by their superior performance with image, speech, or audio signal inputs. They have three main types of layers, which are:
Convolutional layer
Pooling layer
Fully-connected (FC) layer
The convolutional layer is the first layer of a convolutional network. While convolutional layers can be followed by additional convolutional layers or pooling layers, the fully-connected layer is the final layer. With each layer, the CNN increases in its complexity, identifying greater portions of the image. Earlier layers focus on simple features, such as colors and edges. As the image data progresses through the layers of the CNN, it starts to recognize larger elements or shapes of the object until it finally identifies the intended object.
"
Deep Learning,Convolutional Layer,Why is a convolutional neural network preferred over a dense neural network for an image classification task?,"The number of parameters in a convolutional neural network is much more diminutive than that of a Dense Neural Network. Hence, a CNN is less likely to overfit.
CNN allows you to look at the weights of a filter and visualize what the network learned. So, this gives a better understanding of the model.
CNN trains models in a hierarchical way, i.e., it learns the patterns by explaining complex patterns using simpler ones.
"
Deep Learning,Convolutional Layer,Constrained Optimization and Lagrange Multipliers,"we consider the constrained optimization problem 
min x f(x) 
subject to gi(x) ⩽ 0 for all i = 1, . . . , m .
We associate to problem the Lagrangian by introducing the Lagrange multipliers λi ⩾ 0 corresponding to each inequality constraint respectively so that
L(x,λ) = f(x) +sum_i=1..m(λi * gi(x)) = f(x) + λ ⊤ g(x), 
where in the last line we have concatenated all constraints gi(x) into a vector g(x), and all the Lagrange multipliers into a vector λ ∈ Rm. 
The associated Lagrangian dual problem is given by problem 
max λ∈Rm D(λ) 
subject to λ ⩾ 0 , 
where λ are the dual variables and D(λ) = minx∈Rd L(x,λ).
In contrast to the original optimization problem, which has constraints, minx∈Rd L(x,λ) is an unconstrained optimization problem for a given value of λ. If solving minx∈Rd L(x,λ) is easy, then the overall problem is easy to solve. We can see this by observing from that L(x,λ) is affine with respect to λ. Therefore minx∈Rd L(x,λ) is a pointwise minimum of affine functions of λ, and hence D(λ) is concave even though f(·) and gi(·) may be nonconvex. The outer problem, maximization over λ, is the maximum of a concave function and can be efficiently computed.
Assuming f(·) and gi(·) are differentiable, we find the Lagrange dual problem by differentiating the Lagrangian with respect to x, setting the differential to zero, and solving for the optimal value.
"
Deep Learning,Convolutional Layer,Convolutional Layer,"The convolutional layer is the core building block of a CNN, and it is where the majority of computation occurs. It requires a few components, which are input data, a filter, and a feature map. Let’s assume that the input will be a color image, which is made up of a matrix of pixels in 3D. This means that the input will have three dimensions—a height, width, and depth—which correspond to RGB in an image. We also have a feature detector, also known as a kernel or a filter, which will move across the receptive fields of the image, checking if the feature is present. This process is known as a convolution.
The feature detector is a two-dimensional (2-D) array of weights, which represents part of the image. While they can vary in size, the filter size is typically a 3x3 matrix; this also determines the size of the receptive field. The filter is then applied to an area of the image, and a dot product is calculated between the input pixels and the filter. This dot product is then fed into an output array. Afterwards, the filter shifts by a stride, repeating the process until the kernel has swept across the entire image. The final output from the series of dot products from the input and the filter is known as a feature map, activation map, or a convolved feature.
As you can see in the image above, each output value in the feature map does not have to connect to each pixel value in the input image. It only needs to connect to the receptive field, where the filter is being applied. Since the output array does not need to map directly to each input value, convolutional (and pooling) layers are commonly referred to as “partially connected” layers. However, this characteristic can also be described as local connectivity.
Note that the weights in the feature detector remain fixed as it moves across the image, which is also known as parameter sharing. Some parameters, like the weight values, adjust during training through the process of backpropagation and gradient descent. However, there are three hyperparameters which affect the volume size of the output that need to be set before the training of the neural network begins. These include:
1. The number of filters affects the depth of the output. For example, three distinct filters would yield three different feature maps, creating a depth of three. 
2. Stride is the distance, or number of pixels, that the kernel moves over the input matrix. While stride values of two or greater is rare, a larger stride yields a smaller output.
3. Zero-padding is usually used when the filters do not fit the input image. This sets all elements that fall outside of the input matrix to zero, producing a larger or equally sized output. There are three types of padding:
Valid padding: This is also known as no padding. In this case, the last convolution is dropped if dimensions do not align.
Same padding: This padding ensures that the output layer has the same size as the input layer
Full padding: This type of padding increases the size of the output by adding zeros to the border of the input.
After each convolution operation, a CNN applies a Rectified Linear Unit (ReLU) transformation to the feature map, introducing nonlinearity to the model.
As we mentioned earlier, another convolution layer can follow the initial convolution layer. When this happens, the structure of the CNN can become hierarchical as the later layers can see the pixels within the receptive fields of prior layers.  As an example, let’s assume that we’re trying to determine if an image contains a bicycle. You can think of the bicycle as a sum of parts. It is comprised of a frame, handlebars, wheels, pedals, et cetera. Each individual part of the bicycle makes up a lower-level pattern in the neural net, and the combination of its parts represents a higher-level pattern, creating a feature hierarchy within the CNN.
Ultimately, the convolutional layer converts the image into numerical values, allowing the neural network to interpret and extract relevant patterns.
"
Deep Learning,Convolutional Layer,Output shape of Convolution layer,"[(W−K+2P)/S]+1.
W is the input volume
K is the Kernel size
P is the padding 
S is the stride
"
Deep Learning,Convolutional Layer,How to calculate the number of parameters in the convolution layer?,"Parameters in one filter of size(3,3)= 3*3 =9
The filter will convolve over all three channels concurrently(input_image depth=3). So parameters in one filter will be 3*3*3=27 
[filter size * input_data depth]
Bias =1 
 [One bias will be added to each filter]
Total parameters for one filter kernel of size (3,3) for depth 3=(3*3*3)+1=28
The total number of filters= 5.
Total parameters for 5 filter kernel of size (3,3) , input_image depth(3)= 28*5=140
"
Deep Learning,Convolutional Layer,Atrous(Dilated) Convolution,"To understand how atrous convolution differs from the standard convolution, we first need to know what receptive field is. Receptive Field is defined as the size of the region of the input feature map that produces each output element. In the case of Fig.1, the receptive field is 3x3 as each element in the output feature map sees(uses) 3x3 input elements.
Deep CNNs use a combination of Convolutions and max-pooling. This has the disadvantage that, at each step, the spatial resolution of the feature map is halved. Implanting the resultant feature map onto the original image results in sparse feature extraction. This effect can be seen in Fig. 2. The conv. filter downsamples the input image by a factor of two. Upsampling and imposing the feature map on the image shows that the responses correspond to only 1/4th of the image locations(Sparse feature extraction).
Atrous(Dilated) convolution fixes this problem and allows for dense feature extraction. This is achieved a new parameter called rate(r). Put simply, atrous convolution is akin to the standard convolution except that the weights of an atrous convolution kernel are spaced r locations apart, i.e., the kernel of dilated convolution layers are sparse.
By controlling the rate parameter, we can arbitrarily control the receptive fields of the conv. layer. This allows the conv. filter to look at larger areas of the input(receptive field) without a decrease in the spatial resolution or increase in the kernel size. 
Compared to standard convolution used in Fig. 2, dense features are extracted by using a dilated kernel with rate r=2. Dilated convolutions can be trivially implemented by just setting the dilation parameter to the required dilation rate.
"
Deep Learning,Pooling,Pooling Layer,"Pooling layers, also known as downsampling, conducts dimensionality reduction, reducing the number of parameters in the input. Similar to the convolutional layer, the pooling operation sweeps a filter across the entire input, but the difference is that this filter does not have any weights. Instead, the kernel applies an aggregation function to the values within the receptive field, populating the output array. There are two main types of pooling:
Max pooling: As the filter moves across the input, it selects the pixel with the maximum value to send to the output array. As an aside, this approach tends to be used more often compared to average pooling.
Average pooling: As the filter moves across the input, it calculates the average value within the receptive field to send to the output array.
While a lot of information is lost in the pooling layer, it also has a number of benefits to the CNN. They help to reduce complexity, improve efficiency, and limit risk of overfitting. 
"
Deep Learning,Pooling,Global Average Pooling,"The feature maps of the last convolutional layer are vectorized and fed into fully connected layers followed by a softmax logistic regression layer. This structure bridges the convolutional structure with traditional neural networks. It treats the convolutional layers as feature extractors, and the resulting feature is classified in a traditional way. 
The fully connected layers are prone to overfitting. You can use Dropout as a regularizer which randomly sets half of the activations to the fully connected layers to zero during training. It has improved the generalization ability and largely prevents overfitting. 
You can use another strategy called global average pooling to replace the Flatten layers in CNN. It generates one feature map for each corresponding category of the classification task in the last Conv layer.
"
Deep Learning,Pooling,Flatten Layer vs GlobalAveragePooling,"Flatten Layer will take a tensor of any shape and transform it into a one-dimensional tensor but keeping all values in the tensor. For example a tensor (samples, 10, 10, 32) will be flattened to (samples, 10 * 10 * 32).
An architecture like this has the risk of overfitting to the training dataset. In practice, dropout layers are used to avoid overfitting.
Global Average Pooling does something different. It applies average pooling on the spatial dimensions until each spatial dimension is one, and leaves other dimensions unchanged. For example, a tensor (samples, 10, 10, 32) would be output as (samples, 1, 1, 32).
"
Deep Learning,Pooling,Fully-Connected Layer,"The name of the full-connected layer aptly describes itself. As mentioned earlier, the pixel values of the input image are not directly connected to the output layer in partially connected layers. However, in the fully-connected layer, each node in the output layer connects directly to a node in the previous layer.
This layer performs the task of classification based on the features extracted through the previous layers and their different filters. While convolutional and pooling layers tend to use ReLu functions, FC layers usually leverage a softmax activation function to classify inputs appropriately, producing a probability from 0 to 1.
"
Deep Learning,RNN,Recurrent Neural Networks,"A recurrent neural network (RNN) is a type of artificial neural network which uses sequential data or time series data. These deep learning algorithms are commonly used for ordinal or temporal problems, such as language translation, natural language processing (nlp), speech recognition, and image captioning; they are incorporated into popular applications such as Siri, voice search, and Google Translate. Like feedforward and convolutional neural networks (CNNs), recurrent neural networks utilize training data to learn. They are distinguished by their “memory” as they take information from prior inputs to influence the current input and output. While traditional deep neural networks assume that inputs and outputs are independent of each other, the output of recurrent neural networks depend on the prior elements within the sequence. While future events would also be helpful in determining the output of a given sequence, unidirectional recurrent neural networks cannot account for these events in their predictions. 
Another distinguishing characteristic of recurrent networks is that they share parameters across each layer of the network. While feedforward networks have different weights across each node, recurrent neural networks share the same weight parameter within each layer of the network. That said, these weights are still adjusted in the through the processes of backpropagation and gradient descent to facilitate reinforcement learning.
Recurrent neural networks leverage backpropagation through time (BPTT) algorithm to determine the gradients, which is slightly different from traditional backpropagation as it is specific to sequence data. The principles of BPTT are the same as traditional backpropagation, where the model trains itself by calculating errors from its output layer to its input layer. These calculations allow us to adjust and fit the parameters of the model appropriately. BPTT differs from the traditional approach in that BPTT sums errors at each time step whereas feedforward networks do not need to sum errors as they do not share parameters across each layer.
Through this process, RNNs tend to run into two problems, known as exploding gradients and vanishing gradients. These issues are defined by the size of the gradient, which is the slope of the loss function along the error curve. When the gradient is too small, it continues to become smaller, updating the weight parameters until they become insignificant—i.e. 0. When that occurs, the algorithm is no longer learning. Exploding gradients occur when the gradient is too large, creating an unstable model. In this case, the model weights will grow too large, and they will eventually be represented as NaN. One solution to these issues is to reduce the number of hidden layers within the neural network, eliminating some of the complexity in the RNN model.
"
Deep Learning,RNN,Long short-term memory (LSTM),"This is a popular RNN architecture, which was introduced by Sepp Hochreiter and Juergen Schmidhuber as a solution to vanishing gradient problem. In their paper (PDF, 388 KB) (link resides outside IBM), they work to address the problem of long-term dependencies. That is, if the previous state that is influencing the current prediction is not in the recent past, the RNN model may not be able to accurately predict the current state. As an example, let’s say we wanted to predict the italicized words in following, “Alice is allergic to nuts. She can’t eat peanut butter.” The context of a nut allergy can help us anticipate that the food that cannot be eaten contains nuts. However, if that context was a few sentences prior, then it would make it difficult, or even impossible, for the RNN to connect the information. To remedy this, LSTMs have “cells” in the hidden layers of the neural network, which have three gates–an input gate, an output gate, and a forget gate. These gates control the flow of information which is needed to predict the output in the network.  For example, if gender pronouns, such as “she”, was repeated multiple times in prior sentences, you may exclude that from the cell state.
The different steps in LSTM include the following.
Step 1:The network helps decide what needs to be remembered and forgotten
Step 2:The selection is made for cell state values that can be updated
Step 3: The network decides as to what can be made as part of the current output
"
Deep Learning,RNN,LSTM architecture,"In the LSTM figure, we can see that we have 8 different weight parameters (4 associated with the hidden state (cell state) and 4 associated with the input vector). We also have 4 different bias parameters. To better understand this we can use the following equations and better understand the operations in LSTM cell.
Here, with the help of the above equations, we can clearly see a total of 4 biases and 8 weights. Let's take an example.
Seq_len of the input sentence (S)= 12
embedding dimension (E)= 30
No of LSTM cells (hidden units) (H)= 10
Batch_size (B) = 1
The input (x) will be batch size * embedding dimension = B*D
The previous hidden state will be batch size * hidden units = B*H
Equation 1: forget gate = [(1*10).(10*10)+(1*30).(30*10) + (1*10)]
= (1*10) = (B*H)
Equation 2: update gate = [(1*10).(10*10)+(1*30).(30*10) + (1*10)]
= (1*10) = (B*H)
Equation 3: candidate memory=[(1*10).(10*10)+(1*30).(30*10)+(1*10)]
= (1*10) = (B*H)
Equation 4: output gate =[(1*10).(10*10)+(1*30).(30*10) + (1*10)]
= (1*10) = (B*H)
Since all weights follow the same structure these can be combined together can then multiplied with the respective output. weights associated with hidden state are called kernel weights and weights associated with input are called recurrent kernel weights.
Note:
1. Since LSTM processes data in sequential nature. It will receive 1 word at a time and the same LSTM cell will receive the next subsequent words. No. of LSTM cell doesn’t mean that many times LSTM is repeated. It means it can be unfolded up to the sequence length. In the actual LSTM cell, the same cell will receive all the words one by one.
2. Sequence length does not have any effect on the weights and bias dimension. It can be clearly seen in the above calculations.
3. Weight is multiplied by taking the transpose of weight, but here I have rearranged weight and input for simplification.
To see all the weights and bias dimensions, I have put them in a table and named them accordingly as per equations.
Here, with the help of the above equations, we can clearly see a total of 4 biases and 8 weights. Let's take an example.
Seq_len of the input sentence (S)= 12
embedding dimension (E)= 30
No of LSTM cells (hidden units) (H)= 10
Batch_size (B) = 1
The input (x) will be batch size * embedding dimension = B*D
The previous hidden state will be batch size * hidden units = B*H
Equation 1: forget gate = [(1*10).(10*10)+(1*30).(30*10) + (1*10)]
= (1*10) = (B*H)
Equation 2: update gate = [(1*10).(10*10)+(1*30).(30*10) + (1*10)]
= (1*10) = (B*H)
Equation 3: candidate memory=[(1*10).(10*10)+(1*30).(30*10)+(1*10)]
= (1*10) = (B*H)
Equation 4: output gate =[(1*10).(10*10)+(1*30).(30*10) + (1*10)]
= (1*10) = (B*H)
Since all weights follow the same structure these can be combined together can then multiplied with the respective output. weights associated with hidden state are called kernel weights and weights associated with input are called recurrent kernel weights.
Note:
1. Since LSTM processes data in sequential nature. It will receive 1 word at a time and the same LSTM cell will receive the next subsequent words. No. of LSTM cell doesn’t mean that many times LSTM is repeated. It means it can be unfolded up to the sequence length. In the actual LSTM cell, the same cell will receive all the words one by one.
2. Sequence length does not have any effect on the weights and bias dimension. It can be clearly seen in the above calculations.
3. Weight is multiplied by taking the transpose of weight, but here I have rearranged weight and input for simplification.
To see all the weights and bias dimensions, I have put them in a table and named them accordingly as per equations.
"
Deep Learning,Regularization,Regularization Methods for Neural Networks,"The simplest and perhaps most common regularization method is to add a penalty to the loss function in proportion to the size of the weights in the model.
Weight Regularization (weight decay): Penalize the model during training based on the magnitude of the weights.
This will encourage the model to map the inputs to the outputs of the training dataset in such a way that the weights of the model are kept small. This approach is called weight regularization or weight decay and has proven very effective for decades for both simpler linear models and neural networks.
A simple alternative to gathering more data is to reduce the size of the model or improve regularization, by adjusting hyperparameters such as weight decay coefficients…
Below is a list of five of the most common additional regularization methods.
Activity Regularization: Penalize the model during training base on the magnitude of the activations.
Weight Constraint: Constrain the magnitude of weights to be within a range or below a limit.
Dropout: Probabilistically remove inputs during training.
Noise: Add statistical noise to inputs during training.
Early Stopping: Monitor model performance on a validation set and stop training when performance degrades.
Most of these methods have been demonstrated (or proven) to approximate the effect of adding a penalty to the loss function.
Each method approaches the problem differently, offering benefits in terms of a mixture of generalization performance, configurability, and/or computational complexity.
"
Deep Learning,Regularization,Batch Normalization,"To understand what happens without normalization, let’s look at an example with just two features that are on drastically different scales. Since the network output is a linear combination of each feature vector, this means that the network learns weights for each feature that are also on different scales. Otherwise, the large feature will simply drown out the small feature.
Then during gradient descent, in order to “move the needle” for the Loss, the network would have to make a large update to one weight compared to the other weight. This can cause the gradient descent trajectory to oscillate back and forth along one dimension, thus taking more steps to reach the minimum.
Just like the parameters (eg. weights, bias) of any network layer, a Batch Norm layer also has parameters of its own:
Two learnable parameters called beta and gamma.
Two non-learnable parameters (Mean Moving Average and Variance Moving Average) are saved as part of the ‘state’ of the Batch Norm layer.
These parameters are per Batch Norm layer. So if we have, say, three hidden layers and three Batch Norm layers in the network, we would have three learnable beta and gamma parameters for the three layers. Similarly for the Moving Average parameters.
During training, we feed the network one mini-batch of data at a time. During the forward pass, each layer of the network processes that mini-batch of data. The Batch Norm layer processes its data as follows:
1. Activations
The activations from the previous layer are passed as input to the Batch Norm. There is one activation vector for each feature in the data.
2. Calculate Mean and Variance
For each activation vector separately, calculate the mean and variance of all the values in the mini-batch.
3. Normalize
Calculate the normalized values for each activation feature vector using the corresponding mean and variance. These normalized values now have zero mean and unit variance.
4. Scale and Shift
This step is the huge innovation introduced by Batch Norm that gives it its power. Unlike the input layer, which requires all normalized values to have zero mean and unit variance, Batch Norm allows its values to be shifted (to a different mean) and scaled (to a different variance). It does this by multiplying the normalized values by a factor, gamma, and adding to it a factor, beta. Note that this is an element-wise multiply, not a matrix multiply.
What makes this innovation ingenious is that these factors are not hyperparameters (ie. constants provided by the model designer) but are trainable parameters that are learned by the network. In other words, each Batch Norm layer is able to optimally find the best factors for itself, and can thus shift and scale the normalized values to get the best predictions.
5. Moving Average
In addition, Batch Norm also keeps a running count of the Exponential Moving Average (EMA) of the mean and variance. During training, it simply calculates this EMA but does not do anything with it. At the end of training, it simply saves this value as part of the layer’s state, for use during the Inference phase.
We will return to this point a little later when we talk about Inference. The Moving Average calculation uses a scalar ‘momentum’ denoted by alpha below. This is a hyperparameter that is used only for Batch Norm moving averages and should not be confused with the momentum that is used in the Optimizer.
Vector Shapes
Below, we can see the shapes of these vectors. The values that are involved in computing the vectors for a particular feature are also highlighted in red. However, remember that all feature vectors are computed in a single matrix operation.
Shapes of Batch Norm vectors (Image by Author)
After the forward pass, we do the backward pass as normal. Gradients are calculated and updates are done for all layer weights, as well as for all beta and gamma parameters in the Batch Norm layers.
"
Deep Learning,Regularization,Batch Normalization during Inference,"As we discussed above, during Training, Batch Norm starts by calculating the mean and variance for a mini-batch. However, during Inference, we have a single sample, not a mini-batch. How do we obtain the mean and variance in that case?
Here is where the two Moving Average parameters come in — the ones that we calculated during training and saved with the model. We use those saved mean and variance values for the Batch Norm during Inference.
Ideally, during training, we could have calculated and saved the mean and variance for the full data. But that would be very expensive as we would have to keep values for the full dataset in memory during training. Instead, the Moving Average acts as a good proxy for the mean and variance of the data. It is much more efficient because the calculation is incremental — we have to remember only the most recent Moving Average.
"
Deep Learning,Regularization,Order of placement of Batch Norm layer,"There are two opinions for where the Batch Norm layer should be placed in the architecture — before and after activation. The original paper placed it before, although I think you will find both options frequently mentioned in the literature. Some say ‘after’ gives better results.
"
Deep Learning,Regularization,What is Layer Normalization?,"Layer Normalization was proposed by researchers Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. In layer normalization, all neurons in a particular layer effectively have the same distribution across all features for a given input.
For example, if each input has d features, it’s a d-dimensional vector. If there are B elements in a batch, the normalization is done along the length of the d-dimensional vector and not across the batch of size B.
Normalizing across all features but for each of the inputs to a specific layer removes the dependence on batches. This makes layer normalization well suited for sequence models such as transformers and recurrent neural networks (RNNs) that were popular in the pre-transformer era.
Here’s an example showing the computation of the mean and variance for layer normalization. We consider the example of a mini-batch containing three input samples, each with four features.
From these steps, we see that they’re similar to the steps we had in batch normalization. However, instead of the batch statistics, we use the mean and variance corresponding to specific input to the neurons in a particular layer, say k. This is equivalent to normalizing the output vector from the layer k-1.
"
Deep Learning,Regularization,Why do we use Conv layers without bias before Batch Normalization? ,"Batch Normalization already includes the addition of the bias term. Recap that BatchNorm is already:
gamma * normalized(x) + bias
So there is no need (and it makes no sense) to add another bias term in the convolution layer. Simply speaking BatchNorm shifts the activation by their mean values. Hence, any constant will be canceled out.
Long story short: Even if you implement the ConvWithBias+BatchNorm, it will behave like ConvWithoutBias+BatchNorm. It is the same as multiple fully-connected layers without activation function will behave like a single one.
"
Deep Learning,Regularization,Batch Normalization vs Layer Normalization,"Batch normalization normalizes each feature independently across the mini-batch. Layer normalization normalizes each of the inputs in the batch independently across all features.
As batch normalization is dependent on batch size, it’s not effective for small batch sizes. Layer normalization is independent of the batch size, so it can be applied to batches with smaller sizes as well.
Batch normalization requires different processing at training and inference times. As layer normalization is done along the length of input to a specific layer, the same set of operations can be used at both training and inference times.
Batch Normalization in Convolutional Neural Networks
Batch Norm works in a very similar way in Convolutional Neural Networks. Although we could do it in the same way as before, we have to follow the convolutional property.
In convolutions, we have shared filters that go along the feature maps of the input (in images, the feature map is generally the height and width). These filters are the same on every feature map. It is then reasonable to normalize the output, in the same way, sharing it over the feature maps.
In other words, this means that the parameters used to normalize are calculated along with each entire feature map. In a regular Batch Norm, each feature would have a different mean and standard deviation. Here, each feature map will have a single mean and standard deviation, used on all the features it contains.
"
Deep Learning,Regularization,Root Mean Square Layer Normalization,"Layer normalization (LayerNorm) has been successfully applied to various deep neural networks to help stabilize training and boost model convergence because of its capability in handling re-centering and re-scaling of both inputs and weight matrix. However, the computational overhead introduced by LayerNorm makes these improvements expensive and significantly slows the underlying network, e.g. RNN in particular. 
RMSNorm regularizes the summed inputs to a neuron in one layer according to root mean square (RMS), giving the model re-scaling invariance property and implicit learning rate adaptation ability. RMSNorm is computationally simpler and thus more efficient than LayerNorm Extensive experiments on several tasks using diverse network architectures show that RMSNorm achieves comparable performance against LayerNorm but reduces the running time by 7%∼64% on different models.
y= γ * x / sqrt(RMS[x] + ϵ)
"
Deep Learning,Segmentation,Semantic Segmentation,"Semantic segmentation is a natural step in the progression from coarse to fine inference: The origin could be located at classification, which consists of making a prediction for a whole input. The next step is localization / detection, which provide not only the classes but also additional information regarding the spatial location of those classes. Finally, semantic segmentation achieves fine-grained inference by making dense predictions inferring labels for every pixel, so that each pixel is labeled with the class of its enclosing object ore region.
"
Deep Learning,Segmentation,Metrics for Semantic Segmentation,"Let's discuss the metrics which are generally used to understand and evaluate the results of a model.
Pixel Accuracy
Pixel accuracy is the most basic metric which can be used to validate the results. Accuracy is obtained by taking the ratio of correctly classified pixels w.r.t total pixels
Accuracy = (TP+TN) / (TP+TN+FP+FN)
The main disadvantage of using such a technique is the result might look good if one class overpowers the other. Say for example the background class covers 90% of the input image we can get an accuracy of 90% by just classifying every pixel as background
Intersection Over Union
IOU is defined as the ratio of intersection of ground truth and predicted segmentation outputs over their union. If we are calculating for multiple classes, IOU of each class is calculated and their mean is taken. It is a better metric compared to pixel accuracy as if every pixel is given as background in a 2 class input the IOU value is (90/100+0/100)/2 i.e 45% IOU which gives a better representation as compared to 90% accuracy.
When using this approximation, IoU becomes differentiable and can be used as a loss function. The comparison between IoU loss and Binary Cross Entropy loss is made by testing two deep neural network models on multiple datasets and data splits.
Frequency weighted IOU
This is an extension over mean IOU which we discussed and is used to combat class imbalance. If one class dominates most part of the images in a dataset like for example background, it needs to be weighed down compared to other classes. Thus instead of taking the mean of all the class results, a weighted mean is taken based on the frequency of the class region in the dataset.
F1 Score
The metric popularly used in classification F1 Score can be used for segmentation task as well to deal with class imbalance.
Average Precision
The average precision (AP) is a way to summarize the precision-recall curve into a single value representing the average of all precisions. The AP is calculated according to the next equation. Using a loop that goes through all precisions/recalls, the difference between the current and next recalls is calculated and then multiplied by the current precision. In other words, the AP is the weighted sum of precisions at each threshold where the weight is the increase in recall.
"
Deep Learning,Segmentation,Loss functions for Semantic Segmentation,"Cross Entropy Loss
Simple average of cross-entropy classification loss for every pixel in the image can be used as an overall function. But this again suffers due to class imbalance which FCN proposes to rectify using class weights
UNet tries to improve on this by giving more weight-age to the pixels near the border which are part of the boundary as compared to inner pixels as this makes the network focus more on identifying borders and not give a coarse output.
Focal Loss
Focal loss was designed to make the network focus on hard examples by giving more weight-age and also to deal with extreme class imbalance observed in single-stage object detectors. The same can be applied in semantic segmentation tasks as well
Dice Loss
Dice function is nothing but F1 score. This loss function directly tries to optimize F1 score. Similarly direct IOU score can be used to run optimization as well
Tversky Loss
It is a variant of Dice loss which gives different weight-age to FN and FP
Boundary loss
One variant of the boundary loss is applied to tasks with highly unbalanced segmentations. This loss’s form is that of a distance metric on space contours and not regions. In this manner, it tackles the problem posed by regional losses for highly imbalanced segmentation tasks.
Hausdorff distance
It is a technique used to measure similarity between boundaries of ground truth and predicted. It is calculated by finding out the max distance from any point in one boundary to the closest point in the other. Reducing directly the boundary loss function is a recent trend and has been shown to give better results especially in use-cases like medical image segmentation where identifying the exact boundary plays a key role.
The advantage of using a boundary loss as compared to a region based loss like IOU or Dice Loss is it is unaffected by class imbalance since the entire region is not considered for optimization, only the boundary is considered.
"
Deep Learning,Segmentation,What Semantic Segmentation models do you know?,"U-Net
U-Net is a convolutional neural network originally developed for segmenting biomedical images. When visualized its architecture looks like the letter U and hence the name U-Net. Its architecture is made up of two parts, the left part – the contracting path and the right part – the expansive path. The purpose of the contracting path is to capture context while the role of the expansive path is to aid in precise localization.
The contracting path is made up of two three-by-three convolutions. The convolutions are followed by a rectified linear unit and a two-by-two max-pooling computation for downsampling.
FastFCN —Fast Fully Convolutional Network
In this architecture, a Joint Pyramid Upsampling(JPU) module is used to replace dilated convolutions since they consume a lot of memory and time. It uses a fully-connected network at its core while applying JPU for upsampling. JPU upsamples the low-resolution feature maps to high-resolution feature maps.
DeepLab
In this architecture, convolutions with upsampled filters are used for tasks that involve dense prediction. Segmentation of objects at multiple scales is done via atrous spatial pyramid pooling. Finally, DCNNs are used to improve the localization of object boundaries. Atrous convolution is achieved by upsampling the filters through the insertion of zeros or sparse sampling of 
Mask R-CNN
In this architecture, objects are classified and localized using a bounding box and semantic segmentation that classifies each pixel into a set of categories. Every region of interest gets a segmentation mask. A class label and a bounding box are produced as the final output. The architecture is an extension of the Faster R-CNN. The Faster R-CNN is made up of a deep convolutional network that proposes the regions and a detector that utilizes the regions.
"
Deep Learning,Detection,What is object detection?,"Object detection is the field of computer vision that deals with the localization and classification of objects contained in an image or video.
To put it simply: Object detection comes down to drawing bounding boxes around detected objects which allow us to locate them in a given scene (or how they move through it).
"
Deep Learning,Detection,Object detection vs image segmentation,"Image segmentation is the process of defining which pixels of an object class are found in an image. 
Semantic image segmentation will mark all pixels belonging to that tag, but won’t define the boundaries of each object.
Object detection instead will not segment the object, but will clearly define the location of each individual object instance with a box.
Combining semantic segmentation with object detection leads to instance segmentation, which first detects the object instances, and then segments each within the detected boxes (known in this case as regions of interest).
"
Deep Learning,Detection,Types and modes of object detection,"Before deep learning took off in 2013, almost all object detection was done through classical machine learning techniques. Common ones included viola-jones object detection technique, scale-invariant feature transforms (SIFT), and histogram of oriented gradients.  
These would detect a number of common features across the image, and classify their clusters using logistic regression, color histograms, or random forests. Today’s deep learning-based techniques vastly outperform these.
Deep learning-based approaches use neural network architectures like RetinaNet, YOLO (You Only Look Once), CenterNet, SSD (Single Shot Multibox detector), Region proposals (R-CNN, Fast-RCNN, Faster RCNN, Cascade R-CNN) for feature detection of the object, and then identification into labels.
Object detection generally is categorized into 2 stages:
Single-stage object detectors.
A single-stage detector removes the RoI extraction process and directly classifies and regresses the candidate anchor boxes. Examples are: YOLO family (YOLOv2, YOLOv3, YOLOv4, and YOLOv5) CornerNet, CenterNet, and others.
Two-stage object detectors.
Two-stage detectors divide the object detection task into two stages: extract RoIs (Region of interest), then classify and regress the RoIs. Examples of object detection architectures that are 2 stage oriented include R-CNN, Fast-RCNN, Faster-RCNN, Mask-RCNN and others.
"
Deep Learning,Detection,R-CNN Model Family ,"Their proposed R-CNN model is comprised of three modules; they are:
Module 1: Region Proposal. Generate and extract category independent region proposals, e.g. candidate bounding boxes.
Module 2: Feature Extractor. Extract feature from each candidate region, e.g. using a deep convolutional neural network.
Module 3: Classifier. Classify features as one of the known class, e.g. linear SVM classifier model.
The R-CNN Model family includes the following:
R-CNN—This utilizes a selective search method to locate RoIs in the input images and uses a DCN (Deep Convolutional Neural Network)-based region wise classifier to classify the RoIs independently. 
SPPNet and Fast R-CNN—This is an improved version of R-CNN that deals with the extraction of the RoIs from the feature maps. This was found to be much faster than the conventional R-CNN architecture.
Faster R-CNN—This is an improved version of Fast R-CNN that was trained end to end by introducing RPN (region proposal network). An RPN is a network utilized in generating RoIs by regressing the anchor boxes. Hence, the anchor boxes are then used in the object detection task. 
Mask R-CNN adds a mask prediction branch on the Faster R-CNN, which can detect objects and predict their masks at the same time. 
R-FCN  replaces the fully connected layers with the position-sensitive score maps for better detecting objects. 
Cascade R-CNN addresses the problem of overfitting at training and quality mismatch at inference by training a sequence of detectors with increasing IoU thresholds.
"
Deep Learning,Detection,YOLO Model Family,"The model works by first splitting the input image into a grid of cells, where each cell is responsible for predicting a bounding box if the center of a bounding box falls within the cell. Each grid cell predicts a bounding box involving the x, y coordinate and the width and height and the confidence. A class prediction is also based on each cell.
For example, an image may be divided into a 7×7 grid and each cell in the grid may predict 2 bounding boxes, resulting in 94 proposed bounding box predictions. The class probabilities map and the bounding boxes with confidences are then combined into a final set of bounding boxes and class labels. The image taken from the paper below summarizes the two outputs of the model.
The YOLO family model includes the following:
YOLO uses fewer anchor boxes (divide the input image into an S × S grid) to do regression and classification. This was built using darknet neural networks.
YOLOv2 improves the performance by using more anchor boxes and a new bounding box regression method. 
YOLOv3 is an enhanced version of the v2 variant with a deeper feature detector network and minor representational changes. YOLOv3 has relatively speedy inference times with it taking roughly 30ms per inference.
YOLOv4 (YOLOv3 upgrade) works by breaking the object detection task into two pieces, regression to identify object positioning via bounding boxes and classification to determine the object's class. YOLO V4 and its successors are technically the product of a different set of researchers than versions 1-3.
YOLOv5 is an improved version of YOLOv4 with a mosaic augmentation technique for increasing the general performance of YOLOv4.
"
Deep Learning,Detection,CenterNet,"CenterNet is a deep detection architecture that removes the need for anchors and the computationally heavy NMS. It is based on the insight that box predictions can be sorted for relevance based on the location of their centers, rather than their overlap with the object. This insight is now being used in other deep learning tasks.
The CenterNet family model includes the following:
SSD places anchor boxes densely over an input image and uses features from different convolutional layers to regress and classify the anchor boxes. 
DSSD introduces a deconvolution module into SSD to combine low level and high-level features. While R-SSD uses pooling and deconvolution operations in different feature layers to combine low-level and high-level features.
RON proposes a reverse connection and an objectness prior to extracting multiscale features effectively.
RefineDet refines the locations and sizes of the anchor boxes for two times, which inherits the merits of both one-stage and two-stage approaches. 
CornerNet is another keypoint-based approach, which directly detects an object using a pair of corners. Although CornerNet achieves high performance, it still has more room to improve.
CenterNet explores the visual patterns within each bounding box. For detecting an object, this uses a triplet, rather than a pair, of keypoints. CenterNet evaluates objects as single points by predicting the x and y coordinate of the object’s center and it’s area of coverage (width and height). It is a unique technique that has proven to out-perform variants like the SSD and R-CNN family.  
"
Deep Learning,Optimization,Neural Network Optimization,"Pruning
Quantization
Post Training Quantization: reducing the size of the weights stored (e.g. from 32-bit floating point numbers to 8-bit)
Quantization-Aware Training: There could be an accuracy loss in a post-training model quantization and to avoid this and if you don’t want to compromise the model accuracy we do quantization aware training. This technique ensures that the forward pass matches precision for both training and inference.
Low-rank approximation and sparsity 
Main idea is to approximate the redundant filters of a layer using a linear combination of fewer filters. Compressing layers in this way reduces the network’s memory footprint, the computational complexity of convolutional 	operations and can yield significant speedups. 
Examples: 
Singular Value Decomposition 
Tucker decomposition 
Canonical Polyadic decomposition
Knowledge distillation 
Trained to minimize the sum of two different cross entropy functions:
one involving the original hard labels obtained using a softmax with T=1 
one involving the softened targets, T>1
"
Deep Learning,Optimization,Pruning,"Pruning is a technique of removing unimportant parameters (weights) of a deep neural network. It aims to achieve several goals:
Reduction in storage (smaller file size)
Speed-up during inference (testing)
There are two main types of pruning techniques namely: Structured Pruning and Unstructured Pruning.
Structured Pruning
Structured pruning removes a structure (building block) of the target neural network such as:
Neuron for a Fully Connected Layer
Channel of filter for a Convolution Layer
Self-attention head for Transformer
Benefits of structured pruning:
Speed-up during inference (testing)
Trains a neural network up to certain level of performance
Prune the p% of the neurons / channels of the network
Fine-tune or retrain the pruned network for a few epochs
Repeats prune and retrain/fine-tune
Unstructured Pruning
Unstructured pruning is also called magnitude pruning. Unstructured pruning converts some of the parameters or weights with smaller magnitude into zeros.
Dense: lots of non-zero values
Sparse: lots of zeros
Unstructured pruning converts an original dense network into a sparse network. The size of the parameter matrix (or weight matrix) of the sparse network is the same as the size of parameter matrix of the original network. Sparse network has more zeros in their parameter matrix.

For unstructured pruning, adds a L1 or L2 regularization term to the loss function which penalizes non-zero parameters. During training, the regularization will push the non-zero parameters to zeros.
During pruning, set a threshold. Prune parameters with magnitude smaller than threshold, and keep parameters with magnitude larger than threshold. The pruned parameters are set to zero and frozen.
Benefits of unstructured pruning:
Reduction in storage (smaller file size)
"
Deep Learning,Optimization,Distillation of Knowledge in Neural Networks,"Knowledge distillation is used to compress a complex and large neural network into a smaller and simpler one, while still retaining the accuracy and performance of the resultant model. This process involves training a smaller neural network to mimic the behavior of a larger and more complex ""teacher"" network by learning from its predictions or internal representations.
The goal of knowledge distillation is to reduce the memory footprint and computational requirements of a model without significantly sacrificing its performance. Knowledge distillation was first introduced by Hinton et al. in 2015. Since then, the idea has gained significant attention in the research community.
In the context of knowledge distillation, the ""knowledge"" captured by the teacher network is transferred to the student network through a process of supervised learning. During this process, the student network is trained to minimize the difference between its predictions and the predictions of the teacher network on a set of training examples.
Knowledge distillation involves two main steps: training the teacher network and training the student network.
During the first step, a large and complex neural network, or the teacher network, is trained on a dataset using a standard training procedure. Once the teacher network has been trained, it is used to generate ""soft"" labels for the training data, which are probability distributions over the classes instead of binary labels. These soft labels are more informative than hard labels and capture the uncertainty and ambiguity in the predictions of the teacher network.
In the second step, a smaller neural network, or the student network, is trained on the same dataset using the soft labels generated by the teacher network. The student network is trained to minimize the difference between its own predictions and the soft labels generated by the teacher network.
The intuition behind this approach is that the soft labels contain more information about the input data and the teacher network's predictions than the hard labels. Therefore, the student network can learn to capture this additional information and generalize better to new examples.
"
Deep Learning,Optimization,Floating Point Precision,"Floating Point Precision is a representation of a number through binary. To communicate numbers to our computers, values must be translated to binary 1s and 0s, in which the order and sequence of digits matter. The most common Floating Point Precision formats are Half Precision (FP16), Single Precision (FP32) and Double Precision (FP64), each with their own advantages, disadvantages, and usefulness in specific applications.
Floating Point Precision are structured in a way such that it can define a wide range of values. The first binary digit represents the sign for the number to be positive or negative. The next set of binary digits is the exponent with a base of 2 representing the whole number value. The final set of binary digits represent the significand or mantissa which represents the value after the decimal point. 
FP32
The format that was the workhorse of deep learning for a long time. IEEE 754 format, the single-precision floating-point with:
1 bit sign
8 bits exponent
23 bits fraction
Range: ~1.18e-38 … ~3.40e38 with 6–9 significant decimal digits precision.
Usage:
The standard type for neural network computations for a long time. Weights, activations and other values in neural networks have long been represented in FP32 by default.
For many scientific computations (especially iterative ones) the precision is not enough, leading to accumulation of errors.
Software support:
On most C/C++ systems represents the float type.
Supported in TensorFlow (as tf.float32)/PyTorch (as torch.float32 or torch.float).
Hardware support:
Normally supported in x86 CPUs.
Normally supported in NVIDIA/AMD GPUs.
FP16
Again, the IEEE 754 standard format, the half-precision floating-point format with:
1 bit sign
5 bits exponent
10 bits fraction
Range: ~5.96e−8 (6.10e−5) … 65504 with 4 significant decimal digits precision.
Usage:
There is a trend in DL towards using FP16 instead of FP32 because lower precision calculations seem to be not critical for neural networks. Additional precision gives nothing, while being slower, takes more memory and reduces speed of communication.
Can be used for training, typically using mixed-precision training (TensorFlow/PyTorch).
Can be used for post-training quantization for faster inference (TensorFlow Lite). Other formats in use for post-training quantization are integer INT8 (8-bit integer), INT4 (4 bits) and even INT1 (a binary value).
Software support:
Currently not in the C/C++ standard (but there is a short float proposal). Some C/C++ systems support __fp16 type. Otherwise, can be used with special libraries.
Supported in TensorFlow (as tf.float16)/PyTorch (as torch.float16 or torch.half).
Hardware support:
Not supported in x86 CPUs (as a distinct type).
Was poorly supported on older gaming GPUs (with 1/64 performance of FP32, see the post on GPUs for more details). Right now well-supported on modern GPUs, e.g. NVIDIA RTX series.
BFLOAT16
Another 16-bit format originally developed by Google is called “Brain Floating Point Format”, or “bfloat16” for short. The name flows from “Google Brain”, which is an artificial intelligence research group at Google where the idea for this format was conceived.
The original IEEE FP16 was not designed with deep learning applications in mind, its dynamic range is too narrow. BFLOAT16 solves this, providing dynamic range identical to that of FP32.
So, BFLOAT16 has:
1 bit sign
8 bits exponent
7 bits fraction
The bfloat16 format, being a truncated IEEE 754 FP32, allows for fast conversion to and from an IEEE 754 FP32. In conversion to the bfloat16 format, the exponent bits are preserved while the significand field can be reduced by truncation.
Usage:
Seems to be replacing FP16 right now. Unlike FP16, which typically requires special handling via techniques such as loss scaling, BF16 comes close to being a drop-in replacement for FP32 when training and running deep neural networks.
TF32
TensorFloat-32, or TF32, is the new math mode in NVIDIA A100 GPUs.
TF32 uses the same 10-bit mantissa as the half-precision (FP16) math, shown to have more than sufficient margin for the precision requirements of AI workloads. And TF32 adopts the same 8-bit exponent as FP32 so it can support the same numeric range.
So, TF32 has:
1 bit sign
8 bits exponent
10 bits fraction
The advantage of TF32 is that the format is the same as FP32. When computing inner products with TF32, the input operands have their mantissas rounded from 23 bits to 10 bits. The rounded operands are multiplied exactly, and accumulated in normal FP32.
FP8
Introduced in the paper, FP8 Formats for Deep Learning, two types of fp8 encodings were introduced, E4M3 and E5M2, with E standing for exponents and M for mantissa.
E4M3 — it consists of 1 sign bit, 4 exponent bits and 3 bits of mantissa. It can store values up to +/-448 and nan.
E5M2 — it consists of 1 sign bit, 5 exponent bits and 2 bits of mantissa. It can store values up to +/-57344, +/- inf and nan. The tradeoff of the increased dynamic range is lower precision of the stored values.
NF4
In QLoRA’s paper, one of the innovative things introduced is the Normal Float 4 bit, NF4. QLoRA stores weights in NF4 and uses BF16 for computation. So the compressesd bits are not the same during training, they are decompressed to 16 bits of BF16 in most cases.
NF4 is a datatype adapted for weights that have been initialized using a normal distribution.
"
Deep Learning,Miscellaneous,What are some of the uses of Autoencoders in Deep Learning?,"An autoencoder is a type of artificial neural network used to learn data encodings in an unsupervised manner.
The aim of an autoencoder is to learn a lower-dimensional representation (encoding) for a higher-dimensional data, typically for dimensionality reduction, by training the network to capture the most important parts of the input image.
Autoencoders are used to convert black and white images into colored images.
Autoencoder helps to extract features and hidden patterns in the data.
It is also used to reduce the dimensionality of data.
It can also be used to remove noises from images.
"
Deep Learning,Miscellaneous,What is the Computational Graph?,"A computational graph is a graphical presentation that is based on TensorFlow. It has a wide network of different kinds of nodes wherein each node represents a particular mathematical operation. The edges in these nodes are called tensors. This is the reason the computational graph is called a TensorFlow of inputs. The computational graph is characterized by data flows in the form of a graph; therefore, it is also called the DataFlow Graph.
Forward pass is the procedure for evaluating the value of the mathematical expression represented by computational graphs. Doing forward pass means we are passing the value from variables in forward direction from the left (input) to the right where the output is.
In the backward pass, our intention is to compute the gradients for each input with respect to the final output. These gradients are essential for training the neural network using gradient descent.
"
Deep Learning,Miscellaneous,Overconfidence,"Overfitting – The model memorizes training data too well and assumes its predictions are always correct.
Poor Probability Calibration – Deep neural networks tend to output overconfident probabilities, even when wrong.
Lack of Diverse Data – If the model is trained on a narrow dataset, it may not encounter complex or uncertain cases, leading to overconfidence.
Loss Function Choice – Cross-entropy loss can encourage excessive confidence if not regularized properly.
Softmax Overconfidence – Softmax amplifies probabilities, making the model appear more confident than it should be.
How to Fix Overconfidence?
Temperature Scaling 
Adjusts probability sharpness to make predictions less extreme.
Label Smoothing 
Spreads some probability mass to other classes instead of making a single class 100% confident.
Ensemble Methods 
Combining multiple models and averaging their predictions reduces overconfidence.
Bayesian Neural Networks & Monte Carlo Dropout 🎲
Introduces uncertainty estimation by enabling dropout during inference:
Proper Regularization 
L1/L2 regularization, Dropout, BatchNorm, and Data Augmentation help prevent excessive confidence.
How to Detect Overconfidence?
Use calibration metrics like Reliability Diagrams or Expected Calibration Error (ECE).
If the model often assigns high probabilities to wrong predictions, it's likely overconfident.
Conclusion:
To reduce overconfidence, use temperature scaling, label smoothing, ensembling, Bayesian methods, and proper regularization.
"
LLM,Transformers,Transformers,"A transformer is a deep learning architecture developed by researchers at Google and based on the multi-head attention mechanism, proposed in a 2017 paper ""Attention Is All You Need"".
The transformer architecture is composed of an encoder and a decoder, each of which is made up of multiple layers of self-attention and feedforward neural networks. The self-attention mechanism is the heart of the transformer, allowing the model to weigh the importance of different words in a sentence based on their affinity with each other. This is similar to how a human might read a sentence, focusing on the most relevant parts of the text rather than reading it linearly from beginning to end.
In addition to self-attention, the transformer also introduces positional bias, which allows the model to keep track of the relative positions of words in a sentence. This is important because the order of words in a sentence can significantly impact its meaning.
"
LLM,Transformers,Attention,"Attention allowed us to focus on parts of our input sequence while we predicted our output sequence
Self attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.
In simpler terms, self attention helps us create similar connections but within the same sentence. Look at the following example:
“I poured water from the bottle into the cup until it was full.”
it => cup“I poured water from the bottle into the cup until it was empty.”
it=> bottle
By changing one word “full” — > “empty” the reference object for “it” changed. If we are translating such a sentence, we will want to know the word “it” refers to.
The three kinds of Attention possible in a model:
Encoder-Decoder Attention: Attention between the input sequence and the output sequence.
Self attention in the input sequence: Attends to all the words in the input sequence.
Self attention in the output sequence: One thing we should be wary of here is that the scope of self attention is limited to the words that occur before a given word. This prevents any information leaks during the training of the model. This is done by masking the words that occur after it for each step. So for step 1, only the first word of the output sequence is NOT masked, for step 2, the first two words are NOT masked and so on.
Keys, Values, and Queries:
The three random words I just threw at you in this heading are vectors created as abstractions are useful for calculating self attention, more details on each below. These are calculated by multiplying your input vector(X) with weight matrices that are learnt while training.
Query Vector: q= X * Wq. Think of this as the current word.
Key Vector: k= X * Wk. Think of this as an indexing mechanism for Value vector. Similar to how we have key-value pairs in hash maps, where keys are used to uniquely index the values.
Value Vector: v= X * Wv. Think of this as the information in the input word.
What we want to do is take query q and find the most similar key k, by doing a dot product for q and k. The closest query-key product will have the highest value, followed by a softmax that will drive the q.k with smaller values close to 0 and q.k with larger values towards 1. This softmax distribution is multiplied with v. The value vectors multiplied with ~1 will get more attention while the ones ~0 will get less. The sizes of these q, k and v vectors are referred to as “hidden size” by various implementations.
The values represent the index for q, k and i.
All these matrices Wq, Wk and Wv are learnt while being jointly trained during the model training.
Calculating Self attention from q, k and v:
If we are calculating self attention for #i input word,
Step 1: Multiply qᵢ by the kⱼ key vector of word.
Step 2: Then divide this product by the square root of the dimension of key vector.
This step is done for better gradient flow which is specially important in cases when the value of the dot product in previous step is too big. As using them directly might push the softmax into regions with very little gradient flow.
Step 3: Once we have scores for all js, we pass these through a softmax. We get normalized value for each j.
Step 4: Multiply softmax scores for each j with vᵢ vector.
The idea/purpose here is, very similar attention, to keep preserve only the values v of the input word(s) we want to focus on by multiplying them with high probability scores from softmax ~1, and remove the rest by driving them towards 0, i.e. making them very small by multiplying them with the low probability scores ~0 from softmax.
Calculating output of self attention for the ith input word. If you are looking for an analogy between self attention and attention, think of z serving the purpose of context vectors and not global alignment weights.
"
LLM,Transformers,Multi-Head Attention,"The output of each sub-layer needs to be of the same dimension which is 512 in our paper.
=> zᵢ needs to be of 512 dimensions.
=> vᵢ needs to be of 512 dimensions as zᵢ are just sort of weighted sums of vᵢs.
Additionally, we want to allow the model to focus on different positions is by calculating self attention multiple times with different sets of q, k and v vectors, then take an average of all those outputs to get our final z.
So instead of dealing with these humongous vectors and averaging multiple outputs, we reduce the size of our k,q and v vectors to some smaller dimension — reduces size of Wq, Wk, and Wv matrices as well. We keep the multiple sets (h) of k, q and v and refer to each set as an “attention head”, hence the name multi-headed attention. And lastly, instead of averaging to get final z, we concatenate them.
The size of the concatenated vector will be too large to be fed to the next sub-layer, so we scale it down by multiplying it with another learnt matrix Wo.
Multiple attention heads allowed the model to jointly attend to information from different representation sub-spaces at different positions which was inhibited by averaging in a single attention head.
"
LLM,Transformers,Key-Value Cache,"As suggested by its name, Key-Value cache is a technique designed to speedup the autoregressive process by caching and reusing the previous keys and values, rather than re-computing them at each decoding step.
Note that KV cache is typically used only during the inference stage, since in training we still need to process the entire input sequence in parallel.
KV cache is commonly implemented as a rolling buffer. At each decoding step, only the new query Q is computed, while the K and V stored in the cache will be reused, so that the attention will be computed using the new Q and reused K, V. Meanwhile, the new token’s K and V will also be appended to the cache for later use.
However, the speedup achieved by KV cache comes at a cost of memory, since KV cache often scales with batch size × sequence length × hidden size × number of heads, leading to a memory bottleneck when we have larger batch size or longer sequences.
That further leads to two techniques aiming at addressing this limitation: Multi-Query Attention and Grouped-Query Attention.
"
LLM,Transformers,Multi-Query Attention(MQA),"Multi-Query Attention is a variation of multi-head attention.
The approach of MQA is to keep the original number of heads for Q, but have only one head for K and V. This means that all the Q heads share the same set of K and V heads, hence the name Multi-Query.
In MQA, the size of the key and value tensors is b * k and b * v, while in MHA, the size of the key and value is b * h * k and b * h * v, where h represents the number of heads.
In general, MQA achieves inference acceleration through the following methods:
The KV cache size is reduced by a factor of h(number of heads), which means that the tensors that need to be stored in the GPU memory are also reduced. The space saved can be used to increase the batch size, thereby improving efficiency.
The amount of data read from memory is reduced, which reduces the waiting time for computational units and improves computational utilization.
MQA has a relatively small KV cache that can fit into the cache (SRAM). MHA, on the other hand, has a larger KV cache that cannot be entirely stored in the cache and needs to be read from the GPU memory (DRAM), which is time-consuming.
"
LLM,Transformers,Grouped Query Attention (GQA),"Grouped Query Attention simplifies how LLMs understand large amounts of text by bundling similar pieces together. This makes the model faster and smarter, as it can focus on groups of words at a time instead of each word individually.
Grouped Query Attention (GQA) is a method that interpolates between multi-query attention (MQA) and multi-head attention (MHA) in Large Language Models (LLMs). It aims to achieve the quality of MHA while maintaining the speed of MQA.
In GQA, query heads are divided into groups, each of which shares a single key head and value head. This approach allows GQA to interpolate between multi-head and multi-query attention, achieving a balance between quality and speed. For instance, GQA with a single group (and therefore a single key and value head) is equivalent to MQA, while GQA with groups equal to the number of heads is equivalent to MHA.
"
LLM,Transformers,Multi-head Latent Attention,"The basic idea of MLA is to compress the attention input h_t into a low-dimensional latent vector with dimension d_c, where d_c is much lower than the original (h_n · d_h). Later when we need to calculate attention, we can map this latent vector back to the high-dimensional space to recover the keys and values. As a result, only the latent vector needs to be stored, leading to significant memory reduction.
This process can be more formally described with the following equations, where c^{KV}_t is the latent vector, W^{DKV} is the compressing matrix that maps h_t‘s dimension from (h_n · d_h) to d_c (here D in the superscript stands for ""down-projection"", meaning compressing the dimension), while W^{UK} and W^{UV} are both up-projection matrices that map the shared latent vector back to the high-dimensional space.
Similarly, we can also map the queries into a latent, low-dimensional vector and then map it back to the original, high-dimensional space:
"
LLM,Transformers,Encoder,"Positional Encoding
The input sequence is first embedded into a high-dimensional vector space using an embedding layer, which maps each token in the sequence to a vector representation. After that, these vector representations got elementwise added with the positional embedding.
2. Multi-Head Attention:
Combined input and positional embedding then fee into the first Encoder block here first combined embedding is passed through the multi-head attention layer.
3. Layer Normalization and Residual
Output from the Multi-head attention and initial combined embedding (as a residual) is passed through the layer Normalization layer.
4. Feed Forward Neural Network :
Output after layer Normalization is passed through a feed-forward neural network.
5. Layer Normalization and Residual
Finally, the output of the Feed Forward Neural Network is added with an output of the first layer normalization, and then the combined vector is passed layer normalization layer.
"
LLM,Transformers,Decoder,"Given z, the decoder then generates an output sequence (y, …, yₘ) of symbols one element at a time.
Decoder Input Embeddings & Positional Encoding
The decoder’s starting is very similar to that of the encoder. To obtain positional embeddings, the input passes through layers of positional encoding and embedding. The first multi-head attention layer receives the positional embeddings and uses them to calculate the attention scores for the input from the decoder.
Decoders First Multi-Headed Attention
The way that this multi-headed attention layer function is a little different. You must stop the decoder from conditioning to upcoming tokens because it generates the sequence word by word and is autoregressive. The word “fine,” for instance, should not be available when calculating attention ratings for the word “am” because it is a future term that was created subsequently. Just the words that come before and after the word “am” should have access to each other. All other words can only pay attention to prior words, and this is true for all other words.
We require a technique to stop attention scores from being calculated for future words. This process is known as masking. You apply a look ahead mask to stop the decoder from seeing tokens in the future. Before figuring out the softmax and after scaling the scores, the mask is added. Let’s examine how this functions.
Encoder-Decoder Attention
In encoder-decoder attention, VALUE and KEY are from the output of the encoder, QUERY is from the output of the self-attention layer in the decoder. The model is going to train on the transformation matrix to apply to value, key, and query according. The output of encoder-decoder attention is still the weighted sum of values with the weights as the output from the softmax of the product between key and query.
Look-Ahead Mask
The mask is a matrix with values of 0 and negative infinities that is the same size as the attention ratings. The upper right triangle of the score matrix is filled with negative infinities when the mask is added to the scaled attention scores.
The mask is used because, when the masked scores are softmaxed, the negative infinities are zeroed out, leaving no attention scores for subsequent tokens. As you can see in the graph below, the attention score for “am” is 0 for the word “fine,” but it has values for all words that come before it. In essence, this instructs the model to ignore those terms.
The first multi-headed attention layer’s calculation of the attention scores only differs in this masking. The mask is still being applied to several heads in this layer before being concatenated and passed through a linear layer for additional processing. A masked output vector with instructions on how the model should pay attention to the decoder’s input is the result of the initial multi-headed attention.
Point-wise Feed Forward Layer with Second Multi-Headed Decoder
The second attention tier with many heads. The first multi-headed attention layer’s outputs are the values for this layer, whereas the encoder’s outputs are the queries and keys. The decoder can choose which encoder input to focus on by using this technique to match the encoder’s input to the decoder’s input. The second multi-headed attention’s output is processed further after passing through a pointwise feedforward layer.
Final Softmax with a Linear Classifier for Output Probabilities
The output of the last linear layer, which serves as a classifier, is passed through the final pointwise feedforward layer. As many classes as you have determined how large the classifier is. For instance, the output of that classier will be 10,000 words in size if you have 10,000 classes for 10,000 words. The softmax layer receives the classifier’s output and generates probability scores between 0 and 1. Our anticipated word is equivalent to the index with the highest likelihood score.
Once a token is anticipated, the decoder adds the output to its list of inputs and starts the decoding process all over again. In our situation, the final class that is given to the end token is the prediction with the highest probability.
The decoder may alternatively be layered N layers high, with each layer receiving input from the layers above it as well as the encoder. The model can learn to extract and concentrate on various attentional combinations from its attention heads by stacking the layers, potentially improving its prediction power.
"
LLM,Transformers,Decoder Stack,"The output of the decoder stack at each step is fed back to the decoder in the next time step — pretty similar to how outputs from previous steps in RNNs were used as next hidden states. And just as we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to preserve the position of each word. This positional encoding + word embedding combo is then fed into a masked multi-headed self attention.
This self-attention sub-layer in the decoder stack is modified to prevent positions from attending to subsequent positions — you can’t look at future words. This masking ensures that the predictions for position i can depend only on the known outputs at positions less than i.
The outputs from the encoder stack are then used as multiple sets of key vectors k and value vectors v, for the “encoder decoder attention” layer. It helps the decoder focus on the contextually relevant parts in the input sequence for that step. (The part similar to global attention vectors.) The q vector comes from the “output self attention” layer.
Once we get the output from the decoder, we do a softmax again to select the final probabilities of words.
"
LLM,Transformers,Transformer Encoder-Decoder Architecture,"The transformer encoder-decoder architecture is used for tasks like language translation, where the model must take in a sentence in one language and output a sentence in another language. The encoder takes in the input sentence and produces a fixed-size vector representation of it, which is then fed into the decoder to generate the output sentence. The decoder uses both self-attention and cross-attention, where the attention mechanism is applied to the output of the encoder and the input of the decoder.
One of the most popular transformer encoder-decoder models is the T5 (Text-to-Text Transfer Transformer), which was introduced by Google in 2019. The T5 can be fine-tuned for a wide range of NLP tasks, including language translation, question answering, summarization, and more.
Real-world examples of the transformer encoder-decoder architecture include Google Translate, which uses the T5 model to translate text between languages, and Facebook’s M2M-100, a massive multilingual machine translation model that can translate between 100 different languages.
"
LLM,Transformers,Transformer Encoder Architecture,"The transformer encoder architecture is used for tasks like text classification, where the model must classify a piece of text into one of several predefined categories, such as sentiment analysis, topic classification, or spam detection. The encoder takes in a sequence of tokens and produces a fixed-size vector representation of the entire sequence, which can then be used for classification.
One of the most popular transformer encoder models is BERT (Bidirectional Encoder Representations from Transformers), which was introduced by Google in 2018. BERT is pre-trained on large amounts of text data and can be fine-tuned for a wide range of NLP tasks.
Unlike the encoder-decoder architecture, the transformer encoder is only concerned with the input sequence and does not generate any output sequence. It applies self-attention mechanism to the input tokens, allowing it to focus on the most relevant parts of the input for the given task.
Real-world examples of the transformer encoder architecture include sentiment analysis, where the model must classify a given review as positive or negative, and email spam detection, where the model must classify a given email as spam or not spam.
"
LLM,Transformers,Transformer Decoder Architecture,"The transformer decoder architecture is used for tasks like language generation, where the model must generate a sequence of words based on an input prompt or context. The decoder takes in a fixed-size vector representation of the context and uses it to generate a sequence of words one at a time, with each word being conditioned on the previously generated words.
One of the most popular transformer decoder models is the GPT-3 (Generative Pre-trained Transformer 3), which was introduced by OpenAI in 2020. The GPT-3 is a massive language model that can generate human-like text in a wide range of styles and genres.
The transformer decoder architecture introduces a technique called triangle masking for attention, which ensures that the attention mechanism only looks at tokens to the left of the current token being generated. This prevents the model from “cheating” by looking at tokens that it hasn’t generated yet.
Real-world examples of the transformer decoder architecture include text generation, where the model must generate a story or article based on a given prompt or topic, and chatbots, where the model must generate responses to user inputs in a natural and engaging way.
"
LLM,Transformers,Drawbacks of Transformers,"The drawbacks of the transformer architecture are:
High computational cost due to the attention mechanism, which increases quadratically with sequence length.
Difficulty in interpretation and debugging due to the attention mechanism operating over the entire input sequence.
Prone to overfitting when fine-tuned on small amounts of task-specific data.
Despite these downsides, the transformer architecture remains a powerful and widely-used tool in NLP, and research is ongoing to mitigate its computational requirements and improve its interpretability and robustness.
"
LLM,Transformers,GPT vs BERT: What’s The Difference?,"BERT is a Transformer encoder, which means that, for each position in the input, the output at the same position is the same token (or the [MASK] token for masked tokens), that is the inputs and output positions of each token are the same. Models with only an encoder stack like BERT generate all its outputs at once.
BERT has two training objectives, and the most important of them is the Masked Language Modeling (MLM) objective. is With the MLM objective, at step the following happens:
select some tokens
(each token is selected with the probability of 15%)
replace these selected tokens
(with the special token [MASK] - with p=80%, with a random token - with p=10%, with the original token (remain unchanged) - with p=10%)
predict original tokens (compute loss).
The illustration below shows an example of a training step for one sentence. You can go over the slides to see the whole process.
GPT is an autoregressive transformer decoder, which means that each token is predicted and conditioned on the previous token. We don't need an encoder, because the previous tokens are received by the decoder itself. This makes these models really good at tasks like language generation, but not good at classification. These models can be trained with unlabeled large text corpora from books or web articles.
In conclusion, while both GPT and BERT are examples of transformer architectures that have been influencing the field of natural language processing in recent years, they have different strengths and weaknesses that make them suitable for different types of tasks. GPT excels at generating long sequences of text with high accuracy whereas BERT focuses more on the understanding context within given texts in order to perform more sophisticated tasks such as question answering or sentiment analysis. Data scientists, developers, and machine learning engineers should decide which architecture best fits their needs before embarking on any NLP project using either model. Ultimately, both GPT and BERT are powerful tools that offer unique advantages depending on the task at hand.
"
LLM,Positional Embeddings,Input and Output Pre-processing in Transformers,"The input words are represented using some form of embedding. This is done for both encoder and decoder.
Word embedding on their own lack any positional information which is achieved in RNNs by virtue of their sequential nature. Meanwhile in self-attention, due to softmax, any such positional information is lost.
To preserve the positional information, the transformer injects a vector to individual input embeddings (could be using word embeddings for corresponding to the input words). These vectors follow a specific periodic function (Example: combination of various sines/cosines having different frequency, in short not in sync with each other) that the model learns and is able to determine the position of individual word with each other based on the values .
This injected vector is called “positional encoding” and are added to the input embeddings at the bottoms of both encoder and decoder stacks.
"
LLM,Positional Embeddings,Absolute Positional Embeddings,"In the context of a sentence, suppose we have an embedding representing a word. To encode its position, we use another vector of identical dimensionality, where each vector uniquely represents a position in the sentence. For instance, a specific vector is designated for the second word in a sentence. Thus, each sentence position gets its distinct vector. The input for the Transformer layer is then formed by summing the word embedding with its corresponding positional embedding.
There are primarily two methods to generate these embeddings:
Learning from Data: Here, positional vectors are learned during training, just like other model parameters. We learn a unique vector for each position, say from 1 to 512. However, this introduces a limitation — the maximum sequence length is capped. If the model only learns up to position 512, it cannot represent sequences longer than that.
Sinusoidal Functions: This method involves constructing unique embeddings for each position using a sinusoidal function. Although the intricate details of this construction are complex, it essentially provides a unique positional embedding for every position in a sequence. Empirical studies have shown that learning from data and using sinusoidal functions offer comparable performance in real-world models.
Limitations of Absolute Positional Embeddings
Despite their widespread use, absolute positional embeddings are not without drawbacks:
Limited Sequence Length: As mentioned, if a model learns positional vectors up to a certain point, it cannot inherently represent positions beyond that limit.
Independence of Positional Embeddings: Each positional embedding is independent of others. This means that in the model’s view, the difference between positions 1 and 2 is the same as between positions 2 and 500. However, intuitively, positions 1 and 2 should be more closely related than position 500, which is significantly farther away. This lack of relative positioning can hinder the model’s ability to understand the nuances of language structure.
"
LLM,Positional Embeddings,Relative Positional Embeddings,"Article: https://arxiv.org/pdf/2104.09864 
Rather than focusing on a token’s absolute position in a sentence, relative positional embeddings concentrate on the distances between pairs of tokens. This method doesn’t add a position vector to the word vector directly. Instead, it alters the attention mechanism to incorporate relative positional information.
Case Study: The T5 Model
One prominent model that utilizes relative positional embeddings is T5 (Text-to-Text Transfer Transformer). T5 introduces a nuanced way of handling positional information:
Bias for Positional Offsets: T5 uses a bias, a floating-point number, to represent each possible positional offset. For example, a bias B1 might represent the relative distance between any two tokens that are one position apart, regardless of their absolute positions in the sentence.
Integration in Self-Attention Layer: This matrix of relative position biases is added to the product of the query and key matrices in the self-attention layer. This ensures that tokens at the same relative distance are always represented by the same bias, regardless of their position in the sequence.
Scalability: A significant advantage of this method is its scalability. It can extend to arbitrarily long sequences, a clear benefit over absolute positional embeddings.
Challenges with Relative Positional Embeddings
Despite their theoretical appeal, relative positional embeddings pose certain practical challenge.
Performance Issues: Benchmarks comparing T5’s relative embeddings with other types have shown that they can be slower, particularly for longer sequences. This is primarily due to the additional computational step in the self-attention layer, where the positional matrix is added to the query-key matrix.
Complexity in Key-Value Cache Usage: As each additional token alters the embedding for every other token, this complicates the effective use of key-value caches in Transformers. For those unfamiliar, key-value caches are crucial in enhancing efficiency and speed in Transformer models.
Due to these engineering complexities, relative embeddings haven’t been widely adopted, especially in larger language models.
"
LLM,Positional Embeddings,Rotary Positional Embeddings,"RoPE represents a novel approach in encoding positional information. Traditional methods, either absolute or relative, come with their limitations. Absolute positional embeddings assign a unique vector to each position, which though straightforward, doesn’t scale well and fails to capture relative positions effectively. Relative embeddings, on the other hand, focus on the distance between tokens, enhancing the model’s understanding of token relationships but complicating the model architecture.
RoPE ingeniously combines the strengths of both. It encodes positional information in a way that allows the model to understand both the absolute position of tokens and their relative distances. This is achieved through a rotational mechanism, where each position in the sequence is represented by a rotation in the embedding space. The elegance of RoPE lies in its simplicity and efficiency, enabling models to better grasp the nuances of language syntax and semantics.
RoPE introduces a novel concept. Instead of adding a positional vector, it applies a rotation to the word vector. Imagine a two-dimensional word vector for “dog.” To encode its position in a sentence, RoPE rotates this vector. The angle of rotation (θ) is proportional to the word’s position in the sentence. For instance, the vector is rotated by θ for the first position, 2θ for the second, and so on. This approach has several benefits:
Stability of Vectors: Adding tokens at the end of a sentence doesn’t affect the vectors for words at the beginning, facilitating efficient caching.
Preservation of Relative Positions: If two words, say “pig” and “dog,” maintain the same relative distance in different contexts, their vectors are rotated by the same amount. This ensures that the angle, and consequently the dot product between these vectors, remains constant
The technical implementation of RoPE involves rotation matrices. In a 2D case, the equation from the paper incorporates a rotation matrix that rotates a vector by an angle of Mθ, where M is the absolute position in the sentence. This rotation is applied to the query and key vectors in the self-attention mechanism of the Transformer.
For higher dimensions, the vector is split into 2D chunks, and each pair is rotated independently. This can be visualized as an n-dimensional corkscrew rotating in space.
The rotation is executed through simple vector operations rather than matrix multiplication for efficiency. An important property is that words closer together are more likely to have a higher dot product, while those far apart have a lower one, reflecting their relative relevance in a given context.
"
LLM,Training,Training Steps,"Step 1 — Pre-training: In this phase, Large Language Models (LLMs) like GPT-3 are trained on a massive dataset from the internet to predict the next word in a sequence of text. The data is cleaned, preprocessed, and tokenized, and transformer architectures are commonly used for this purpose. The model learns language patterns but doesn’t yet understand instructions or questions.
Step 2 — Supervised Fine-Tuning or Instruction Tuning: In this stage, the model is provided with user messages as input and AI trainer responses as targets. The model learns to generate responses by minimizing the difference between its predictions and the provided responses. It begins to understand instructions and learns to retrieve knowledge based on them.
Step 3 — Reinforcement Learning from Human Feedback (RLHF): RLHF is applied as a second fine-tuning step to align the model with human preferences, focusing on being helpful, honest, and harmless (HHH). This involves two sub-steps:
Training Reward Model Using Human Feedback: Multiple model outputs for the same prompt are generated and ranked by human labelers to create a reward model. This model learns human preferences for HHH content.
Replacing Humans with Reward Model for Large-Scale Training: Once the reward model is trained, it can replace humans in labeling data. Feedback from the reward model is used to further fine-tune the LLM at a large scale.
RLHF helps improve the model’s behavior and alignment with human values, ensuring it provides useful, truthful, and safe responses.
"
LLM,Training,Pre-training,"In the pre-training phase, the model is trained as the next word predictor on internet scale data.
In pre-training phase
Gather a large and diverse dataset from the internet. This dataset contains text from a wide range of sources to ensure the model learns a broad spectrum of language patterns.
Clean and preprocess the data to remove noise, formatting issues, and irrelevant information.
Tokenize the cleaned text data into smaller units, such as words or subword pieces (e.g., Byte-Pair Encoding or WordPiece).
For LLMs like GPT-3, transformer architectures are commonly used due to their effectiveness in handling sequential data.
Pre-training of Large Language Models (LLMs) occurs by training the model to predict the next word in a sequence of text, using a massive dataset, to enable it to understand and generate human-like language.
Output of model after step 1
What if we use a model after just pre-training where it has just learned to predict the next word only & does not take input as question or instruction. During training data model might have seen those sequences of questions as some sort of question paper then the model just predicts the next words.
"
LLM,Training,Memory Consumption during Training,"Let’s examine the memory consumption of the current training system. For example, a 1.5B parameter GPT-2 model requires 3GB (1.5B * 16bit) of memory for its weights (or parameters) in 16-bit precision, yet, it cannot be trained on a single GPU with 32GB memory using Tensorflow or PyTorch. One may wonder where all the memory goes. During model training, most of the memory is consumed by model states, i.e., tensors comprising of optimizer states, gradients, and parameters. Besides these model states, the rest of the memory is consumed by activations, temporary buffers and fragmented memory which we call residual states. We look at the memory consumption from both in details. 
Model States: Optimizer States, Gradients and Parameters 
Majority of the device memory is consumed by model states during training. Consider for instance, Adam, one of the most popular optimizers for DL training. Adam requires storing two optimizer states, 1) the time averaged momentum and 2) variance of the gradients to compute the updates.
Therefore, to train a model with Adam, there has to be enough memory to hold a copy of both the momentum and variance of the gradients. In addition, there needs to be enough memory to store the gradients and the weights themselves. Of these three types of the parameter-related tensors, the optimizer states usually consume the most memory, specially when mixed-precision training is applied.
Mixed-Precision Training The state-of-the-art approach to train large models on the current generation of NVIDIA GPUs is via mixed precision training, where parameters and activations are stored as fp16, enabling the use of the high throughput tensor core units on these GPUs. During mixed-precision training, both the forward and backward propagation are performed using fp16 weights and activations. However, to effectively compute and apply the updates at the end of the backward propagation, the mixed-precision optimizer keeps an fp32 copy of the parameters as well as an fp32 copy of all the other optimizer states.
Let’s take Adam as a concrete example. Mixed precision training of a model with Φ parameters using Adam requires enough memory to hold an fp16 copy of the parameters and the gradients, with memory requirements of 2Φ and 2Φ bytes respectively. In addition, it needs to hold the optimizer states: an fp32 copy of the parameters, momentum and variance, with memory requirements of 4Φ, 4Φ, and 4Φ bytes, respectively.
In total, this results 16Φ bytes of memory requirement. For a model such as GPT-2 with 1.5 Billion parameters, this leads to a memory requirement of at least 24 GB, which is significantly higher than the meager 3 GB of memory required to hold the fp16 parameters alone.
Residual Memory Consumption 
Activations can take up a significant amount of memory during training. As a concrete example, the 1.5B parameter GPT-2 model trained with sequence length of 1K and batch size of 32 requires about 60 GB of memory. 
The activation memory of a transformer-based model is proportional to the number of transformer layers × hidden dimensions × sequence length × batch size. 
Activation checkpointing (or gradient checkpointing) is a common approach to reduce the activation memory by approximately the square root of the total activations at the expense of 33% re-computation overhead. This would reduce the activation memory consumption of this model from 60 GB to about 8 GB. 
Despite the significant reduction, the activation memory can grow quite large for bigger models even with activation checkpointing. For example, a GPT-like model with 100 billion parameters requires around 60 GB of memory for batch size 32, even when using activation checkpointing.
Temporary buffers used for storing intermediate results consumes non-trivial amount of memory for large models. Operations such as gradient all-reduce, or gradient norm computation tend to fuse all the gradients into a single flattened buffer before applying the operation in an effort to improve throughput. For example, the bandwidth of all-reduce across devices improves with large message sizes. While the gradient themselves are usually stored as fp16 tensors, the fused buffer can be an fp32 tensor depending on the operation. When the size of the model is large, these temporary buffer sizes are non-trivial. For example, for a model with 1.5B parameters, a flattened fp32 buffer would required 6 GB of memory
Memory Fragmentation: So far we have discussed the actual memory consumption during training. Additionally, it is possible to run out of usable memory even when there is plenty of available memory. This can happen with memory fragmentation. A request for a memory will fail if there isn’t enough contiguous memory to satisfy it, even if the total available memory is larger than requested. We observe significant memory fragmentation when training very large models, resulting in out of memory issue with over 30% of memory still available in some extreme cases.
"
LLM,Training,Fine-tuning,"Research shows that the pattern-recognition abilities of foundation language models are so powerful that they sometimes require relatively little additional training to learn specific tasks. That additional training helps the model make better predictions on a specific task. This additional training, called fine-tuning, unlocks an LLM's practical side.
Fine-tuning trains on examples specific to the task your application will perform. Engineers can sometimes fine-tune a foundation LLM on just a few hundred or a few thousand training examples.
Despite the relatively tiny number of training examples, standard fine-tuning is often computationally expensive. That's because standard fine-tuning involves updating the weight and bias of every parameter on each backpropagation iteration. Fortunately, a smarter process called parameter-efficient tuning can fine-tune an LLM by adjusting only a subset of parameters on each backpropagation iteration.
A fine-tuned model's predictions are usually better than the foundation LLM's predictions. However, a fine-tuned model contains the same number of parameters as the foundation LLM. So, if a foundation LLM contains ten billion parameters, then the fine-tuned version will also contain ten billion parameters.
"
LLM,Training,"What is Fine-tuning, and Why is it Important?","Fine-tuning is the process of taking a pre-trained model and further training it on a domain-specific dataset.
Most LLM models today have a very good global performance but fail in specific task-oriented problems. The fine-tuning process offers considerable advantages, including lowered computation expenses and the ability to leverage cutting-edge models without the necessity of building one from the ground up.
Transformers grant access to an extensive collection of pre-trained models suited for various tasks. Fine-tuning these models is a crucial step for improving the model's ability to perform specific tasks, such as sentiment analysis, question answering, or document summarization, with higher accuracy.
Fine-tuning tailors the model to have a better performance for specific tasks, making it more effective and versatile in real-world applications. This process is essential for tailoring an existing model to a particular task or domain.
Whether to engage in fine-tuning hinges on your goals, which typically vary based on the specific domain or task at hand.
Transformers (Audio, Vision, Text) can be fine-tuned in two ways based on the learning algorithms.
Supervised Fine-Tuning(SFT):
Supervised Fine-tuning is used for task-specific use cases. The model is trained on a labeled dataset. One can implement SFT in the following ways:
Full parameter fine-tuning: fine-tuning the whole model.
Parameter-efficient fine-tuning (PEFT): fine-tuning a specific set of parameters.
Instruction fine-tuning: fine-tuning based on instruction-format dataset.
Reinforcement Learning Human Feedback(RLHF):
RLHF involves training models through human interaction. It enhances the model to produce accurate and contextual-aware responses. RLHF can be performed in different ways:
Reward Modeling
Proximal Policy Optimization (PPO)
Preference Learning
*Direct Preference Optimization (DPO)
*Reinforcement Learning AI Feedback (RLAIF)
"
LLM,Training,Prompt Engineering vs RAG vs Fine tuning.,"Let us explore the difference between prompt engineering, RAG, and fine-tuning.
Benefits of Fine Tuning LLMs
Fine-tuning offers several advantages:
Increased Performance: Fine-tuned models adapt to new data, leading to more accurate and reliable outputs.
Efficiency: Fine-tuning saves computational costs by adapting pre-trained models rather than training a model from scratch.
Domain Adaptation: LLMs can be tailored to specific industries like medical, legal, or financial domains by focusing on relevant terminology and structures.
Better Generalization: Models fine-tuned on task-specific data generalize better to the unique patterns and structures of the task.
"
LLM,Training,Gradient Accumulation ,"Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches and only stepping the optimizer after a certain number of batches have been performed.
For instance, if the gradient accumulation factor is set to 2, the process works as follows: We first calculate the gradient on one batch, which gives us a direction on the loss function landscape. Instead of updating the model weights immediately, we calculate another gradient from the next batch, obtaining a potentially different direction. By adding these two gradients together, we find a more accurate path in the loss landscape. To ensure the final update step is properly scaled, we divide the accumulated gradient by the number of batches, preventing any artificial inflation of the step size.
"
LLM,Training,How does Gradient Accumulation affect Batch Normalization?,"It can interfere with Batch Normalization (BN) because BN relies on batch statistics (mean and variance) computed for each mini-batch. When using gradient accumulation, the batch statistics remain based on small batches rather than the effective large batch, potentially leading to suboptimal normalization and training instability.
Layer Normalization (LN) does not depend on batch size, as it normalizes values across channels within a single sample instead of across a mini-batch.
"
LLM,Training,Mixed precision training,"Mixed precision training is a technique that aims to optimize the computational efficiency of training models by utilizing lower-precision numerical formats for certain variables. Traditionally, most models use 32-bit floating point precision (fp32 or float32) to represent and process variables. However, not all variables require this high precision level to achieve accurate results. By reducing the precision of certain variables to lower numerical formats like 16-bit floating point (fp16 or float16), we can speed up the computations. Because in this approach some computations are performed in half-precision, while some are still in full precision, the approach is called mixed precision training.
Most commonly mixed precision training is achieved by using fp16 (float16) data types, however, some GPU architectures (such as the Ampere architecture) offer bf16 and tf32 (CUDA internal data type) data types. 
"
LLM,Supervised Fine-Tuning,Supervised fine-tuning or instruction tuning,"In the SFT or instruction tuning phase
During this process, the model is provided with the user’s message as input and the AI trainer’s response as the target. The model learns to generate responses by minimizing the difference between its predictions and the provided responses.
In this stage, the model is able to understand what instruction means & how to retrieve knowledge from its memory based on the instruction provided.
Output of model after step 2
So to continue with the previous example, In training data model has seen the example “what is capital of India?” & human labeled output for this is “Delhi”.
Now model learned the relation between what is asked from LLM and what should be the output. so if you now ask the question “What is Capital of France ?” the model is more likely to say “Paris”
"
LLM,Supervised Fine-Tuning,Parameter-efficient fine-tuning (PEFT),"PEFT: To efficiently fine-tune models for various applications without fine-tuning all the model parameters. PEFT can be applied to any model — large language models, small language models, and deep neural networks.
Trade-offs: Parameter Efficiency, Training Speed, Memory Efficiency, Model Performance, Inference Costs.
PEFT can be implemented in a couple of methods:
Selective — selecting a subset of initial language model parameters to fine-tune.
Additive — adding trainable layers or parameters to the model. For example, LoRA, Soft Prompt (Prompt Tuning)…
"
LLM,Supervised Fine-Tuning,Prefix Tuning,"Prefix Tuning, proposed by Xiang Lisa Li and Percy Liang, is a form of fine-tuning that introduces task-specific information to a pre-trained language model. Unlike traditional fine-tuning, where all parameters of a model are adjusted, Prefix Tuning introduces a small, trainable module, known as the “prefix,” at the beginning of the model’s processing pipeline. This prefix, once trained, produces a sequence of embeddings that are prepended to the input sequence before being fed into the main model.
By learning an additional set of embeddings that are prepended to the sequence of vectors representing the main input, Prefix Tuning effectively adds an extra “sentence” to the beginning of the input. However, this “sentence” is not made up of natural language, but rather a learned representation that helps guide the model’s behavior in a task-specific manner.
Prefix Tuning involves learning an additional set of embeddings, called “prefix embeddings,” which are prepended to the input sequence before being processed by the main model. These embeddings are not made up of natural language, but rather, are abstract representations that help guide the model’s behavior in a task-specific manner.
The prefix is trained by minimizing the error on a task-specific dataset. Once the training is complete, the learned prefix embeddings are fixed and can be used to preprocess input sequences for the desired task.
To clarify this process, let’s consider an example. Suppose we have a pre-trained LLM, and we want to fine-tune it for sentiment analysis. We train a prefix tuner using a sentiment analysis dataset, and it learns a fixed sequence of prefix embeddings. When we want to use the fine-tuned LLM for sentiment analysis, we prepend these prefix embeddings to the input sequence before passing it to the main model.
"
LLM,Supervised Fine-Tuning,Soft Prompts,"Soft prompts are created during the process of prompt tuning.
Unlike hard prompts, soft prompts cannot be viewed and edited in text. Prompts consist of an embedding, a string of numbers, that derives knowledge from the larger model.
So for sure, a disadvantage is the lack of interpretability of soft prompts. The AI discovers prompts relevant for a specific task but can’t explain why it chose those embeddings. Like deep learning models themselves, soft prompts are opaque.
Prompt tuning involves using a small trainable model before using the LLM. The small model is used to encode the text prompt and generate task-specific virtual tokens.
"
LLM,Supervised Fine-Tuning,Prompt Tuning,"Prompt tuning is a form of task-specific adaptation where the model is provided with a trainable prompt. Unlike Prefix Tuning, which involves adding the prefix to the input sequence, prompt tuning typically focuses on optimizing the initial part of the input prompt that is used to condition the model. This can be a trainable set of tokens that guide the model's responses to be more task-specific.
"
LLM,Supervised Fine-Tuning,P-tuning,"P-tuning adds trainable prompt embeddings to the input that is optimized by a prompt encoder to find a better prompt, eliminating the need to manually design prompts. The prompt tokens can be added anywhere in the input sequence, and p-tuning also introduces anchor tokens for improving performance.
P-Tuning is similar to prompt tuning, but it uses continuous embeddings (rather than discrete tokens) as the prompt. These continuous embeddings are trained and optimized in a manner that acts as a dynamic “prompt” to guide the model’s behavior. The embeddings are typically learned through gradient-based optimization and serve as a way to influence the model without needing to rely on specific, pre-defined tokens.
"
LLM,Supervised Fine-Tuning,Adapters,"Adapters are small, trainable neural modules that are inserted into certain layers of a pre-trained model, typically between the model’s existing layers. These modules learn the specific task or domain-specific knowledge, while the rest of the pre-trained model's weights remain frozen (i.e., unchanged).
The main goal of adapters is to allow a pre-trained model to be adapted to new tasks efficiently by only fine-tuning the adapter modules. This is in contrast to traditional fine-tuning, where the entire model’s parameters are updated.
How Do Adapters Work?
The typical approach is to insert the adapter modules into transformer layers (like the attention or feedforward layers) of the pre-trained model. The adapter modules are small networks (usually consisting of a down-projection, non-linearity, and up-projection) that process the input and adjust the representation at the given layer.
The model's original weights are kept frozen, and only the parameters of the adapters are updated during training.
"
LLM,Supervised Fine-Tuning,What is LoRa?,"LoRA is an improved finetuning method where instead of finetuning all the weights that constitute the weight matrix of the pre-trained large language model, two smaller matrices that approximate this larger matrix are fine-tuned. These matrices constitute the LoRA adapter. This fine-tuned adapter is then loaded into the pre-trained model and used for inference.
h=W0x+ΔWx=W0x+BAx
A neural network contains many dense layers which perform matrix multiplication. The weight matrices in these layers typically have full-rank. When adapting to a specific task, Aghajanyan et al. (2020) shows that the pre-trained language models have a low “instrisic dimension” and can still learn efficiently despite a random projection to a smaller subspace
This means that while training for a broad, complex task, the weight matrices in a neural network have full rank, which minimizes redundancy. However, when fine-tuning this universal model for a specialized task, not all the knowledge from the original model is necessary. Therefore, only a small fraction of the parameters needs to be trained. In simpler terms, the weight matrices can be represented by smaller matrices with fewer parameters. Thus, during full fine-tuning, the weight matrices can be considered low-rank, indicating that full fine-tuning involves some degree of redundancy.
Inspired by this, we hypothesize the updates to the weights also have a low “intrinsic rank” during adaptation.
Given that low-rank weight matrices suffice for full fine-tuning on a downstream task, it's reasonable to assume that the gradient updates themselves can be represented by low-rank matrices. 
The most significant benefit comes from the reduction in memory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM usage by up to 2/3 if r≪d as we do not need to store the gradients and optimizer states for the frozen parameters. We also observe a 25% speedup during training on GPT-3 175B compared to full fine-tuning as we do not need to calculate the gradient for the vast majority of the parameters.
"
LLM,Supervised Fine-Tuning,What is Quantized LoRA (QLoRA)?,"QLoRA uses 4-bit quantization to compress a pretrained language model. The LM parameters are then frozen and a relatively small number of trainable parameters are added to the model in the form of Low-Rank Adapters. During finetuning, QLoRA backpropagates gradients through the frozen 4-bit quantized pretrained language model into the Low-Rank Adapters. The LoRA layers are the only parameters being updated during training.
QLoRA has one storage data type (usually 4-bit NormalFloat) for the base model weights and a computation data type (16-bit BrainFloat) used to perform computations. QLoRA dequantizes weights from the storage data type to the computation data type to perform the forward and backward passes, but only computes weight gradients for the LoRA parameters which use 16-bit bfloat. The weights are decompressed only when they are needed, therefore the memory usage stays low during training and inference.
"
LLM,Alignment,AI alignment,"Alignment is an emerging field of study where you ensure that an AI system performs exactly what you want it to perform. Think of a framework like Asimov’s Three Laws of Robotics as a rough example. A first google search might lead you to this definition by IBM, “Alignment is the process of encoding human values and goals into large language models to make them as helpful, safe, and reliable as possible. “ In the context of LLMs specifically, alignment is a process that trains an LLM to ensure that the generated outputs align with human values and goals. This is also called as alignment with respect to human preferences, hence “preference optimization”.
"
LLM,Alignment,Why do we need Alignment?,"There are a couple of examples where the model behaves badly.
If I ask the model how’s the weather outside, it might respond saying it's really good. But is this answer helpful?
Also sometimes the model might respond with which is completely wrong. A Very famous example of this is Can coughing effectively stop heart attack? This isn’t true but the model might respond by saying yes it can.
We want our model to be honest and don’t give misleading information which isn’t true.
Also sometime model can provide answers to which it shouldn’t. how can I rob a bank? it definitely should not respond to this. It can create sometimes harmful content as well.
Helpful, honest, and Harmless is also known as HHH. So we want to align the model with human preferences. RLHF helps us to do this.
"
LLM,Alignment,What are the current methods for LLM alignment?,"You will find many alignment methods in research literature, we will only stick to 3 alignment methods for the sake of discussion.
RLHF (Reinforcement Learning with Human Feedback):
Step 1 & 2: Train an LLM (pre-training for the base model + supervised/instruction fine-tuning for a chat variant)
Step 3: RLHF uses an ancillary language model (it could be much smaller than the main LLM) to learn human preferences. This can be done using a preference dataset — it contains a prompt, and a response/set of responses graded by expert human labelers. This is called a “reward model”.
Step 4: Use a reinforcement learning algorithm (eg: PPO — proximal policy optimization), where the LLM is the agent, the reward model provides a positive or negative reward to the LLM based on how well it’s responses align with the “human preferred responses”.
In theory, it is as simple as that. However, implementation isn’t that easy — requiring lot of human experts and compute resources. To overcome the “expense” of RLHF, researchers developed DPO
DPO (Direct Preference Optimization):
Step 1&2 remain the same
Step 4: DPO eliminates the need for the training of a reward model (i.e step 3). How? DPO defines an additional preference loss as a function of it’s policy and uses the language model directly as the reward model. The idea is simple, If you are already training such a powerful LLM, why not train itself to distinguish between good and bad responses, instead of using another model?
DPO is shown to be more computationally efficient (in case of RLHF you also need to constantly monitoring the behavior of the reward model) and has better performance than RLHF in several settings.
ORPO (Odds Ratio Preference Optimization):
The newest method so far, ORPO combines Step 2, 3 & 4 into a single step — so the dataset required for this method is a combination of a fine-tuning + preference dataset.
The supervised fine-tuning and alignment/preference optimization is performed in a single step. This is because the fine-tuning step, while allowing the model to specialize to tasks and domains, it can also increase the probability of undesired responses from the model.
ORPO combines the steps using a single objective function by incorporating an odds ratio (OR) term — reward preferred responses & penalizing rejected responses.
"
LLM,Alignment,Reinforcement Learning from Human Feedback,"For RLHF you will start with an instruction fine-tuned model. The objective of the RLHF is
Maximize helpfulness
Minimize harm
Avoid dangerous topics
RLHF steps
Training reward model using Human feedback
In RLHF, we will generate multiple outputs for the same prompt & ask the human labeler to rank output from best to worst. This data is used to train another NN model which is called the reward model. This reward model is now able to understand human preferences. Think of it as training an intern by experts to identify Helpful, honest, and Harmless content.
Replacing humans with a reward model for large-scale training
Once the reward model is trained, this can be used instead of human beings to label data & feedback on it can be used to further fine-tune LLM at a large scale.
"
LLM,Alignment,Reward Model ,"Obtaining human feedback is time-consuming and expensive. As a workaround, we can train another model called Reward Model, as a proxy for human feedback. The goal of a reward model is to evaluate the degree of alignment a model response has with human preferences. On a simpler note, a reward model is a model that takes (prompt, response) pair as input and outputs a reward/score as output. This can be formulated as a simple regression or classification task. The real challenge in building such a model is good quality dataset. The perception of good/bad differs from person to person and to map it to a scaler quantity is infeasible.
One workaround is to ask labelers to compare two responses and decide which one is better. This kind of dataset is called a comparison dataset, each record comprises (prompt, chosen response, rejected response).
Training
To train a reward model the comparison dataset should be in the format of (prompt, chosen response, rejected response ), i.e. the better option comes first. The ordering is crucial because it is the base assumption while designing the loss function for the reward model. Any model that can take in variable length text input and output a scaler value can be used. Typically we use an SFT model which aligns with our task and remove the last de-embedding layer while adding a single neuron in the last layer for the scaler output.
For every epoch, we do two passes of the model. In the first pass, we feed in prompt and chosen response to the Reward Model, the output is Rchosen. In the second pass, we feed in the same prompt along with the rejected response. The output, in this case, is Rrejected. Next, we use the loss function defined below to update the reward model.
The intuition behind the loss function is to maximize the gap between chosen response score and rejected response score. For a very high reward score for chosen response and a low reward score for rejected response, the loss would be 0.
"
LLM,Alignment,Proximal Policy Optimization (PPO),"PPO is a widely used reinforcement learning algorithm known for its balance between simplicity and performance. It employs a clipped surrogate objective function to limit the size of policy updates, ensuring stable training. PPO is computationally efficient, requires minimal hyperparameter tuning, and performs well across various tasks. It is frequently used to fine-tune large language models (LLMs) by maximizing rewards based on human feedback, ensuring stable and reliable policy updates.
How PPO Works
Rollout:
Process: The language model generates a response or continuation based on a given query, which could be the start of a sentence or another prompt.
Objective: Create query/response pairs that will be evaluated in the next step.
2. Evaluation:
Process: The query and response pairs are evaluated using a function, model, human feedback, or a combination of these methods.
Objective: Produce a scalar value (reward) for each query/response pair. This reward reflects the quality and alignment of the response with human preferences.
3. Optimization:
Process: The optimization step involves calculating the log-probabilities of the tokens in the sequences using both the trained model and a reference model (usually the pre-trained model before fine-tuning).
KL-Divergence: The Kullback-Leibler (KL) divergence between the outputs of the active model and the reference model is used as an additional reward signal. This ensures that the generated responses do not deviate too far from the reference language model.
Training: The active language model is trained using PPO, incorporating both the primary reward signal and the KL-divergence penalty.
Challenges of PPO
Complexity: The optimization step is intricate and requires careful handling of log-probabilities and KL-divergence.
Slow Iteration: PPO involves numerous iterations, debugging, and fine-tuning to achieve optimal performance.
Debugging Difficulty: As an RL algorithm, PPO can be slow and challenging to debug, especially when ensuring the correct application of human feedback and reward signals.
"
LLM,Alignment,RLAIF: Reinforcement Learning from Human Feedback with AI Feedback,"Reinforcement learning from human feedback (RLHF) is effective at aligning large language models (LLMs) to human preferences, but gathering high quality human preference labels is a key bottleneck. We conduct a head-to-head comparison of RLHF vs. RL from AI Feedback (RLAIF) - a technique where preferences are labeled by an off-the-shelf LLM in lieu of humans, and we find that they result in similar improvements. On the task of summarization, human evaluators prefer generations from both RLAIF and RLHF over a baseline supervised fine-tuned model in ~70% of cases. Furthermore, when asked to rate RLAIF vs. RLHF summaries, humans prefer both at equal rates. These results suggest that RLAIF can yield human-level performance, offering a potential solution to the scalability limitations of RLHF.
"
LLM,Alignment,Direct Preference Optimization (DPO),"Overview
Direct Preference Optimization (DPO) is an alternative to Proximal Policy Optimization (PPO) for fine-tuning models with human alignment. Unlike PPO, DPO avoids complex normalization terms that can be difficult to optimize. Instead, DPO uses an updated or re-parameterized cross-entropy loss function that incorporates preference data. The training loss function penalizes the model for generating less-preferred outputs and increases the log probability of preferred responses.
Advantages of DPO
Simplicity: DPO does not require the complex normalization terms used in PPO, making it easier to optimize.
Direct Optimization: Uses a straightforward approach to maximize the log-likelihood of preferred responses, simplifying the fine-tuning process.
Efficiency: Faster and easier to implement compared to PPO, requiring fewer iterations and less debugging.
How DPO Works
Train the SFT Model:
Objective: Ensure the data used for training is appropriate and in-distribution for the DPO algorithm.
Process: Begin with supervised fine-tuning (SFT) to align the model with initial user expectations.
2. Data Collection:
Process: Gather a preference dataset consisting of positive and negative selected pairs of generated responses for given prompts.
Objective: Create a dataset that clearly distinguishes between preferred and less-preferred outputs based on human feedback.
3. Optimization:
Process: Maximize the log-likelihood of the DPO loss directly. The loss function is designed to penalize the model for producing less-preferred outputs while increasing the log probability of preferred responses.
Objective: Directly optimize the model to align its outputs with human preferences using the collected preference data.
"
LLM,Alignment,Odds Ratio Preference Optimization (ORPO),"Overview
Odds Ratio Preference Optimization (ORPO) is a new fine-tuning technique that integrates Supervised Fine-Tuning (SFT) and preference alignment into a single process. By leveraging preference data, ORPO posits that a minor penalty for disfavored outputs, combined with a strong adaptation signal for the chosen response via a simple log odds ratio term appended to the Negative Log-Likelihood (NLL) loss, is sufficient for preference-aligned SFT. This approach eliminates the need for an additional preference alignment phase, saving computational resources and memory.
Key Features of ORPO
Reference Model-Free: ORPO does not require a separate reference model for preference alignment, streamlining the fine-tuning process.
No Reward Model Needed: Unlike PPO, ORPO eliminates the need for a reward model, simplifying the architecture.
Integrated Loss Function: Combines the benefits of PPO and DPO through a loss function that incorporates cross-entropy and odds ratio preference optimization, penalizing disfavored responses while emphasizing preferred ones.
Efficient and Effective: ORPO widens the distance between chosen and rejected responses, ensuring better alignment with human preferences without additional computational overhead.	
How ORPO Works
Train the SFT Model:
Objective: Ensure the initial data is appropriate for training, aligning the model with user expectations through supervised fine-tuning.
2. Data Collection:
Process: Gather preference data consisting of prompts and their respective chosen and rejected responses.
Objective: Create a dataset that clearly distinguishes between preferred and less-preferred outputs based on human feedback.
3. Optimization:
Process: Apply a loss function that combines cross-entropy loss with an odds ratio preference optimization term.
Objective: Penalize disfavored responses and strongly adapt to preferred ones, using a single integrated process for fine-tuning.
ORPO Loss Function
Cross-Entropy Loss: Measures the difference between the predicted probability distribution and the true distribution of responses.
Odds Ratio Preference Optimization (ORPO) Term: Increases the log probability of preferred responses and decreases the log probability of disfavored responses.
Loss = Cross-Entropy Loss + Odds Ratio Preference Optimization
This integrated loss function helps fine-tune the model to produce outputs that align more closely with human preferences by directly penalizing less-preferred responses and enhancing the probability of chosen responses.
"
LLM,Alignment,Kahneman-Tversky Optimization (KTO),"Kahneman-Tversky Optimization (KTO) is an approach to AI alignment inspired by prospect theory, developed by Daniel Kahneman and Amos Tversky. It focuses on making AI systems more aligned with human decision-making biases and risk assessments.
Key Ideas of KTO Alignment:
Humans Are Not Perfectly Rational 
Traditional AI optimization assumes people make rational choices (like in expected utility theory).
Kahneman & Tversky showed that humans often make irrational but predictable decisions due to cognitive biases.
Loss Aversion & Risk Perception 
People fear losses more than they value equivalent gains (e.g., losing $100 feels worse than gaining $100 feels good).
AI using KTO takes this into account, optimizing not just for expected value but also for perceived risk.
Framing Effects & Decision Context 
The way choices are framed influences decisions (e.g., ""90% survival"" vs. ""10% mortality"").
AI aligned with KTO ensures that recommendations and interactions consider human psychological framing.
"
LLM,Alignment,DPO vs PPO vs KTO,"When to Use Each?
PPO → Best for traditional reinforcement learning with clear rewards (e.g., robotics, games).
DPO → Best for aligning AI models with human preferences (e.g., fine-tuning LLMs).
KTO → Best for AI systems needing human-like risk perception (e.g., finance, medical AI).
"
LLM,MultiGpu Training,CUDA Cores vs Tensor Cores,"What are NVIDIA CUDA Cores?
NVIDIA CUDA (Compute Unified Device Architecture) cores are the fundamental processing units in NVIDIA GPUs that execute general-purpose computations.
Some key points about CUDA cores:
Architecture: CUDA cores are the basic building blocks of an NVIDIA GPU's compute engine. They are designed to perform a wide range of floating-point and integer operations in parallel.
Parallelism: CUDA cores are organized into groups called Streaming Multiprocessors (SMs), which allow for massive parallel processing of data-parallel workloads.
Performance: The number of CUDA cores in a GPU is a key indicator of its overall computational power. More CUDA cores generally translate to higher performance for tasks like rendering, scientific computing, image/video processing, and machine learning inference.
Programming: CUDA cores can be programmed using the CUDA programming model, which allows developers to leverage the parallel processing capabilities of NVIDIA GPUs for general-purpose computations.
What are NVIDIA Tensor Cores?
NVIDIA Tensor cores are specialized processing units found in NVIDIA GPUs that are designed to accelerate deep learning and machine learning workloads.
Key characteristics of Tensor cores:
Architecture: Tensor cores are specifically optimized for performing matrix operations, which are a fundamental component of neural network computations.
Deep Learning Acceleration: Tensor cores can perform operations like 4x4 matrix multiply-accumulate (GEMM) in a single instruction, providing a significant performance boost for deep learning training and inference.
Specialized Instructions: Tensor cores leverage specialized hardware and instructions to achieve higher throughput for the matrix operations central to deep learning algorithms.
Performance: The number of Tensor cores in a GPU is a key indicator of its deep learning performance. More Tensor cores generally translate to faster deep learning training and inference.
Power Efficiency: Tensor cores are more power-efficient than general-purpose CUDA cores for deep learning workloads, making them well-suited for data center and edge deployment scenarios.
Programming: Tensor cores can be programmed using the CUDA programming model and deep learning frameworks like TensorFlow and PyTorch, which can automatically leverage the Tensor core acceleration.
CUDA Cores vs Tensor Cores: What's the Difference?
CUDA cores provide the raw computational power for general-purpose GPU computing, while Tensor cores are specialized processing units designed to accelerate the matrix operations that are central to deep learning and machine learning workloads.
CUDA cores are general-purpose processing units, while Tensor cores are specialized for deep learning matrix operations.
CUDA cores excel at a wider range of floating-point computations, while Tensor cores are optimized for the specific matrix operations common in deep learning.
The number of CUDA cores in a GPU provides a measure of the overall computational power, while the number of Tensor cores indicates the deep learning performance.
Modern NVIDIA GPUs like the H100 contain both CUDA cores and Tensor cores to provide a balance of general-purpose and deep learning-specific acceleration.
"
LLM,MultiGpu Training,GPU Performance,"GPU Architecture Fundamentals
The GPU is a highly parallel processor architecture, composed of processing elements and a memory hierarchy. At a high level, NVIDIA® GPUs consist of a number of Streaming Multiprocessors (SMs), on-chip L2 cache, and high-bandwidth DRAM. Arithmetic and other instructions are executed by the SMs; data and code are accessed from DRAM via the L2 cache. As an example, an NVIDIA A100 GPU contains 108 SMs, a 40 MB L2 cache, and up to 2039 GB/s bandwidth from 80 GB of HBM2 memory. 
Each SM has its own instruction schedulers and various instruction execution pipelines. Multiply-add is the most frequent operation in modern neural networks, acting as a building block for fully-connected and convolutional layers, both of which can be viewed as a collection of vector dot-products.
GPU Execution Model
To utilize their parallel resources, GPUs execute many threads concurrently. 
There are two concepts critical to understanding how thread count relates to GPU performance: 
GPUs execute functions using a 2-level hierarchy of threads. A given function’s threads are grouped into equally-sized thread blocks, and a set of thread blocks are launched to execute the function. 
GPUs hide dependent instruction latency by switching to the execution of other threads. Thus, the number of threads needed to effectively utilize a GPU is much higher than the number of cores or instruction pipelines. 
The 2-level thread hierarchy is a result of GPUs having many SMs, each of which in turn has pipelines for executing many threads and enables its threads to communicate via shared memory and synchronization. At runtime, a thread block is placed on an SM for execution, enabling all threads in a thread block to communicate and synchronize efficiently. Launching a function with a single thread block would only give work to a single SM, therefore to fully utilize a GPU with multiple SMs one needs to launch many thread blocks. Since an SM can execute multiple thread blocks concurrently, typically one wants the number of thread blocks to be several times higher than the number of SMs. The reason for this is to minimize the “tail” effect, where at the end of a function execution only a few active thread blocks remain, thus underutilizing the GPU for that period of time 
Math And Memory Bounds
Following the convention of various linear algebra libraries (such as BLAS), we will say that matrix A is an M x K matrix, meaning that it has M rows and K columns. Similarly, B and C will be assumed to be K x N and M x N matrices, respectively. 
The product of A and B has M x N values, each of which is a dot-product of K-element vectors. Thus, a total of M * N * K fused multiply-adds (FMAs) are needed to compute the product. Each FMA is 2 operations, a multiply and an add, so a total of 2 * M * N * K FLOPS are required. For simplicity, we are ignoring the α and β parameters for now; as long as K is sufficiently large, their contribution to arithmetic intensity is negligible. 
To estimate if a particular matrix multiply is math or memory limited, we compare its arithmetic intensity to the ops:byte ratio of the GPU, as described in Understanding Performance. Assuming an NVIDIA® V100 GPU and Tensor Core operations on FP16 inputs with FP32 accumulation, the FLOPS:B ratio is 138.9 if data is loaded from the GPU’s memory. 
As an example, let’s consider a M x N x K = 8192 x 128 x 8192 GEMM. For this specific case, the arithmetic intensity is 124.1 FLOPS/B, lower than V100’s 138.9 FLOPS:B, thus this operation would be memory limited. If we increase the GEMM size to 8192 x 8192 x 8192 arithmetic intensity increases to 2730, much higher than FLOPS:B of V100 and therefore the operation is math limited. In particular, it follows from this analysis that matrix-vector products (general matrix-vector product or GEMV), where either M=1 or N=1, are always memory limited; their arithmetic intensity is less than 1. 
It is worth keeping in mind that the comparison of arithmetic intensity with the ops:byte ratio is a simplified rule of thumb, and does not consider many practical aspects of implementing this computation (such as non-algorithm instructions like pointer arithmetic, or the contribution of the GPU’s on-chip memory hierarchy). 
"
LLM,MultiGpu Training,Strategies,"Begin by estimating how much vRAM is required to train your model. For models hosted on the 🤗 Hub, use our Model Memory Calculator, which gives you accurate calculations within a few percent margin.
Parallelization strategy for a single Node / multi-GPU setup
When training a model on a single node with multiple GPUs, your choice of parallelization strategy can significantly impact performance. Here’s a breakdown of your options:
Case 1: Your model fits onto a single GPU
If your model can comfortably fit onto a single GPU, you have two primary options:
DDP - Distributed DataParallel
Zero Redundancy Optimizer (ZeRO) - depending on the situation and configuration used, this method may or may not be faster, however, it’s worth experimenting with it.
Case 2: Your model doesn’t fit onto a single GPU:
If your model is too large for a single GPU, you have several alternatives to consider:
PipelineParallel (PP)
ZeRO
TensorParallel (TP)
With very fast inter-node connectivity (e.g., NVLINK or NVSwitch) all three strategies (PP, ZeRO, TP) should result in similar performance. However, without these, PP will be faster than TP or ZeRO. The degree of TP may also make a difference. It’s best to experiment with your specific setup to determine the most suitable strategy.
TP is almost always used within a single node. That is TP size <= GPUs per node.
Case 3: Largest layer of your model does not fit onto a single GPU
If you are not using ZeRO, you have to use TensorParallel (TP), because PipelineParallel (PP) alone won’t be sufficient to accommodate the large layer.
If you are using ZeRO, additionally adopt techniques from the Methods and tools for efficient training on a single GPU.
Parallelization strategy for a multi-Node / multi-GPU setup
When you have fast inter-node connectivity (e.g., NVLINK or NVSwitch) consider using one of these options:
ZeRO - as it requires close to no modifications to the model
A combination of PipelineParallel (PP) with TensorParallel(TP) and DataParallel(DP) - this approach will result in fewer communications, but requires significant changes to the model
When you have slow inter-node connectivity and still low on GPU memory:
Employ a combination of DataParallel(DP) with PipelineParallel(PP), TensorParallel(TP), and ZeRO.
"
LLM,MultiGpu Training,Collective Operations ,"Before diving into distributed training, it’s beneficial to first understand the basic operations involved in multi-GPU and multi-node communication.
For this purpose, we'll focus on the NVIDIA NCCL
The NVIDIA Collective Communication Library (NCCL) implements multi-GPU and multi-node communication primitives optimized for NVIDIA GPUs and Networking. NCCL provides routines such as all-gather, all-reduce, broadcast, reduce, reduce-scatter as well as point-to-point send and receive that are optimized to achieve high bandwidth and low latency over PCIe and NVLink high-speed interconnects within a node and over NVIDIA Mellanox Network across nodes.
Leading deep learning frameworks such as Caffe2, Chainer, MxNet, PyTorch and TensorFlow have integrated NCCL to accelerate deep learning training on multi-GPU multi-node systems.
Collective operations have to be called for each rank (hence CUDA device) to form a complete collective operation. Failure to do so will result in other ranks waiting indefinitely.
AllReduce 
The AllReduce operation performs reductions on data (for example, sum, min, max) across devices and stores the result in the receive buffer of every rank.
In a sum allreduce operation between k ranks, each rank will provide an array in of N values, and receive identical results in array out of N values, where out[i] = in0[i]+in1[i]+…+in(k-1)[i].
Broadcast 
The Broadcast operation copies an N-element buffer from the root rank to all the ranks.
Important note: The root argument is one of the ranks, not a device number, and is therefore impacted by a different rank to device mapping.
Reduce 
The Reduce operation performs the same operation as AllReduce, but stores the result only in the receive buffer of a specified root rank.
Important note: The root argument is one of the ranks (not a device number), and is therefore impacted by a different rank to device mapping.
Note: A Reduce, followed by a Broadcast, is equivalent to the AllReduce operation.
ReduceScatter 
The ReduceScatter operation performs the same operation as Reduce, except that the result is scattered in equal-sized blocks between ranks, each rank getting a chunk of data based on its rank index.
The ReduceScatter operation is impacted by a different rank to device mapping since the ranks determine the data layout.
AllGather 
The AllGather operation gathers N values from k ranks into an output buffer of size k*N, and distributes that result to all ranks.
The output is ordered by the rank index. The AllGather operation is therefore impacted by a different rank to device mapping.
Note: Executing ReduceScatter, followed by AllGather, is equivalent to the AllReduce operation.
"
LLM,MultiGpu Training,Distributed Data Parallel,"DistributedDataParallel (DDP) is a powerful module in PyTorch that allows you to parallelize your model across multiple machines, making it perfect for large-scale deep learning applications. To use DDP, you’ll need to spawn multiple processes and create a single instance of DDP per process.
But how does it work? DDP uses collective communications from the torch.distributed package to synchronize gradients and buffers across all processes. This means that each process will have its own copy of the model, but they’ll all work together to train the model as if it were on a single machine.
To make this happen, DDP registers an autograd hook for each parameter in the model. When the backward pass is run, this hook fires and triggers gradient synchronization across all processes. This ensures that each process has the same gradients, which are then used to update the model.
The recommended way to use DDP is to spawn one process for each model replica. The model replica can span multiple devices. DDP processes can be placed on the same machine or across machines. Note that GPU devices cannot be shared across DDP processes (i.e. one GPU for one DDP process).
"
LLM,MultiGpu Training,"Model Parallelism, Tensor Parallelism, Pipeline Parallelism ","When a model does not fit in the device memory, model parallelism split the model among processes, in vertical or horizontal way.
Naive Model Parallelism 
This approach involves distributing groups of model layers across multiple GPUs by assigning specific layers to specific GPUs. As data flows through these layers, it is moved to the same GPU as the layer, while the other layers remain untouched.
In this example, when data moves through layers within one GPU, it’s no different from regular forward pass. However, moving data between layers on different GPUs results in a communication overhead. If the participating GPUs are on the same compute node (e.g. same physical machine) this copying is fast, but if the GPUs are distributed across different compute nodes (e.g. multiple machines), the communication overhead could be substantially greater.
The main problem with Naive Model Parallelism is that аll but one GPU are idle at any given moment, which is very inefficient.
Pipeline Parallelism 
PP is almost identical to a naive MP, but it solves the GPU idling problem by chunking the incoming batch into micro-batches and artificially creating a pipeline, which allows different GPUs to concurrently participate in the computation process.
But this comes at the expense of a great deal of technical complication.
Tensor Parallelism 
In Tensor Parallelism, each GPU processes a slice of a tensor and only aggregates the full tensor for operations requiring it. So, unlike Model Parallelism (MP), we don't have to wait for the previous GPUs to finish processing the previous layers of the model. This allows for more efficient processing and reduced idle time.
The main building block of any transformer is a fully connected nn.Linear followed by a nonlinear activation GeLU. The dot dot-product part of it, following the Megatron’s paper notation, can be written as Y = GeLU(XA), where X is an input vector, Y is the output vector, and A is the weight matrix.
If we look at the computation in matrix form, you can see how the matrix multiplication can be split between multiple GPUs:
Data Parallelism + Pipeline Parallelism + Tensor Parallelism
To get an even more efficient training a 3D parallelism is used where PP is combined with TP and DP.
"
LLM,MultiGpu Training,FSDP - Fully Sharded Data Parallel ,"FSDP expands upon distributed data parallel, by parallelizing not just data, but the model parameters, the optimizer states and gradients associated with the model. Specifically - each GPU only stores a subset of the entire model and the associated subset of optimizer states and gradients.
FSDP Units 
FSDP breaks down a model instance into smaller units and then flattens and shards all of the parameters within each unit. The sharded parameters are communicated and recovered on-demand before computations, and then they are immediately discarded afterwards. This approach ensures that FSDP only needs to materialize parameters from one unit at a time, which significantly reduces peak memory consumption
FSDP Workflow 
Let us consider FSDP unit1 that contains [layer1,layer2] to explain this process.
Forward pass:
Before forward computation enters layer1, FSDP collects the unsharded parameters for layer1 and layer2 by gathering shards from other peer ranks.
With the unsharded parameters, FSDP runs the local computation of those layers
Then frees the peer shards it just collected to reduce memory footprint
Therefore, during the entire forward pass, FSDP only needs to fully materialize one unit at a time, while all other units can stay sharded.
Backward pass:
Similarly, during the backward computation, FSDP unit1 recovers the unsharded parameters for layer1 and layer2 before backward reaches layer2
When the autograd engine finishes the backward computation of these two layers, FSDP frees the peer shards and launches ReduceScatter to reduce and shard gradients.
Hence, after backward computation, each rank only keeps a shard of both parameters and gradients	
While FSDP significantly optimizes memory usage by sharding parameters, it introduces some communication overhead due to the frequent need to gather and scatter parameters and gradients across GPUs. This overhead is a trade-off for the reduced memory footprint, and its impact can vary depending on the network bandwidth and latency between GPUs. Efficient implementation of the gather and scatter operations, along with optimizations such as overlapping communication with computation, can help mitigate this overhead to maintain high training throughput.
"
LLM,MultiGpu Training,ZeRO,"ZeRO is a memory-optimization strategy developed as part of DeepSpeed. It reduces memory redundancy in distributed training by partitioning model states (weights, gradients, and optimizer states) across multiple GPUs instead of replicating them.
ZeRO Stages
ZeRO is implemented in three stages, progressively reducing memory usage:
ZeRO-1 (Optimizer State Sharding)
Each GPU stores only a portion of the optimizer states instead of a full copy.
Reduces memory usage but still keeps full model weights on each GPU.
ZeRO-2 (Optimizer + Gradient Sharding)
Further shards gradients across GPUs, reducing redundancy even more.
Model weights are still fully replicated on each GPU.
ZeRO-3 (Full Model Sharding)
The most aggressive stage where model weights, gradients, and optimizer states are fully sharded across all GPUs.
This allows training models much larger than a single GPU’s memory but comes with increased communication overhead.
🔹 ZeRO is part of DeepSpeed and is well-optimized for mixed precision training (e.g., ZeRO + FP16).
Reducing Activation Memory:
Managing Temporary buffers
Managing fragmented Memory
"
LLM,RAG,What is RAG?,"LLMs, although capable of generating text that is both meaningful and grammatically correct, these LLMs suffer from a problem called hallucination. Hallucination in LLMs is the concept where the LLMs confidently generate wrong answers, that is they make up wrong answers in a way that makes us believe that it is true. This has been a major problem since the introduction of the LLMs. These hallucinations lead to incorrect and factually wrong answers. Hence Retrieval Augmented Generation was introduced.
In RAG, we take a list of documents/chunks of documents and encode these textual documents into a numerical representation called vector embeddings, where a single vector embedding represents a single chunk of document and stores them in a database called vector store. The models required for encoding these chunks into embeddings are called encoding models or bi-encoders. These encoders are trained on a large corpus of data, thus making them powerful enough to encode the chunks of documents in a single vector embedding representation.
"
LLM,RAG,Explain Semantic Chunking,"In order to abide by the context window of the LLM , we usually break text into smaller parts / pieces which is called chunking.
Different chunking methods:
Fixed size chunking
Recursive Chunking
Document Specific Chunking
Semantic Chunking
Agentic Chunking
Semantic chunking involves taking the embeddings of every sentence in the document, comparing the similarity of all sentences with each other, and then grouping sentences with the most similar embeddings together. By focusing on the text’s meaning and context, Semantic Chunking significantly enhances the quality of retrieval. It’s a top-notch choice when maintaining the semantic integrity of the text is vital.
The hypothesis here is we can use embeddings of individual sentences to make more meaningful chunks. Basic idea is as follows :-
Split the documents into sentences based on separators(.,?,!)
Index each sentence based on position.
Group: Choose how many sentences to be on either side. Add a buffer of sentences on either side of our selected sentence.
Calculate distance between group of sentences.
Merge groups based on similarity i.e. keep similar sentences together.
Split the sentences that are not similar.
"
LLM,RAG,Keyword-Based Retrieval,"This retrieval type uses a keyword-based retriever, also known as a sparse retriever. An example of such a retriever is BM25Retriever. 
Sparse retrievers work with keywords, looking for words shared between the document and the query. They operate on a bag-of-words level and don’t consider the order of words or their contextual meanings, which means they may not capture semantic nuances as effectively as dense retrievers. 
These retrievers don’t need any training and are fast and effective. They can work on any language and any domain.
"
LLM,RAG,Vector-Based Retrieval,"This retrieval type relies on vector-based, or dense, retrievers, such as EmbeddingRetriever. Dense retrievers use a model to transform both the documents and the query into numerical vectors (embeddings). Then, they compare both embeddings and, based on that, fetch the documents most similar to the query.
Dense retrievers are very good at capturing nuances in queries and documents, recognizing similarities that go beyond keyword matching. They can recognize contextual and semantic information about words and their relationships within a sentence. 
Unlike sparse retrievers, dense retrievers need to be trained. This means they perform best on the domain and language they were trained on. They’re also more computationally expensive than keyword-based retrievers.
"
LLM,RAG,Hybrid Retrieval,"Sparse retrievers are fast and can quickly reduce the number of candidate documents. Dense retrievers are better at capturing semantic nuances, thus improving the relevance of search results. 
For example, when searching for product IDs, keyword search is best. When given the query “P12642”, a sparse retriever would fetch “Miura climbing shoes” as a result. Dense retrievers would be thrown off by such a query since they can return results with a similar product ID.
On the other hand, a query like “What are EVs?” would be easier for vector-based retrievers. They would retrieve results like “Electric cars are..”, while sparse retrievers would look for the exact keyword match.
Combining both retrieval methods in one system makes it more robust to different kinds of queries and documents. 
Once the retrievers fetch the most relevant documents, you can use a combination strategy to produce the final ranking and return the top documents as search results. 
A good use case for hybrid retrieval is when your documents are from a niche domain, and it’s unlikely the model was trained on it. Hybrid retrieval saves you the time and money you’d need to train or fine-tune a model and it’s a good trade-off between speed and accuracy.
"
LLM,RAG,Cross-Encoders,"Cross-Encoders are used for sentence pair scoring and sentence pair classification tasks.
Bi-Encoder vs. Cross-Encoder
First, it is important to understand the difference between Bi- and Cross-Encoder.
Bi-Encoders produce for a given sentence a sentence embedding. We pass to a BERT independently the sentences A and B, which result in the sentence embeddings u and v. These sentence embedding can then be compared using cosine similarity:
In contrast, for a Cross-Encoder, we pass both sentences simultaneously to the Transformer network. It produces then an output value between 0 and 1 indicating the similarity of the input sentence pair:
A Cross-Encoder does not produce a sentence embedding. Also, we are not able to pass individual sentences to a Cross-Encoder.
As detailed in our paper, Cross-Encoder achieve better performances than Bi-Encoders. However, for many application they are not practical as they do not produce embeddings we could e.g. index or efficiently compare using cosine similarity.
When to use Cross- / Bi-Encoders?
Cross-Encoders can be used whenever you have a pre-defined set of sentence pairs you want to score. For example, you have 100 sentence pairs and you want to get similarity scores for these 100 pairs.
Bi-Encoders are used whenever you need a sentence embedding in a vector space for efficient comparison. Applications are for example Information Retrieval / Semantic Search or Clustering. Cross-Encoders would be the wrong choice for these application: Clustering 10,000 sentence with CrossEncoders would require computing similarity scores for about 50 Million sentence combinations, which takes about 65 hours. With a Bi-Encoder, you compute the embedding for each sentence, which takes only 5 seconds. You can then perform the clustering.
"
LLM,RAG,What is Reciprocal Rank Fusion?,"Reciprocal Rank Fusion is a rank aggregation method that combines rankings from multiple sources into a single, unified ranking. In the context of RAG, these sources typically use different retrieval models or approaches.
The RRF Formula
The core of RRF is captured in its formula:
RRF(d) = Σ(r ∈ R) 1 / (k + r(d))
Where:
- d is a document
- R is the set of rankers (retrievers)
- k is a constant (typically 60)
- r(d) is the rank of document d in ranker r
How RRF Works in RAG
Let’s break down the process of using RRF in a RAG system:
1. User Query: The process begins when a user inputs a question or query.
2. Multiple Retrievers: The query is sent to multiple retrievers. These could be different retrieval models (e.g., dense, sparse, hybrid).
3. Individual Rankings: Each retriever produces its own ranking of relevant documents.
4. RRF Fusion: The rankings from all retrievers are combined using the RRF formula.
5. Final Ranking: A unified ranking is produced based on the RRF scores.
6. Generation: The generative model uses the top-ranked documents to produce the final answer.
Mathematical Intuition Behind RRF
Understanding the mathematical intuition behind RRF helps explain why it’s effective:
1. Reciprocal Ranking
Using 1/(rank + k), RRF gives more weight to higher ranks (lower rank numbers). This ensures that documents ranked highly by multiple retrievers are favoured in the final ranking.
2. Diminishing Returns
The contribution to the score decreases non-linearly as rank increases. This model shows the intuition that the difference in relevance between ranks 1 and 2 is likely larger than between ranks 100 and 101.
3. Rank Aggregation
By summing the reciprocal ranks across all retrievers, RRF effectively combines evidence from multiple sources. This makes the final ranking more robust and less susceptible to the quirks or biases of any single retriever.
4. Normalization
The constant k acts as a smoothing factor. It prevents any single retriever from dominating the results and helps handle ties more gracefully, especially among lower-ranked items.
The Mystery of k = 60
One aspect of RRF that often raises questions is the choice of k = 60. While this value isn’t set in stone, it’s commonly used due to several factors:
1. Empirical Performance
Studies have shown that k = 60 performs well across various datasets and retrieval tasks.
2. Balancing Influence
It provides a good balance between the influence of top-ranked and lower-ranked items. For example:
- For rank 1: 1/(1+60) ≈ 0.0164
- For rank 10: 1/(10+60) ≈ 0.0143
- For rank 100: 1/(100+60) ≈ 0.00625
3. Effective Tie-Breaking
k = 60 helps break ties effectively, especially for lower-ranked items where small differences in the original rankings might not be significant.
4. Robustness
This value has shown to be robust across different types of retrieval systems and data distributions.
It’s worth noting that while k = 60 is common, the optimal value can vary depending on the specific application and data characteristics. Some systems may benefit from tuning this parameter.
"
LLM,Similarity Search,Shingling,"Shingling is the process of collecting k-grams on given texts. k-gram is a group of k sequential tokens. Depending on the context, tokens can be words or symbols. The ultimate goal of shingling is by using collected k-grams to encode each document. We will be using one-hot encoding for this. Nevertheless, other encoding methods can also be applied.
Collecting unique shingles of length k = 3 for the sentence ""learning data science is fascinating""
Firstly, unique k-grams for each document are collected. Secondly, to encode each document, a vocabulary is needed which represents a set of unique k-grams in all documents. Then for each document, a vector of zeros with the length equal to the size of the vocabulary is created. For every appearing k-gram in the document, its position in the vocabulary is identified and a ""1"" is placed at the respective position of the document vector. Even if the same k-gram appears several times in a document, it does not matter: the value in the vector will always be 1.
"
LLM,Similarity Search,kNN,"kNN is the simplest and the most naive algorithm for similarity search. Consider a dataset of vectors and a new query vector Q. We would like to find the top k dataset vectors which are the most similar to Q. The first aspect to think about is how to measure a similarity (distance) between two vectors. In fact, there are several similarity metrics to do it. Some of them are illustrated in the figure below.
"
LLM,Similarity Search,Inverted File Index,"""Inverted index (also referred to as a postings list, postings file, or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, or in a document or a set of documents"" – Wikipedia
When performing a query, the hash function of the query is computed and mapped values from the hash table are taken. Each of these mapped values contains its own set of potential candidates which then are fully checked on a condition to be the nearest neighbour for the query. By doing so, the search scope of all database vectors is reduced.
There are different implementations of this index depending on how hash functions are computed. The implementation we are going to look at is the one that uses Voronoi diagrams (or Dirichlet tessellation).
Training
The idea of the algorithm is to create several non-intersecting regions to which each dataset point will belong. Each region has its own centroid which points to the center of that region.
Sometimes Voronoi regions are referred to as cells or partitions.
The main property of Voronoi diagrams is that the distance from a centroid to any point of its region is less than the distance from that point to another centroid.
"
LLM,Similarity Search,Faiss,"Faiss (Facebook AI Search Similarity) is a Python library written in C++ used for optimised Similarity Search. This library presents different types of indexes which are data structures used to efficiently store the data and perform queries.
"
LLM,Similarity Search,MinHashing,"At this stage, initial texts have been vectorised. The similarity of vectors can be compared via Jaccard index. Remember that Jaccard index of two sets is defined as the number of common elements in both sets divided by the length of all the elements.
Jaccard Index is defined as the intersection over the union of two sets
If a pair of encoded vectors is taken, the intersection in the formula for Jaccard index is the number of rows that both contain 1 (i.e. k-gram appears in both vectors) and the union is the number of rows with at least one 1 (k-gram is presented at least in one of the vectors).
Formula for Jaccard Index of two vectorsExample of calculating Jaccard Index for two vectors using the formula above
The current problem right now is the sparsity of encoded vectors. Computing a similarity score between two one-hot encoded vectors would take a lot of time. Transforming them to a dense format would make it more efficient to operate on them later. Ultimately, the goal is to design such a function that will transform these vectors to a smaller dimension preserving the information about their similarity. The method that constructs such a function is called MinHashing.
MinHashing is a hash function that permutes the components of an input vector and then returns the first index where the permutated vector component equals 1.
Example of calculating a minhash value for a given vector and permutation
For getting a dense representation of a vector consisting of n numbers, n minhash functions can be used to obtain n minhash values which form a signature.
It may not sound obvious at first but several minhash values can be used to approximate Jaccard similarity between vectors. In fact, the more minhash values are used, the more accurate the approximation is.
Calculation of signature matrix and how it is used to compute similarities between vectors. Similarities computed using Jaccard similarity and signatures should normally be approximately equal.
This is just a useful observation. It turns out that there is a whole theorem behind the scenes. Let us understand why Jaccard index can be calculated by using signatures.
"
LLM,Similarity Search,Locality Sensitive Hashing,"At the current moment, we can transform raw texts into dense signatures of equal length preserving the information about similarity. Nevertheless, in practice, such dense signatures still usually have high dimensions and it would be inefficient to directly compare them.
Consider n = 10⁶ documents with their signatures of length 100. Assuming that a single number of a signature requires 4 bytes to store, then the whole signature would require 400 bytes. For storing n = 10⁶ documents, 400 MB of space is needed which is doable in reality. But comparing each document with each other in a brute-force manner would require approximately 5 * 10¹¹ comparisons which is too much, especially when n is even larger.
To avoid the problem, it is possible to build a hash table to accelerate search performance but even if two signatures are very similar and differ only in 1 position, they are still likely to have a different hash (because vector remainders are likely to be different). However, we normally want them to fall into the same bucket. This is where LSH comes to the rescue.
LSH mechanism builds a hash table consisting of several parts which puts a pair of signatures into the same bucket if they have at least one corresponding part.
LSH takes a signature matrix and horizontally divides it into equal b parts called bands each containing r rows. Instead of plugging the whole signature into a single hash function, the signature is divided by b parts and each subsignature is processed independently by a hash function. As a consequence, each of the subsignatures falls into separate buckets.
Example of using LSH. Two signatures of length 9 are divided into b = 3 bands each containing r = 3 rows. Each subvector is hashed into one of k possible buckets. Since there is a match in the second band (both subvectors have the same hash value), we consider a pair of these signatures as candidates to be the nearest neighbours.
If there is at least one collision between corresponding subvectors of two different signatures, the signatures are considered candidates. As we can see, this condition is more flexible since for considering vectors as candidates they do not need to be absolutely equal. Nevertheless, this increases the number of false positives: a pair of different signatures can have a single corresponding part but in overall be completely different. Depending on the problem, it is always better to optimize parameters b, r and k.
"
LLM,Sampling,Top-k and Top-p Sampling,"When you opt for sampling rather than greedy decoding, you’ll have an additional two hyperparameters with which to influence a model’s output: top-k and top-p sampling values.  
The Top-k sampling value is an integer that ranges from 1 to 100 (with a default value of 50) that specifies that the tokens sampled by the model should be those with the highest probabilities until the set value is reached. To better illustrate how top-k sampling works, let’s use a brief example.
Let’s say you have the sentence “I went to meet a friend…”. 

Now, out of the vast number of ways to end this sentence, let’s look at the five examples provided below – each beginning with a different token:
at the library 
for a brief work lunch
to discuss our shared homework assignment
in the centre of the city 
on the other side of town 
From there, let’s assign each of the initial tokens for each sentence a probability.
Now, if we set the top-k sampling value to 2, it will only add at and for to the sampling sunset from which it selects an output token. Setting it to 5, by contrast, would mean all options could be considered. So, in short, the higher the k-sampling value, the greater the potential variety in output.
Alternatively, the Top-p sampling value is a decimal number in the range of 0.0 to 1.0, that configures a model to sample the tokens with the highest probabilities until the sum of those probabilities reaches the set value.
Returning to the above table, if the top-p sampling value is set to 0.7, once again, at and for will be the only tokens included in the subset, as their combined probabilities are 0.55 (0.30 + 0.25). As at, for, and to have a cumulative probability of 0.77 (0.30 + 0.25 + 0.22), this breaches the set threshold of 0.7 and to is excluded from the subset as a result. As with top-k sampling, the higher the value, the more varied the output. 
Lastly, in the event both sampling values are set, top-k takes precedence – with all probabilities outside the set threshold set to 0. 
"
LLM,Sampling,Temperature,"Temperature performs a similar function to the above-described top-k and top-p sampling values, providing a way to vary the range of possible output tokens and influence the model’s “creativity”. It is represented by a decimal number between 0.0 (which is effectively the same as greedy decoding, whereby the token with the highest probability is added to the output) and 2.0 (maximum creativity). 
The temperature hyperparameter influences output by changing the shape of the token probability distribution. For low temperatures, the difference between probabilities is amplified, so tokens with higher probabilities become even more likely to be output compared to less-likely tokens. Consequently, you should set a lower temperature value when you want your model to generate more predictable or dependable responses.
In contrast, high temperatures cause token probabilities to converge closer to one another, so less likely or unusual tokens receive an increased chance of being output. In light of this, you should set a higher temperature value when you want to increase the randomness and creativity of responses.
"
LLM,Sampling,Stop Sequences,"Aside from the max output tokens hyperparameter, the other way to influence the length of an LLM’s response is by specifying a stop sequence, i.e., a string composed of one or more characters, which automatically stops a model’s output. A common example of a stop sequence is a period (full stop).
Alternatively, you can specify the end of a sequence by setting a stop token limit – which is an integer value rather than a string. For instance,  if the stop token limit is set to 1, the generated output will stop at a sentence. If it’s set to 2, on the other hand, the response will be constrained to a paragraph. 
A reason you might set a stop sequence or stop token limit is that, similar to the max output tokens parameter, you have greater control over inference, which may be a concern if budget is a consideration. 
"
LLM,Sampling,Frequency and Presence Penalties,"A frequency, or repetition, penalty, which is a decimal between -2.0 and 2.0, is a an LLM hyperparameter that indicates to a model that it should refrain from using the same tokens too often. It works by lowering the probabilities of tokens that were recently added to a response, so they’re less likely to be repeated to produce a more diverse output.

The presence penalty works in a similar way but is only applied to tokens that have been used at least once – while the frequency is applied proportionally to how often a specific token has been used. In other words, the frequency penalty affects output by preventing repetition, while the presence penalty encourages a wider assortment of tokens. 
"
LLM,Inference,"Compute-Bound, Memory-Bound, and Overhead-Bound Regimes","When optimizing machine learning and deep learning workloads (or any computational tasks), performance bottlenecks usually fall into three categories:
 1. Compute-Bound Regime (CPU/GPU Limited)
The system spends most of its time performing computations.
The bottleneck is processing power (FLOPs, tensor operations, etc.), not memory or communication.
Common in dense matrix multiplications, deep learning forward/backward passes.
 2. Memory-Bound Regime (Bandwidth Limited)
The system spends most of its time waiting for data to be fetched from memory (VRAM, RAM).
Computation itself is not the bottleneck, but memory bandwidth is.
Common in large models with high memory access needs (LLMs, CNNs, attention layers with large KV Cache).
 3. Overhead-Bound Regime (Latency Limited)
Performance is limited by non-computational overheads, such as: 
Kernel launch latency.
CPU-GPU synchronization delays.
Framework inefficiencies (Python GIL, scheduling delays).
Communication bottlenecks in distributed training (e.g., network latency in multi-GPU setups).
Link: https://horace.io/brrr_intro.html 
"
LLM,Inference,LLM inference,"For each request:
You start with a sequence of tokens (called the ""prefix"" or ""prompt"").
The LLM produces a sequence of completion tokens, stopping only after producing a stop token or reaching a maximum sequence length.
This toy example shows a hypothetical model which supports a maximum sequence length of 8 tokens (T1, T2, …, T8). Starting from the prompt tokens (yellow), the iterative process generates a single token at a time (blue). Once the model generates an end-of-sequence token (red), the generation loop stops. This example shows a batch of only one input sequence, so the batch size is 1.
The initial ingestion (“prefill”) of the prompt ""What is the capital of California: "" takes about as much time as the generation of each subsequent token. This is because the prefill phase pre-computes some inputs of the attention mechanism that remain constant over the lifetime of the generation. This prefill phase efficiently uses the GPU’s parallel compute because these inputs can be computed independently of each other.
LLM inference is memory-IO bound, not compute bound. In other words, it currently takes more time to load 1MB of data to the GPU’s compute cores than it does for those compute cores to perform LLM computations on 1MB of data. This means that LLM inference throughput is largely determined by how large a batch you can fit into high-bandwidth GPU memory.
The amount of GPU memory consumed scales with the base model size + the length of the token sequence. In Numbers every LLM developer should know, it’s estimated that a 13B parameter model consumes nearly 1MB of state for each token in a sequence. On a higher-end A100 GPU with 40GB RAM, back-of-the-envelope math suggests that since 14 GB are left after storing the 26GB of model parameters, ~14k tokens can be held in memory at once. This may seem high but is actually quite limiting; if we limit our sequence lengths to 512, we can process at most ~28 sequences in a batch. The problem is worse for higher sequence lengths; a sequence length of 2048 means our batch size is limited to 7 sequences. Note that this is an upper bound since it doesn’t leave room for storing intermediate computations.
"
LLM,Inference,Metrics for LLM Serving,"Time To First Token (TTFT): How quickly users start seeing the model's output after entering their query. Low waiting times for a response are essential in real-time interactions, but less important in offline workloads. This metric is driven by the time required to process the prompt and then generate the first output token.
Time Per Output Token (TPOT): Time to generate an output token for each user that is querying our system. This metric corresponds with how each user will perceive the ""speed"" of the model. For example, a TPOT of 100 milliseconds/tok would be 10 tokens per second per user, or ~450 words per minute, which is faster than a typical person can read.
Latency: The overall time it takes for the model to generate the full response for a user. Overall response latency can be calculated using the previous two metrics: latency = (TTFT) + (TPOT) * (the number of tokens to be generated).
Throughput: The number of output tokens per second an inference server can generate across all users and requests.
Requests Per Second (RPS): This is the average number of requests that can be successfully completed by the system in a 1-second period.
Our goal? The fastest time to first token, the highest throughput, and the quickest time per output token. In other words, we want our models to generate text as fast as possible for as many users as we can support.
Notably, there is a tradeoff between throughput and time per output token: if we process 16 user queries concurrently, we'll have higher throughput compared to running the queries sequentially, but we'll take longer to generate output tokens for each user.
If you have overall inference latency targets, here are some useful heuristics for evaluating models:
Output length dominates overall response latency: For average latency, you can usually just take your expected/max output token length and multiply it by an overall average time per output token for the model.
Input length is not significant for performance but important for hardware requirements: The addition of 512 input tokens increases latency less than the production of 8 additional output tokens in the MPT models. However, the need to support long inputs can make models harder to serve. For example, we recommend using the A100-80GB (or newer) to serve MPT-7B with its maximum context length of 2048 tokens.
Overall latency scales sub-linearly with model size: On the same hardware, larger models are slower, but the speed ratio won't necessarily match the parameter count ratio. MPT-30B latency is ~2.5x that of MPT-7B latency. Llama2-70B latency is ~2x that of Llama2-13B latency.
"
LLM,Inference,LLM Optimization Techniques,"Optimizing LLM inference requires reducing latency, memory usage, and computational overhead. Here are key techniques:
1. Efficient Computation Techniques
Kernel Fusion – Combines multiple GPU operations into one to reduce memory bottlenecks (e.g., fused attention, fused layer normalization).
Quantization – Reduces precision (e.g., FP16, INT8, 4-bit quantization) to lower memory usage and speed up computation.
Tensor Parallelism – Splits large matrix multiplications across multiple GPUs.
Speculative Decoding – Uses a smaller model to predict multiple tokens in advance, reducing inference time.
2. Memory Optimization
Paged Attention (vLLM) – Efficient memory management for KV cache, avoiding redundant memory copies.
FlashAttention – Optimized attention mechanism that minimizes memory reads/writes, improving speed.
KV Cache Optimization – Reuses previously computed key-value pairs to speed up autoregressive decoding.
3. Model Optimization
LoRA (Low-Rank Adaptation) – Fine-tunes only a small subset of model parameters, reducing computation.
Mixture of Experts (MoE) – Activates only a subset of model parameters per input, saving computation.
Pruning – Removes less important model weights to reduce size and inference cost.
4. Parallelism & Distributed Computing
Pipeline Parallelism – Splits model layers across multiple GPUs to balance computation.
Sequence Parallelism – Divides long input sequences across multiple GPUs for better efficiency.
Triton / TensorRT-LLM – Uses optimized GPU execution engines for high-performance inference.
5. Engineering Optimizations
Batching Requests – Processes multiple queries at once to maximize GPU utilization.
Token Merging (DeepSpeed-Mii) – Dynamically merges redundant tokens for faster processing.
Continuous Batching (vLLM) – Dynamically merges new and ongoing requests for improved throughput.
"
LLM,Inference,Kernel Fusion,"Kernel fusion is an optimization technique that combines multiple GPU operations (kernels) into a single, more efficient kernel to reduce memory access overhead and improve performance.
How it's used in LLM optimization:
Reduces memory bottlenecks – Instead of writing intermediate results to memory and reading them back, fused kernels keep computations in GPU registers.
Improves throughput – Fewer kernel launches mean lower scheduling overhead.
Optimizes common operations – In LLMs, fusion is used for matrix multiplications, activation functions, and layer norm computations.
Used in frameworks – Libraries like FlashAttention, TensorRT-LLM, and vLLM leverage fusion to speed up inference.
This helps LLMs run faster and use less memory, making them more efficient for real-time applications.
"
LLM,Inference,Quantization,"Quantization is a technique to reduce the computational and memory costs of running inference by representing the weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32).
Reducing the number of bits means the resulting model requires less memory storage, consumes less energy (in theory), and operations like matrix multiplication can be performed much faster with integer arithmetic. It also allows to run models on embedded devices, which sometimes only support integer data types.
Let’s consider a float x in [a, b], then we can write the following quantization scheme, also called the affine quantization scheme:
x = S * (x_q - Z)
where:
x_q is the quantized int8 value associated to x
S and Z are the quantization parameters
S is the scale, and is a positive float32
Z is called the zero-point, it is the int8 value corresponding to the value 0 in the float32 realm. This is important to be able to represent exactly the value 0 because it is used everywhere throughout machine learning models.
The quantized value x_q of x in [a, b] can be computed as follows:
x_q = round(x/S + Z)
Per-tensor and per-channel quantization
Depending on the accuracy / latency trade-off you are targetting you can play with the granularity of the quantization parameters:
Quantization parameters can be computed on a per-tensor basis, meaning that one pair of (S, Z) will be used per tensor.
Quantization parameters can be computed on a per-channel basis, meaning that it is possible to store a pair of (S, Z) per element along one of the dimensions of a tensor. For example for a tensor of shape [N, C, H, W], having per-channel quantization parameters for the second dimension would result in having C pairs of (S, Z). While this can give a better accuracy, it requires more memory.
Quantization Methods
Weights-only (wNa16 – N bits weights, 16 bits activations)
Reduces memory footprint
Speedups memory-bound
Keeps activations in higher precision to preserve accuracy.
GPT-Q
A post-training quantization technique where each row of the weight matrix is quantized independently to find a version of the weights that minimizes error. These weights are quantized to int4, stored as int32 (int4 x 8) and dequantized (restored) to fp16 on the fly during inference. This can save memory by almost 4x because the int4 weights are often dequantized in a fused kernel. You can also expect a substantial speedup in inference due to lower bandwidth requirements for lower bitwidth.
→ W4A16
→ 3,25x (1,5x–2x actually)
→ decoding only
→ Code and CUDA kernels published
→ Highly popular in open source
→ LLaMa.cpp CPU inference
wNaM (w4a8, w8a8)
also reduces compute 
speedups compute-bound
harder because of outliers
FP8
Static amax-scaling quantization
Several formats (E5M2, E4M3)
Per-tensor/per-token 
SmoothQuant
W8a8
Per-channel scale is accurate but not efficient
Since weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation.
AWQ
Activation-aware Weight Quantization (AWQ) (w4a8) doesn’t quantize all the weights in a model, and instead, it preserves a small percentage of weights that are important for LLM performance. This significantly reduces quantization loss such that you can run models in 4-bit precision without experiencing any performance degradation.
Calibration
The section above described how quantization from float32 to int8 works, but one question remains: how is the [a, b] range of float32 values determined? That is where calibration comes in to play.
Calibration is the step during quantization where the float32 ranges are computed. For weights it is quite easy since the actual range is known at quantization-time. But it is less clear for activations, and different approaches exist:
Post training dynamic quantization: the range for each activation is computed on the fly at runtime. While this gives great results without too much work, it can be a bit slower than static quantization because of the overhead introduced by computing the range each time. It is also not an option on certain hardware.
Post training static quantization: the range for each activation is computed in advance at quantization-time, typically by passing representative data through the model and recording the activation values. In practice, the steps are:
Observers are put on activations to record their values.
A certain number of forward passes on a calibration dataset is done (around 200 examples is enough).
The ranges for each computation are computed according to some calibration technique.
Quantization aware training: the range for each activation is computed at training-time, following the same idea than post training static quantization. But “fake quantize” operators are used instead of observers: they record values just as observers do, but they also simulate the error induced by quantization to let the model adapt to it.
For both post training static quantization and quantization aware training, it is necessary to define calibration techniques, the most common are:
Min-max: the computed range is [min observed value, max observed value], this works well with weights.
Moving average min-max: the computed range is [moving average min observed value, moving average max observed value], this works well with activations.
Histogram: records a histogram of values along with min and max values, then chooses according to some criterion:
Entropy: the range is computed as the one minimizing the error between the full-precision and the quantized data.
Mean Square Error: the range is computed as the one minimizing the mean square error between the full-precision and the quantized data.
Percentile: the range is computed using a given percentile value p on the observed values. The idea is to try to have p% of the observed values in the computed range. While this is possible when doing affine quantization, it is not always possible to exactly match that when doing symmetric quantization. You can check how it is done in ONNX Runtime for more details.
Advantages and Disadvantages
Advantages:
Lesser memory consumption: Lower bit width results in less memory for storage
Fast Inference: This is due to efficient computation due to its lower memory bandwidth requirements
Less energy consumption: Larger model need more data movement and storage resulting in more energy consumption. Hence a smaller model results in compartively lesser energy usage.
Smaller models: Can quantize to suit the need and deploy to device with samller hadware specifications.
Disadvantages
Potential loss in accuracy due to squeezing of high precision weight to lower precision.
"
LLM,Inference,KV Caching,"KV caching occurs during multiple token generation steps and only happens in the decoder (i.e., in decoder-only models like GPT, or in the decoder part of encoder-decoder models like T5). Models like BERT are not generative and therefore do not have KV caching.
Since the decoder is causal (i.e., the attention of a token only depends on its preceding tokens), at each generation step we are recalculating the same previous token attention, when we actually just want to calculate the attention for the new token.
This is where KV comes into play. By caching the previous Keys and Values, we can focus on only calculating the attention for the new token.
Why is this optimization important? As seen in the picture above, the matrices obtained with KV caching are way smaller, which leads to faster matrix multiplications. The only downside is that it needs more GPU VRAM (or CPU RAM if GPU is not being used) to cache the Key and Value states.
"
LLM,Inference,LLM batching ,"GPUs are massively-parallel compute architectures, with compute rates (measured in floating-point operations per second, or flops) in the teraflop (A100) or even petaflop (H100) range. Despite these staggering amounts of compute, LLMs struggle to achieve saturation because so much of the chip’s memory bandwidth is spent loading model parameters.
Batching is one way to improve the situation; instead of loading new model parameters each time you have an input sequence, you can load the model parameters once and then use them to process many input sequences. This more efficiently uses the chip’s memory bandwidth, leading to higher compute utilization, higher throughput, and cheaper LLM inference.
Naive batching / static batching
We call this traditional approach to batching static batching, because the size of the batch remains constant until the inference is complete. Here’s an illustration of static batching in context of LLM inference:
Unlike traditional deep learning models, batching for LLMs can be tricky due to the iterative nature of their inference. Intuitively, this is because requests can ""finish"" earlier in a batch, but it is tricky to release their resources and add new requests to the batch that may be at different completion states. This means that as the GPU is underutilized as generation lengths of different sequences in a batch differ from the largest generation length of the batch. In the figure on the right above, this is illustrated by the white squares after end-of-sequence tokens for sequences 1, 3, and 4.
How often does static batching under-utilize the GPU? It depends on the generation lengths of sequences in a batch. Without restrictive assumptions on user input and model output, unoptimized production-grade LLM systems simply can’t serve traffic without underutilizing GPUs and incurring unnecessarily high costs. We need to optimize how we serve LLMs for their power to be broadly accessible.
Continuous (Inflight) batching
The industry recognized the inefficiency and came up with a better approach. Orca: A Distributed Serving System for Transformer-Based Generative Models is a paper presented in OSDI ‘22 which is the first to our knowledge to tackle this problem. Instead of waiting until every sequence in a batch has completed generation, Orca implements iteration-level scheduling where the batch size is determined per iteration. The result is that once a sequence in a batch has completed generation, a new sequence can be inserted in its place, yielding higher GPU utilization than static batching.
Reality is a bit more complicated than this simplified model: since the prefill phase takes compute and has a different computational pattern than generation, it cannot be easily batched with the generation of tokens. Continuous batching frameworks currently manage this via hyperparameter: waiting_served_ratio, or the ratio of requests waiting for prefill to those waiting end-of-sequence tokens.
Speaking of frameworks, Hugging Face has productionized continuous batching in their Rust- and Python-based text-generation-inference LLM inference server. We use their implementation to understand the performance characteristics of continuous batching in our benchmarks below.
"
LLM,Inference,Paged Attention ,"PagedAttention is a new attention mechanism implemented in vLLM (GitHub). It takes inspiration from traditional OS concepts such as paging and virtual memory. They allow the KV cache (what is computed in the “prefill” phase, discussed above) to be non-contiguous by allocating memory in fixed-size “pages”, or blocks. The attention mechanism can then be rewritten to operate on block-aligned inputs, allowing attention to be performed on non-contiguous memory ranges.
This means that buffer allocation can happen just-in-time instead of ahead-of-time: when starting a new generation, the framework does not need to allocate a contiguous buffer of size maximum_context_length. Each iteration, the scheduler can decide if it needs more room for a particular generation, and allocate on the fly without any degradation to PagedAttention’s performance. This doesn’t guarantee perfect utilization of memory (their blog says the wastage is now limited to under 4%, only in the last block), but it significantly improves upon wastage from ahead-of-time allocation schemes used widely by the industry today.
"
LLM,Inference,Flash Attention,"Scaling the transformer architecture is heavily bottlenecked by the self-attention mechanism, which has quadratic time and memory complexity. Recent developments in accelerator hardware mainly focus on enhancing compute capacities and not memory and transferring data between hardware. This results in attention operation having a memory bottleneck. Flash Attention is an attention algorithm used to reduce this problem and scale transformer-based models more efficiently, enabling faster training and inference.
Standard attention mechanism uses High Bandwidth Memory (HBM) to store, read and write keys, queries and values. HBM is large in memory, but slow in processing, meanwhile SRAM is smaller in memory, but faster in operations. In the standard attention implementation, the cost of loading and writing keys, queries, and values from HBM is high. It loads keys, queries, and values from HBM to GPU on-chip SRAM, performs a single step of the attention mechanism, writes it back to HBM, and repeats this for every single attention step. Instead, Flash Attention loads keys, queries, and values once, fuses the operations of the attention mechanism, and writes them back.
"
LLM,Inference,Chunked-prefills,"SARATHI utilizes “chunked-prefills,” which divide a prefill request into equal-sized chunks, along with “decode-maximal batching,” which forms a batch by combining one prefill chunk with additional decode requests. During inference, the prefill chunk fully utilizes GPU resources, while the decode requests piggyback, significantly reducing computational costs compared to processing decodes independently. This method enables the creation of multiple decode-maximal batches from a single prefill request, optimizing the handling of decode requests. Additionally, the consistent compute load of these batches mitigates imbalances across micro-batches, effectively reducing pipeline inefficiencies.
Chunked-prefills is a mechanism for splitting the prefill phase of large language model inference, based on two key insights. 
First, there is a point of diminishing returns in throughput when increasing the number of prefill tokens for a given model and GPU.
Second, in practical applications, the prefill size is often large (1K–4K tokens in production), making it feasible to split the prefill request into smaller compute units. Implementing chunked-prefills requires careful attention to setting the attention masks. For example, if a 1K token input prompt is split into four chunks of 256 tokens each, the attention masks must be adjusted for each subsequent chunk to ensure that each query token can access all preceding tokens but not those that follow. This approach ensures that the chunked-prefill computation is mathematically equivalent to processing the full prefill in one go.
"
LLM,Inference,Speculative decoding,"Speculative Decoding is an LLM inference optimization technique that speeds up text generation by first using a smaller, faster model (draft model) to generate multiple token candidates. The main, larger model then verifies and corrects these predictions in parallel, reducing the number of sequential steps needed.
How It Works:
Drafting → A lightweight model predicts a few tokens ahead.
Verification → The larger LLM processes these tokens in parallel, accepting correct ones and adjusting incorrect ones.
Acceleration → This reduces the number of sequential forward passes, improving efficiency without degrading quality.
Key Benefits:
Faster inference (reduces sequential processing)
Lower latency (especially for high-throughput systems).
Can work with existing models without retraining.
It's commonly used in vLLM, TGI, and NVIDIA's TRT-LLM for optimizing LLM responses. 
EAGLE
Drafter is a head instead of model
Frozen base model’s body
High AccRate
Head – 1-layer transformer over hiddens of base model
Tree of hypotheses
"
LLM,Inference,Knowledge Distillation,"Most fine-tuned LLMs contain enormous numbers of parameters. Consequently, foundation LLMs require enormous computational and environmental resources to generate predictions. Note that large swaths of those parameters are typically irrelevant for a specific application.
Distillation creates a smaller version of an LLM. The distilled LLM generates predictions much faster and requires fewer computational and environmental resources than the full LLM. However, the distilled model’s predictions are generally not quite as good as the original LLM’s predictions. Recall that LLMs with more parameters almost always generate better predictions than LLMs with fewer parameters.
The most common form of distillation uses bulk inference to label data. This labeled data is then used to train a new, smaller model (known as the student model) that can be more affordably served. The labeled data serves as a channel by which the larger model (known as the teacher model) funnels its knowledge to the smaller model.
Teacher p(y|x): usually a LLM, e.g. GPT-3 (175B)
Achieves SOTA quality
Doesn't fit inference computational budget
Student q(y|x): small LM, e.g. T5 XL (3B)
Unable to reach teacher’s quality by ordinary training
Fits inference computational budget
Hard- and Soft-Label KD
The overall training loss LLL is typically a weighted sum of:
Hard Loss: The cross-entropy between the student’s predictions and the true labels.
Soft Loss: The cross-entropy between the student’s predictions (with temperature scaling) and the teacher’s softened outputs. Cross-Entropy loss. 
KL KD
KL Divergence measures how one probability distribution diverges from another:
The KL-divergence is nicer as a loss since it will equal 0 when the student network matches the teacher on all labels. In contrast, if we use X-entropy, then the loss will fluctuate even when the student and teacher output the exact same thing, and it will fluctuate according to the batch
KLDiv(P||Q) = entropy(P) + crossentropy(P, Q).
Speculative KD
Student generates
Teacher monitors
Teacher’s top-k
No backprop through sampling
"
LLM,Inference,Offline inference,"The number of parameters in an LLM is sometimes so large that online inference is too slow to be practical for real-world tasks like regression or classification. Consequently, many engineering teams rely on offline inference (also known as bulk inference or static inference) instead. In other words, rather than responding to queries at serving time, the trained model makes predictions in advance and then caches those predictions.
It doesn't matter if it takes a long time for an LLM to complete its task if the LLM only has to perform the task once a week or once a month.
For example, Google Search used an LLM to perform offline inference in order to cache a list of over 800 synonyms for Covid vaccines in more than 50 languages. Google Search then used the cached list to identify queries about vaccines in live traffic.
"
LLM,Inference,Optimization Frameworks,"TRT-LLM → Best for NVIDIA GPU users who need extreme performance via TensorRT optimizations.
vLLM → Best for high-throughput applications (e.g., chatbots) due to PagedAttention & continuous batching.
SGLang → Good for serverless or multi-cloud inference, focusing on scalability & efficiency.
TGI → Hugging Face’s production-ready LLM inference API, great for deploying Hugging Face models at scale.
"
LLM,Inference,Triton,"Triton is an open-source framework developed by OpenAI for writing high-performance GPU code with a Python-like syntax. It simplifies writing custom GPU kernels while achieving performance comparable to handwritten CUDA code.
Triton is a language for programming GPUs.
More convenient than CUDA.
Allows writing Python-like code that compiles to PTX (Parallel Thread Execution).
PTX is the same intermediate representation used by CUDA.
Triton Compiler:
Optimizes code by rearranging it for better performance without changing its meaning.
Targets the same hardware as CUDA.
When to Use Triton
- Optimization Steps:
1. Use torch.compile():
    - Start by using torch.compile() to optimize your code.
2. Adapt Your Code:
    - Rewrite code to be more suitable for torch.compile().
        - E.g., eliminate graph breaks to enable CUDA graphs.
3. Profile and Identify Bottlenecks:
    - Find slow parts of your code using profiling tools.
    - Write custom Triton kernels for these parts.
4. Consider CUDA:
    - If still not fast enough, write custom CUDA kernels.
Rough Edges in Triton
- New-ish Project:
    - Contains rough edges; code may not behave as expected.
    - Expected to become more polished over time.
Recommendation:
    - Debugging is important; use “simulator mode” when possible.
    - Be aware of limitations on older GPUs or with certain operations.
Link: https://christianjmills.com/posts/cuda-mode-notes/lecture-014/#auto-tuning 
"
LLM,LLMOps,What is LLMOps (large language model operations)?,"LLMOps, or large language model operations, refers to the practices and processes involved in managing and operating large language models (LLMs). LLMs are artificial intelligence (AI) models trained on vast datasets of text and code, enabling them to perform various language-related tasks, such as text generation, translation, and question answering.
LLMOps involves a comprehensive set of activities, including:
Model deployment and maintenance: deploying and managing LLMs on cloud platforms or on-premises infrastructure
Data management: curating and preparing training data, as well as monitoring and maintaining data quality
Model training and fine-tuning: training and refining LLMs to improve their performance on specific tasks
Monitoring and evaluation: tracking LLM performance, identifying errors, and optimizing models
Security and compliance: ensuring the security and regulatory compliance of LLM operations
What is the difference between LLMOps and MLOps?
LLMOps is a specialized subset of MLOps (machine learning operations), which focuses specifically on the challenges and requirements of managing LLMs. While MLOps covers the general principles and practices of managing machine learning models, LLMOps addresses the unique characteristics of LLMs, such as their large size, complex training requirements, and high computational demands.
How does LLMOps work?
LLMOps involves a number of different steps, including:
Data collection and preparation: LLMs require large amounts of data to train. This data must be collected and prepared in a way that is suitable for training the model.
Model development: LLMs are developed using a variety of techniques, including unsupervised learning, supervised learning, and reinforcement learning.
Model deployment: Once a LLM has been developed, it must be deployed to a production environment. This involves setting up the necessary infrastructure and configuring the model to run on a specific platform.
Model management: LLMs require ongoing management to ensure that they are performing as expected. This includes monitoring the model's performance, retraining the model as needed, and making sure that the model is secure.
Benefits of LLMOps
LLMOps (large language model operations) offers numerous benefits for organizations looking to manage and deploy LLMs (large language models) effectively. These benefits include:
Performance
LLMOps tools and techniques help organizations optimize the performance of their LLMs by identifying and resolving bottlenecks, fine-tuning model parameters, and implementing efficient deployment strategies. This can lead to improved accuracy, faster response times, and better overall user experiences.
Scalability
LLMOps provides a scalable and flexible framework for managing LLMs, enabling organizations to easily adapt to changing demands and requirements. 
Risk reduction
LLMOps helps organizations mitigate risks associated with deploying and operating LLMs. By implementing robust monitoring systems, establishing disaster recovery plans, and conducting regular security audits, LLMOps reduces the likelihood of outages, data breaches, and other disruptions. This proactive approach minimizes the impact of potential risks and ensures the continuous availability and reliability of LLMs.
Efficiency
LLMOps streamlines the entire life cycle of LLMs, from data preparation and model training to deployment and monitoring. Automated tools and standardized processes improve efficiency by reducing manual tasks, optimizing resource utilization, and minimizing the time required for model development and deployment.
Best practices for LLMOps
LLMOps (large language model operations) best practices are a set of guidelines and recommendations that help organizations manage and deploy LLMs (large language models) effectively and efficiently. These best practices cover various aspects of the LLMOps life cycle, including data management, model training, deployment, and monitoring.
Data management
Use high-quality data: LLMs require large amounts of high-quality data to train effectively. Organizations should ensure that the data used for training is clean, accurate, and relevant to the desired use case.
Manage data efficiently: LLMs can generate vast amounts of data during training and inference. Organizations should implement efficient data management strategies, such as data compression and data partitioning, to optimize storage and retrieval.
Establish data governance: Clear data governance policies and procedures should be established to ensure the secure and responsible use of data throughout the LLMOps life cycle.
Model training
Choose the right training algorithm: Different training algorithms are suitable for different types of LLMs and tasks. Organizations should carefully evaluate the available training algorithms and select the one that best aligns with their specific requirements.
Optimize training parameters: Hyperparameter tuning is important for optimizing LLM performance. Experiment with different training parameters, such as learning rate and batch size, to find the optimal settings for your models.
Monitor training progress: Regular monitoring of training progress is essential to identify potential issues and make necessary adjustments. Organizations should implement metrics and dashboards to track key training indicators, such as loss and accuracy.
Deployment
Choose the right deployment strategy: LLMs can be deployed in various ways, such as cloud-based services, on-premises infrastructure, or edge devices. Carefully consider their specific requirements and choose the deployment strategy that best meets their needs.
Optimize deployment performance: Once deployed, LLMs should be monitored and optimized for performance. This may involve scaling resources, adjusting model parameters, or implementing caching mechanisms to improve response times.
Ensure security: Strong security measures should be implemented to protect LLMs and the data they process. This includes access controls, data encryption, and regular security audits.
Monitoring
Establish monitoring metrics: Key performance indicators (KPIs) should be established to monitor the health and performance of LLMs. These metrics may include accuracy, latency, and resource utilization.
Implement real-time monitoring: Real-time monitoring systems should be implemented to detect and respond to any issues or anomalies that may arise during operations.
Analyze monitoring data: Monitoring data should be regularly analyzed to identify trends, patterns, and potential areas for improvement. This analysis helps optimize LLMOps processes and ensure the continuous delivery of high-quality LLMs.
"
LLM,Prompting,Prompting an LLM,"You can achieve a lot with simple prompts, but the quality of results depends on how much information you provide it and how well-crafted the prompt is. A prompt can contain information like the instruction or question you are passing to the model and include other details such as context, inputs, or examples. You can use these elements to instruct the model more effectively to improve the quality of results.
Let's get started by going over a basic example of a simple prompt:
Prompt
The sky is
Output:
blue.
"
LLM,Prompting,Zero-Shot Prompting,"Large language models (LLMs) today, such as GPT-3.5 Turbo, GPT-4, and Claude 3, are tuned to follow instructions and are trained on large amounts of data. Large-scale training makes these models capable of performing some tasks in a ""zero-shot"" manner. Zero-shot prompting means that the prompt used to interact with the model won't contain examples or demonstrations. The zero-shot prompt directly instructs the model to perform a task without any additional examples to steer it.
We tried a few zero-shot examples in the previous section. Here is one of the examples (ie., text classification) we used:
Prompt:
Classify the text into neutral, negative or positive. 
Text: I think the vacation is okay.
Sentiment:
Output:
Neutral
Note that in the prompt above we didn't provide the model with any examples of text alongside their classifications, the LLM already understands ""sentiment"" -- that's the zero-shot capabilities at work.
"
LLM,Prompting,Few-Shot Prompting,"Let's demonstrate few-shot prompting via an example that was presented in Brown et al. 2020. In the example, the task is to correctly use a new word in a sentence.
Prompt:
A ""whatpu"" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:
We were traveling in Africa and we saw these very cute whatpus.
To do a ""farduddle"" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:
Output:
When we won the game, we all started to farduddle in celebration.
We can observe that the model has somehow learned how to perform the task by providing it with just one example (i.e., 1-shot). For more difficult tasks, we can experiment with increasing the demonstrations (e.g., 3-shot, 5-shot, 10-shot, etc.).
Following the findings from Min et al. (2022), here are a few more tips about demonstrations/exemplars when doing few-shot:
""the label space and the distribution of the input text specified by the demonstrations are both important (regardless of whether the labels are correct for individual inputs)""
the format you use also plays a key role in performance, even if you just use random labels, this is much better than no labels at all.
additional results show that selecting random labels from a true distribution of labels (instead of a uniform distribution) also helps.
"
LLM,Prompting,Chain-of-Thought Prompting,"Introduced in Wei et al. (2022), chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding.
Prompt:
The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.
A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.
The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.
A: Adding all the odd numbers (17, 19) gives 36. The answer is True.
The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.
A: Adding all the odd numbers (11, 13) gives 24. The answer is True.
The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.
A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.
The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. 
A:
Output:
Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.
One recent idea that came out more recently is the idea of zero-shot CoT (Kojima et al. 2022) that essentially involves adding ""Let's think step by step"" to the original prompt.
Prompt:
I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?
Let's think step by step.
Output:
First, you started with 10 apples.
You gave away 2 apples to the neighbor and 2 to the repairman, so you had 6 apples left.
Then you bought 5 more apples, so now you had 11 apples.
Finally, you ate 1 apple, so you would remain with 10 apples.
Automatic Chain-of-Thought (Auto-CoT)
When applying chain-of-thought prompting with demonstrations, the process involves hand-crafting effective and diverse examples. This manual effort could lead to suboptimal solutions. Zhang et al. (2022) propose an approach to eliminate manual efforts by leveraging LLMs with ""Let's think step by step"" prompt to generate reasoning chains for demonstrations one by one. This automatic process can still end up with mistakes in generated chains. To mitigate the effects of the mistakes, the diversity of demonstrations matter. This work proposes Auto-CoT, which samples questions with diversity and generates reasoning chains to construct the demonstrations.
Auto-CoT consists of two main stages:
Stage 1): question clustering: partition questions of a given dataset into a few clusters
Stage 2): demonstration sampling: select a representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics
The simple heuristics could be length of questions (e.g., 60 tokens) and number of steps in rationale (e.g., 5 reasoning steps). This encourages the model to use simple and accurate demonstrations.
"
LLM,Agents,LLM Agents,"LLM based agents, hereinafter also referred to as LLM agents for short, involve LLM applications that can execute complex tasks through the use of an architecture that combines LLMs with key modules like planning and memory. When building LLM agents, an LLM serves as the main controller or ""brain"" that controls a flow of operations needed to complete a task or user request. The LLM agent may require key modules such as planning, memory, and tool usage.
To better motivate the usefulness of an LLM agent, let's say that we were interested in building a system that can help answer the following question:
What's the average daily calorie intake for 2023 in the United States?
The question above could potentially be answered using an LLM that already has the knowledge needed to answer the question directly. If the LLM doesn't have the relevant knowledge to answer the question, it's possible to use a simple RAG system where an LLM has access to health related information or reports. Now let's give the system a more complex question like the following:
How has the trend in the average daily calorie intake among adults changed over the last decade in the United States, and what impact might this have on obesity rates? Additionally, can you provide a graphical representation of the trend in obesity rates over this period?
To answer such a question, just using an LLM alone wouldn't be enough. You can combine the LLM with an external knowledge base to form a RAG system but this is still probably not enough to answer the complex query above. This is because the complex question above requires an LLM to break the task into subparts which can be addressed using tools and a flow of operations that leads to a desired final response. A possible solution is to build an LLM agent that has access to a search API, health-related publications, and public/private health database to provide relevant information related to calorie intake and obesity.
In addition, the LLM will need access to a ""code interpreter"" tool that helps take relevant data to produce useful charts that help understand trends in obesity. These are the possible high-level components of the hypothetical LLM agent but there are still important considerations such as creating a plan to address the task and potential access to a memory module that helps the agent keep track of the state of the flow of operations, observations, and overall progress.
"
LLM,Agents,LLM Agent Framework,"Generally speaking, an LLM agent framework can consist of the following core components:
User Request - a user question or request
Agent/Brain - the agent core acting as coordinator
Planning - assists the agent in planning future actions
Memory - manages the agent's past behaviors
"
LLM,Agents,Agent,"A large language model (LLM) with general-purpose capabilities serves as the main brain, agent module, or coordinator of the system. This component will be activated using a prompt template that entails important details about how the agent will operate, and the tools it will have access to (along with tool details).
While not mandatory, an agent can be profiled or be assigned a persona to define its role. This profiling information is typically written in the prompt which can include specific details like role details, personality, social information, and other demographic information. According to [Wang et al. 2023], the strategies to define an agent profile include handcrafting, LLM-generated or data-driven.
"
LLM,Agents,Planning Without Feedback,"The planning module helps to break down the necessary steps or subtasks the agent will solve individually to answer the user request. This step is important to enable the agent to reason better about the problem and reliably find a solution. The planning module will leverage an LLM to decompose a detailed plan which will include subtasks to help address the user question. Popular techniques for task decomposition include Chain of Thought(opens in a new tab) and Tree of Thoughts(opens in a new tab) which can be categorized as single-path reasoning and multi-path reasoning, respectively. Below is a figure comparing different strategies as formalized in Wang et al., 2023(opens in a new tab):
"
LLM,Agents,Planning With Feedback,"The planning modules above don't involve any feedback which makes it challenging to achieve long-horizon planning to solve complex tasks. To address this challenge, you can leverage a mechanism that enables the model to iteratively reflect and refine the execution plan based on past actions and observations. The goal is to correct and improve on past mistakes which helps to improve the quality of final results. This is particularly important in complex real-world environments and tasks where trial and error are key to completing tasks. Two popular methods for this reflection or critic mechanism include ReAct and Reflexion.
"
LLM,Agents,ReAct,"ReAct is inspired by the synergies between ""acting"" and ""reasoning"" which allow humans to learn new tasks and make decisions or reasoning.
Chain-of-thought (CoT) prompting has shown the capabilities of LLMs to carry out reasoning traces to generate answers to questions involving arithmetic and commonsense reasoning, among other tasks (Wei et al., 2022)(opens in a new tab). But its lack of access to the external world or inability to update its knowledge can lead to issues like fact hallucination and error propagation.
ReAct is a general paradigm that combines reasoning and acting with LLMs. ReAct prompts LLMs to generate verbal reasoning traces and actions for a task. This allows the system to perform dynamic reasoning to create, maintain, and adjust plans for acting while also enabling interaction to external environments (e.g., Wikipedia) to incorporate additional information into the reasoning. The figure below shows an example of ReAct and the different steps involved to perform question answering.
"
LLM,Agents,Memory,"The memory module helps to store the agent's internal logs including past thoughts, actions, and observations from the environment, including all interactions between agent and user. There are two main memory types that have been reported in the LLM agent literature:
Short-term memory - includes context information about the agent's current situations; this is typically realized by in-context learning which means it is short and finite due to context window constraints.
Long-term memory - includes the agent's past behaviors and thoughts that need to be retained and recalled over an extended period of time; this often leverages an external vector store accessible through fast and scalable retrieval to provide relevant information for the agent as needed.
Hybrid memory integrates both short-term memory and long-term memory to improve an agent's ability for long-range reasoning and accumulation of experiences.
There are also different memory formats to consider when building agents. Representative memory formats include natural language, embeddings, databases, and structured lists, among others. These can also be combined such as in Ghost in the Minecraft (GITM(opens in a new tab)) that utilizes a key-value structure where the keys are represented by natural language and values are represented by embedding vectors.
Both the planning and memory modules allow the agent to operate in a dynamic environment and enable it to effectively recall past behaviors and plan future actions.
"
LLM,Agents,Tools,"Tools correspond to a set of tool/s that enables the LLM agent to interact with external environments such as Wikipedia Search API, Code Interpreter, and Math Engine. Tools could also include databases, knowledge bases, and external models. When the agent interacts with external tools it executes tasks via workflows that assist the agent to obtain observations or necessary information to complete subtasks and satisfy the user request. In our initial health-related query, a code interpreter is an example of a tool that executes code and generates the necessary chart information requested by the user.
"
LLM,Miscellaneous,MTP — Multi-Token Prediction,"Meta introduces a new training paradigm that alters the overall model architecture. This approach, called multi-token prediction, changes the traditional method by having the model predict several future words simultaneously instead of just one. At each position in a sentence, the model uses multiple prediction pathways, or “heads,” to forecast the next several words all at once, working collaboratively to improve efficiency and coherence.
In simple terms, we modify the LLM to predict the next four words instead of just the next one. To achieve this, we add more output heads to the model.
However, this doesn’t mean we predict 16 tokens in total. Each of the four heads produces four tokens, but we only use the first token from each head, discarding the rest (denoted as words 5, 6, 7, and 8 in the explanation).
The core idea of multi-token prediction is to train the model to predict a sequence of future words from each position in the training data, rather than just the next word. This method uses a shared underlying structure called a transformer trunk to understand the context and then employs multiple independent prediction heads to guess future words in parallel.
"
LLM,Miscellaneous,"If we trained BERT on sequences of 512 tokens, what will happen if we input text with 1024 tokens into the model?","If BERT is trained on sequences of 512 tokens, inputting 1024 tokens will exceed the model's input limit. The text will either be truncated to 512 tokens, or you might need to split the input into smaller segments and process them separately.
If truncation is enabled, the model will automatically cut off the input at the maximum token length (512 tokens). This means any tokens beyond that limit will be discarded, and only the first 512 tokens will be processed.
  Increase the model's maximum token length: You can fine-tune a version of BERT (or use a larger model like Longformer, BigBird, etc.) that supports longer sequences, such as 1024 tokens. This would allow you to process longer texts without truncation.
  Split the input into smaller chunks: If the model can’t handle longer sequences, you can break the 1024-token input into two parts (e.g., the first 512 tokens and the next 512 tokens), process them separately, and then combine the results, depending on the task.
  Use sliding window technique: This involves breaking the input into overlapping chunks (e.g., 512 tokens with a 256-token overlap) and then processing each chunk, allowing the model to capture more context across the sequence.
"
LLM,Miscellaneous,Different Types of RAM,"What is RAM?
Random Access Memory, is a type of computer memory that allows data to be read and written randomly, meaning that the computer can access any location in the memory directly rather than having to read the data in a specific order. This makes RAM an essential component of a computer system, as it enables the CPU to access data quickly and efficiently.
RAM is volatile in nature, which means if the power goes off, the stored information is lost. RAM is used to store the data that is currently processed by the CPU. Most of the programs and data that are modifiable are stored in RAM. 
Mainly RAM have 2types
SRAM (Static RAM)
DRAM (Dynamic RAM)
SRAM Memory Cell
Static memories(SRAM) are memories that consist of circuits capable of retaining their state as long as power is on. Thus this type of memory is called volatile memory. The below figure shows a cell diagram of SRAM. A latch is formed by two inverters connected as shown in the figure. Two transistors T1 and T2 are used for connecting the latch with two-bit lines. The purpose of these transistors is to act as switches that can be opened or closed under the control of the word line, which is controlled by the address decoder. When the word line is at 0-level, the transistors are turned off and the latch remains its information. SRAM does not require refresh time. For example, the cell is at state 1 if the logic value at point A is 1 and at point, B is 0. This state is retained as long as the word line is not activated. 
What is DRAM?
DRAM stores the binary information in the form of electric charges applied to capacitors. The stored information on the capacitors tends to lose over a period of time and thus the capacitors must be periodically recharged to retain their usage. DRAM requires refresh time. The main memory is generally made up of DRAM chips.
Difference Between SRAM and DRAM
The below table lists some of the differences between SRAM and DRAM.
"
LLM,Miscellaneous,Matrix-Matrix Multiplication,"GEMMs (General Matrix Multiplications) are a fundamental building block for many operations in neural networks, for example fully-connected layers, recurrent layers such as RNNs, LSTMs or GRUs, and convolutional layers. In this guide, we describe GEMM performance fundamentals common to understanding the performance of such layers. 
GEMM is defined as the operation C = αAB + βC, with A and B as matrix inputs, α and β as scalar inputs, and C as a pre-existing matrix which is overwritten by the output. A plain matrix product AB is a GEMM with α equal to one and β equal to zero. For example, in the forward pass of a fully-connected layer, the weight matrix would be argument A, incoming activations would be argument B, and α and β would typically be 1 and 0, respectively. β can be 1 in some cases, for example, if we’re combining the addition of a skip-connection with a linear operation.
"
LLM,Links,Links,"Building effective agents: https://www.anthropic.com/research/building-effective-agents 
Efficient Deep Learning: A Comprehensive Overview of Optimization Techniques 👐 📚:
https://huggingface.co/blog/Isayoften/optimization-rush 
Numbers every LLM Developer should know: https://github.com/ray-project/llm-numbers?tab=readme-ov-file#1-mb-gpu-memory-required-for-1-token-of-output-with-a-13b-parameter-model 
LLM Deployment: Best Practices and Tips: https://medium.com/arize-ai/best-practices-for-llm-deployment-81937af82a57 
Making Deep Learning Go Brrrr From First Principles: https://horace.io/brrr_intro.html 
A Visual Guide to Quantization: https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization 
Matrix Multiplication Background User's Guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html 
"
Metrics,Classification,List Classification Metrics,"Accuracy, Cross-Entropy, Precision, Recall, F1-score, AUC ROC
"
Metrics,Classification,Classification Accuracy,"Classification Accuracy is what we usually mean, when we use the term accuracy. It is the ratio of number of correct predictions to the total number of input samples.
It works well only if there are equal number of samples belonging to each class.
"
Metrics,Classification,Logarithmic Loss (cross-entropy loss),"Logarithmic Loss or Log Loss, works by penalising the false classifications. It works well for multi-class classification. When working with Log Loss, the classifier must assign probability to each class for all the samples. Suppose, there are N samples belonging to M classes, then the Log Loss is calculated as below :
Log Loss has no upper bound and it exists on the range [0, ∞). Log Loss nearer to 0 indicates higher accuracy, whereas if the Log Loss is away from 0 then it indicates lower accuracy.
In general, minimising Log Loss gives greater accuracy for the classifier.
"
Metrics,Classification,Hinge Loss,"Hinge loss is used in binary classification problems where the objective is to separate the data points in two classes typically labeled as +1 and -1.
Mathematically, Hinge loss for a data point can be represented as :
L(y,f(x))=max(0,1–y∗f(x))
Here,
y- the actual class (-1 or 1)
f(x) – the output of the classifier for the datapoint
Case 1 : Correct Classification and |y| ≥ 1
In this case the product t.y will always be positive and its value greater than 1 and therefore the value of 1-t.y will be negative. So the loss function value max(0,1-t.y) will always be zero. Here there is no penalty to the model as model correctly classifies the data point.
Case 2 : Correct Classification and |y| < 1
In this case the product t.y will always be positive , but its value will be less than 1 and therefore the value of 1-t.y will be positive with value ranging between 0 to 1. Hence the loss function value will be the value of 1-t.y. This is indicated by the yellow region in above graph. Here though the model has correctly classified the data we are penalizing the model because it has not classified it with much confidence (|y| < 1) as the classification score is less than 1. We want the model to have a classification score of at least 1 for all the points.
Case 3: Incorrect Classification
In this case either of t or y will be negative. Therefore the product t.y will always be negative and the value of (1-t)y will be always positive and greater than 1. So the loss function value max(0,1-t.y) will always be the value given by (1-t)y . Here the loss value will increase linearly with increase in value of y.
"
Metrics,Classification,Advantages and disadvantages of Hinge loss for SVM,"Advantages:
Margin maximization: Hinge loss is designed to maximize the margin between different classes, which is the distance between the separating hyperplane and the closest data points. Maximizing the margin can lead to better generalization performance and improve the ability of the classifier to handle new data.
Robustness to outliers: Hinge loss is less sensitive to outliers than other loss functions like mean squared error. Outliers can have a significant impact on the learned model and can cause overfitting, but hinge loss mitigates this effect by ignoring points that are correctly classified but are still close to the decision boundary.
Sparsity: SVM with hinge loss can result in a sparse model, which means that many of the coefficients in the weight vector are set to zero. This can make the model more interpretable and reduce the computational cost of inference.
Disadvantages:
Non-smoothness: The hinge loss function is non-smooth and non-differentiable at 0, which can make it difficult to optimize using some numerical optimization methods. However, sub-gradient methods can be used to optimize the hinge loss function.
Parameter sensitivity: SVM with hinge loss has a regularization parameter that controls the trade-off between maximizing the margin and minimizing the classification error. The choice of this parameter can have a significant impact on the performance of the model, and selecting the optimal value can be challenging.
Limited applicability: SVM with hinge loss is a binary classifier and cannot be directly applied to multi-class problems. However, there are techniques, such as one-vs-rest or one-vs-one, that can be used to extend SVM with hinge loss to multi-class problems.
Overall, hinge loss for SVM is a popular and effective method for binary classification tasks, particularly in scenarios where there are a large number of features and the data is high-dimensional. However, it may not be suitable for all types of data and may require careful parameter tuning to achieve good performance.
"
Metrics,Classification,Definition of Logit,"The logit function is mathematically defined as the logarithm of the odds of the probability p of a certain event occurring:
Logit(p) = log(p / (1 - p))
Here, p represents the probability of the event, and log denotes the natural logarithm. The odds are the ratio of the probability of the event to the probability of the event not occurring. When p is the probability of success, 1 - p is the probability of failure, and the odds are a way of comparing the likelihood of these two outcomes.
Logit values can be interpreted in terms of odds. A logit of 0 indicates that the odds of the event occurring are equal to the odds of the event not occurring, or in other words, a probability of 0.5. Positive logit values indicate probabilities greater than 0.5, and thus odds favoring the event, while negative values indicate probabilities less than 0.5, with odds against the event.
One of the advantages of using the logit function in logistic regression is that the coefficients β1, β2, ..., βk can be interpreted in terms of the log-odds. Specifically, a one-unit change in an independent variable Xi is associated with a change in the log-odds of the dependent event by βi units, holding all other variables constant.
"
Metrics,Classification,Confusion Matrix,"Confusion Matrix as the name suggests gives us a matrix as output and describes the complete performance of the model.
There are 4 important terms :
True Positives : The cases in which we predicted YES and the actual output was also YES.
True Negatives : The cases in which we predicted NO and the actual output was NO.
False Positives : The cases in which we predicted YES and the actual output was NO.
False Negatives : The cases in which we predicted NO and the actual output was YES.
Accuracy for the matrix can be calculated by taking average of the values lying across the “main diagonal” 
Confusion Matrix forms the basis for the other types of metrics.
"
Metrics,Classification,"Precision, Recall, F1 Score","Precision = TP / (TP + FP): It is the number of correct positive results divided by the number of positive results predicted by the classifier.
Recall = TP/(TP + FN) : It is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive).
F1 Score is used to measure a test’s accuracy
F1 Score is the Harmonic Mean between precision and recall. The range for F1 Score is [0, 1]. It tells you how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances).
High precision but lower recall, gives you an extremely accurate, but it then misses a large number of instances that are difficult to classify. The greater the F1 Score, the better is the performance of our model. Mathematically, it can be expressed as :
F1 Score tries to find the balance between precision and recall.
"
Metrics,Classification,Sensitivity and specificity,"Sensitivity (true positive rate) is the probability of a positive test result, conditioned on the individual truly being positive.
Specificity (true negative rate) is the probability of a negative test result, conditioned on the individual truly being negative.
Sensitivity =TP/(TP + FN) = TP/P = Recall
Specificity =TN/(TN + FP) = TN/N
Recall and specificity do not change because of class imbalance, as these metrics are class-balance insensitive.
"
Metrics,Classification,Area Under Curve,"Area Under Curve (AUC) is one of the most widely used 	metrics for evaluation. It is used for binary classification problem. AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example. Before defining AUC, let us understand two basic terms :
True Positive Rate (TPR, Sensitivity) : True Positive Rate is defined as TP/ (FN+TP). True Positive Rate corresponds to the proportion of positive data points that are correctly considered as positive, with respect to all positive data points.
True Negative Rate (TNR, Specificity) : True Negative Rate is defined as TN / (FP+TN). False Positive Rate corresponds to the proportion of negative data points that are correctly considered as negative, with respect to all negative data points.
False Positive Rate (FPR): False Positive Rate is defined as FP / (FP+TN). False Positive Rate corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points.
False Positive Rate and True Positive Rate both have values in the range [0, 1]. FPR and TPR both are computed at varying threshold values such as (0.00, 0.02, 0.04, …., 1.00) and a graph is drawn. AUC is the area under the curve of plot False Positive Rate vs True Positive Rate at different points in [0, 1]. 
As evident, AUC has a range of [0, 1]. The greater the value, the better is the performance of our model.
"
Metrics,Classification,AUC using Concordance and Tied Percent,"Calculate the predicted probability in logistic regression (or any other binary classification model). It is not restricted to logistic regression.
Divide the data into two datasets. One dataset contains observations having actual value of dependent variable with value 1 (i.e. event) and corresponding predicted probability values. And the other dataset contains observations having actual value of dependent variable 0 (non-event) against their predicted probability scores.
Compare each predicted value in first dataset with each predicted value in second dataset.
Total Number of pairs to compare = x * y
x : Number of observations in first dataset (actual values of 1 in dependent variable)
y : Number of observations in second dataset (actual values of 0 in dependent variable).

In this step, we are performing cartesian product (cross join) of events and non-events. For example, you have 100 events and 1000 non-events. It would create 100k (100*1000) pairs for comparison.
A pair is concordant if 1 (observation with the desired outcome i.e. event) has a higher predicted probability than 0 (observation without the outcome i.e. non-event).
A pair is discordant if 0 (observation without the desired outcome i.e. non-event) has a higher predicted probability than 1 (observation with the outcome i.e. event).
A pair is tied if 1 (observation with the desired outcome i.e. event) has same predicted probability than 0 (observation without the outcome i.e. non-event).
The final percent values are calculated using the formula below -
Percent Concordant = 100*[(Number of concordant pairs)/Total number of pairs]
Percent Discordant = 100*[(Number of discordant pairs)/Total number of pairs]
Percent Tied = 100*[(Number of tied pairs)/Total number of pairs]
Area under curve (AUC) = (Percent Concordant + 0.5 * Percent Tied)/100
"
Metrics,Classification,Fβ score,"The adjusted F-score allows us to weight precision or recall more highly if it is more important for our use case. Its formula is slightly different:
Fβ=(β^2+1) Recall⋅Precision / (Recall+β^2⋅Precision)
A factor indicating how much more important recall is than precision. For example, if we consider recall to be twice as important as precision, we can set β to 2. The standard F-score is equivalent to setting β to one.
"
Metrics,Classification,"Explain Macro Average, Weighted Average and Micro Average","Macro averaging is perhaps the most straightforward among the numerous averaging methods.
The macro-averaged F1 score (or macro F1 score) is computed using the arithmetic mean (aka unweighted mean) of all the per-class F1 scores.
This method treats all classes equally regardless of their support values.
Weighted Average
The weighted-averaged F1 score is calculated by taking the mean of all per-class F1 scores while considering each class’s support.
Support refers to the number of actual occurrences of the class in the dataset. For example, the support value of 1 in Boat means that there is only one observation with an actual label of Boat.
The ‘weight’ essentially refers to the proportion of each class’s support relative to the sum of all support values.
With weighted averaging, the output average would have accounted for the contribution of each class as weighted by the number of examples of that given class.
Micro Average
Micro averaging computes a global average F1 score by counting the sums of the True Positives (TP), False Negatives (FN), and False Positives (FP).
We first sum the respective TP, FP, and FN values across all classes and then plug them into the F1 equation to get our micro F1 score.
In the classification report, you might be wondering why our micro F1 score of 0.60 is displayed as ‘accuracy’ and why there is NO row stating ‘micro avg’.
This is because micro-averaging essentially computes the proportion of correctly classified observations out of all observations. If we think about this, this definition is what we use to calculate overall accuracy.
micro-F1 = accuracy = micro-precision = micro-recall
"
Metrics,Classification,Which average should I choose?,"In general, if you are working with an imbalanced dataset where all classes are equally important, using the macro average would be a good choice as it treats all classes equally.
It means that for our example involving the classification of airplanes, boats, and cars, we would use the macro-F1 score.
If you have an imbalanced dataset but want to assign greater contribution to classes with more examples in the dataset, then the weighted average is preferred.
This is because, in weighted averaging, the contribution of each class to the F1 average is weighted by its size.
Suppose you have a balanced dataset and want an easily understandable metric for overall performance regardless of the class. In that case, you can go with accuracy, which is essentially our micro F1 score.
"
Metrics,Classification,Why F1-Score is a Harmonic Mean(HM) of Precision and Recall?,"If Precision = 0, Recall = 1, their average is 0.5 and F1 is 0.
"
Metrics,Classification,What is Average Precision?,"Average precision is the area under the PR curve.
AP summarizes the PR Curve to one scalar value. Average precision is high when both precision and recall are high, and low when either of them is low across a range of confidence threshold values. The range for AP is between 0 to 1.
"
Metrics,Classification,What is Mean Average Precision (mAP)?,"Mean Average Precision(mAP) is a metric used to evaluate object detection models such as Fast R-CNN, YOLO, Mask R-CNN, etc. The mean of average precision(AP) values are calculated over recall values from 0 to 1.
Average Precision is calculated as the weighted mean of precisions at each threshold; the weight is the increase in recall from the prior threshold.
Mean Average Precision is the average of AP of each class. However, the interpretation of AP and mAP varies in different contexts. For instance, in the evaluation document of the COCO object detection challenge, AP and mAP are the same.
The mAP is calculated by finding Average Precision(AP) for each class and then average over a number of classes.
The mAP incorporates the trade-off between precision and recall and considers both false positives (FP) and false negatives (FN). This property makes mAP a suitable metric for most detection applications.
"
Metrics,Classification,Mean Average Precision for Object Detection,"Object Detection is a well-known computer vision problem where models seek to localize the relevant objects in images and classify those objects into relevant classes. The mAP is used as a standard metric to analyze the accuracy of an object detection model.
Let us walk through an object detection example for mAP calculation.
Consider the below image of cars driving on the highway, and the model’s task is to detect the cars. The output of the model is shown as red boxes. The model gave seven detections from P1 to P7, and the IoU values are calculated w.r.t. ground truth.
For object detection tasks, precision is calculated based on the IoU threshold. The precision value differs based w.r.t IoU threshold. 
If IoU threshold = 0.8 then precision is 66.67%. (4 out of 6 are considered correct)
If IoU threshold = 0.5 then precision is 83.33%. (5 out of 6 are considered correct)
If IoU threshold = 0.2 then precision is 100%.    (6 out of 6 are considered correct)
This shows that the AP metric is dependent on the IoU threshold. Choosing the IoU threshold becomes an arbitrary process for the researcher as it needs to be carefully chosen for each task as the model's accuracy expectation may vary. Hence, to avoid this ambiguity while evaluating an object detection model, the mean average precision(mAP) came into existence.
Calculate AP across a set of IoU thresholds for each class k and then take the average of all AP values. This eliminates the necessity of picking an optimal IoU threshold by using a set of IoU thresholds that covers tail ends of precision and recall values.
According to the COCO 2017 challenge evaluation guidelines, the mAP was calculated by averaging the AP over 80 object classes AND all 10 IoU thresholds from 0.5 to 0.95 with a step size of 0.05.
The primary challenge metric in COCO 2017 challenge is calculated as follows:
AP is calculated for the IoU threshold of 0.5 for each class.
Calculate the precision at every recall value(0 to 1 with a step size of 0.01), then it is repeated for IoU thresholds of 0.55,0.60,…,.95.
Average is taken over all the 80 classes and all the 10 thresholds.
"
Metrics,Classification,Explain ROC curve,"A ROC curve is a plot of the true positive rate (Sensitivity) in function of the false positive rate (100-Specificity) for different cut-off points of a parameter. Each point on the ROC curve represents a sensitivity/specificity pair corresponding to a particular decision threshold. The Area Under the ROC curve (AUC) is a measure of how well a parameter can distinguish between two diagnostic groups (diseased/normal).
Precision-Recall Curve 
Precision-Recall curve is obtained by plotting the model's precision and recall values as a function of the model's confidence score threshold.
“When a model has high recall but low precision, then the model classifies most of the positive samples correctly but it has many false positives(i.e. classifies many Negative samples as Positive).“
“When a model has high precision but low recall, then the model is accurate when it classifies a sample as Positive but it may classify only some of the positive samples.”
The precision-recall curve encapsulates the tradeoff of both metrics and maximizes the effect of both metrics. It gives us a better idea of the overall accuracy of the model.
Based on the problem at hand, the model with an element of confidence score threshold can tradeoff precision for recall and vice versa. For instance, if you are dealing with a cancer tumor detection problem, avoiding false negatives is a higher priority than avoiding false positives.
We should avoid missing tumor detection at the cost of detecting more tumors with less accuracy. Lowering the confidence score threshold will encourage the model to output more predictions (high recall) at the expense of lowering correct predictions(lower precision).
The precision-recall is downward sloping because as the confidence score is decreased, more predictions are made (increasing recall), and fewer correct predictions are made (lowering precision). 
Over the years, AI researchers have tried to combine precision and recall into a single metric to compare models. There are a couple of metrics that are widely used:
F1 Score—It finds the most optimal confidence score threshold where precision and recall give the highest F1 score. The F1 score calculates the balance between precision and recall. If the F1 score is high, precision and recall are high, and vice versa.
AUC (Area Under the Curve) covers the area underneath the precision-recall curve.
The Area Under Curve for precision-recall (PR-AUC) curve summarizes the PR values for different thresholds under a single metric. 
The above image clearly shows how precision and recall values are incorporated in each metric: F1, Area Under Curve(AUC), and Average Precision(AP). The consideration of accuracy metric heavily depends on the type of problem.
AUC and AP are considered superior metrics compared to the F1 score because of the overall area coverage. For interpretability purposes, the researchers use AP as a standard metric.
"
Metrics,Classification,In which cases AU PR is better than AU ROC? ,"AU ROC looks at a true positive rate TPR and false positive rate FPR while AU PR looks at positive predictive value PPV and true positive rate TPR.
Typically, if true negatives are not meaningful to the problem or you care more about the positive class, AU PR is typically going to be more useful; otherwise, If you care equally about the positive and negative class or your dataset is quite balanced, then going with AU ROC is a good idea.
ROC curves should be used when there are roughly equal numbers of observations for each class.
Precision-Recall curves should be used when there is a moderate to large class imbalance.
"
Metrics,Classification,Explain Index of Union (IU),"Perkins and Schisterman [4] stated that the “optimal” cut-point should be chosen as the point which classifies most of the individuals correctly and thus least of them incorrectly. From this point of view, in this study, the Index of Union method is proposed. This method provides an “optimal” cut-point which has maximum sensitivity and specificity values at the same time. In order to find the highest sensitivity and specificity values at the same time, the AUC value is taken as the starting value of them. For example, let AUC value be 0.8. The next step is to look for a cut-point from the coordinates of ROC whose sensitivity and specificity values are simultaneously so close or equal to 0.8. This cut-point is then defined as the “optimal” cut-point. The above criteria correspond to the following equation:The cut-point , which minimizes the  function and the  difference, will be the “optimal” cut-point value.
In other words, the cut-point cˆIU defined by the IU method should satisfy two conditions: (1) sensitivity and specificity obtained at this cut-point should be simultaneously close to the AUC value; (2) the difference between sensitivity and specificity obtained at this cut-point should be minimum. The second condition is not compulsory, but it is an essential condition when multiple cut-points satisfy the equation.
"
Metrics,Classification,Brier score,"The Brier score is a strictly proper scoring rule that measures the accuracy of probabilistic predictions. For unidimensional predictions, it is strictly equivalent to the mean squared error as applied to predicted probabilities.
BS = 1/N∑(ft -ot)^2

in which ftft is the probability that was forecast, otot the actual outcome of the event at instance tt (00 if it does not happen and 11 if it does happen) and NN is the number of forecasting instances. In effect, it is the mean squared error of the forecast. This formulation is mostly used for binary events (for example ""rain"" or ""no rain""). The above equation is a proper scoring rule only for binary events; if a multi-category forecast is to be evaluated, then the original definition given by Brier below should be used.
"
Metrics,Classification,List Classification Losses,"Hinge Loss/Multi class SVM Loss
In simple terms, the score of correct category should be greater than sum of scores of all incorrect categories by some safety margin (usually one). And hence hinge loss is used for maximum-margin classification, most notably for support vector machines. Although not differentiable, it’s a convex function which makes it easy to work with usual convex optimizers used in machine learning domain.
Cross Entropy Loss/Negative Log Likelihood
This is the most common setting for classification problems. Cross-entropy loss increases as the predicted probability diverges from the actual label.
"
Metrics,Regression,List Regression Losses,"Mean Square Error/Quadratic Loss/L2 Loss
As the name suggests, Mean square error is measured as the average of squared difference between predictions and actual observations. It’s only concerned with the average magnitude of error irrespective of their direction. However, due to squaring, predictions which are far away from actual values are penalized heavily in comparison to less deviated predictions. Plus MSE has nice mathematical properties which makes it easier to calculate gradients.
Mean Absolute Error/L1 Loss
Mean absolute error, on the other hand, is measured as the average of sum of absolute differences between predictions and actual observations. Like MSE, this as well measures the magnitude of error without considering their direction. Unlike MSE, MAE needs more complicated tools such as linear programming to compute the gradients. Plus MAE is more robust to outliers since it does not make use of square.
Mean Bias Error
This is much less common in machine learning domain as compared to it’s counterpart. This is same as MSE with the only difference that we don’t take absolute values. Clearly there’s a need for caution as positive and negative errors could cancel each other out. Although less accurate in practice, it could determine if the model has positive bias or negative bias.
"
Metrics,Regression,Mean Absolute Percentage Error,"MAPE is one of the most common methods to measure forecast accuracy. It means Mean Absolute Percentage Error and it measures the percentage error of the forecast in relation to the actual values. As it calculates the average error over time or different products, it doesn’t differentiate between them. This means that it assumes no preference between what day or what product to predict better. It is calculated as follows:
"
Metrics,Regression,Weighted Average Percentage Error,"WAPE, also referred to as the MAD/Mean ratio, means Weighted Average Percentage Error. It weights the error by adding the total sales:
"
Metrics,Regression,Root Mean Squared Log Error,"Root Mean Squared Logarithmic Error is calculated by applying log to the actual and the predicted values and then taking their differences. RMSLE is robust to outliers where the small and the large errors are treated evenly.
It penalizes the model more if the predicted value is less than the actual value while the model is less penalized if the predicted value is more than the actual value. It does not penalize high errors due to the log. Hence the model has a larger penalty for underestimation than overestimation. This can be helpful in situations where we are not bothered by overestimation but underestimation is not acceptable.
RMSLE = √1/n∑ (log(pi+1)−log(ai+1))^2
Where:
n is the total number of observations in the (public/private) data set,
pi is your prediction of target, and
ai is the actual target for i
"
Metrics,Regression,Can I use my metric as the loss function?,"Not always!
Some algorithms require the loss function to be differentiable and some metrics, such as accuracy or any other step function, are not.
These algorithms usually use some form of gradient descent to update the parameters. This is the case for neural network weight stepping, where the partial derivative of the loss with respect to each weight is calculated.
Let’s illustrate this by using accuracy on a classification problem where the model assigns a probability to each mutually exclusive class for a given input.
In this case, a small change to a parameter’s weight may not change the outcome of our predictions but only our confidence in them, meaning the accuracy remains the same. The partial derivative of the loss with respect to this parameter would be 0 (infinity at the threshold) most of the time, preventing our model from learning (a step of 0 would keep the weight and model as is).
In other words, we want the small changes made to the parameter weights to be reflected in the loss function.
Some algorithms don’t require their function to be differentiable but would not work with some functions by their nature. 
Some objective functions are easier to optimize than others. We might want to use a proxy easy loss function instead of a hard one.
We often choose to optimize smooth and convex loss functions because:
They are differentiable anywhere.
A minimum is always a global minimum.
Using gradient descent on such function will lead you surely towards the global minima and not get stuck in a local mimimum or saddle point.
There are plenty of ressources about convex functions on the internet. I’ll share one with you. I personally didn’t get all of it but maybe you will.
Some algorithms seem to empirically work well with non-convex functions. This is the case of Deep Learning for example, where we often use gradient descent on a non-convex loss function.
Another thing you need to be careful of is that different loss functions bring different assumptions to the model. For example, the logistic regression loss assumes a Bernoulli distribution.
"
Metrics,Regression,Distance Metrics,"Properties of Distance Metrics
Symmetry
Non-negativity 
Triangle Inequality 
Metrics:
Euclidean Distance
Manhattan Distance
Minkowski Distance
D(t1,t2)=(∑i=1_N|ti1−ti2|^p)^1/p
This metric provides different interpretations depending on the chosen value of p. For example, by setting p=1, it is referred to as the Manhattan distance, which sums up the absolute differences between observations. Similarly, the Euclidean distance is obtained when p=2. Finally, by setting p=∞, the infinite norm, which computes the largest differences among all pairs of observations, is provided.
Cosine Similarity
There are various ways to measure similarity between sets of data, with Euclidean distance being another commonly used metric. While Euclidean distance focuses on the straight-line distance between two points in space, cosine similarity focuses on the angle between two vectors. This makes cosine similarity more robust in capturing the pattern similarities between two sets of data, even if their magnitudes differ.
Jaccard Distance
The Jaccard coefficient measures similarity between finite sample sets and is defined as the size of the intersection divided by the size of the union of the sample sets:
"
Metrics,Regression,"What are some differences you would expect in a model that minimizes squared error, versus a model that minimizes absolute error? In which cases would each error metric be appropriate?","MSE is more strict to having outliers. MAE is more robust in that sense, but is harder to fit the model for because it cannot be numerically optimized. So when there are less variability in the model and the model is computationally easy to fit, we should use MAE, and if that’s not the case, we should use MSE.
MSE: easier to compute the gradient, MAE: linear programming needed to compute the gradient
MAE more robust to outliers. If the consequences of large errors are great, use MSE
MSE corresponds to maximizing likelihood of Gaussian random variables
"
Metrics,Regression,Why does minimizing the MAE lead to forecasting the median?,"Let’s take a derivative of
1/N∑∣c−yi∣→min
∇cL(f,X,y) = 1/N∑sign(c−yi)=0
#{i∣yi<c}−#{i∣yi>c}=0
Therefore, number of yi less than c must be equal to number of yi greater than c – it’s a median of (y1…yn).
"
Metrics,Regression,Least-Squares Loss,"min θ∈R^D 1/N * ∥y − Xθ∥ 2
"
Metrics,Regression,Formula for R-Squared,"R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. 
goodness of fit measure. variance explained by the regression / total variance
the more predictors you add the higher R^2 becomes.
"
Metrics,Regression,What Is a Variance Inflation Factor (VIF)? ,"A variance inflation factor (VIF) is a measure of the amount of multicollinearity in regression analysis. Multicollinearity exists when there is a correlation between multiple independent variables in a multiple regression model. This can adversely affect the regression results. Thus, the variance inflation factor can estimate how much the variance of a regression coefficient is inflated due to multicollinearity. 
  Detecting multicollinearity is important because while multicollinearity does not reduce the explanatory power of the model, it does reduce the statistical significance of the independent variables. 
  A large VIF on an independent variable indicates a highly collinear relationship to the other variables that should be considered or adjusted for in the structure of the model and selection of independent variables.
The formula for VIF is:
VIF=1/(1−Ri2)
where:Ri2=Unadjusted coefficient of determination forregressing the ith independent variable on theremaining ones
A rule of thumb for interpreting the variance inflation factor:
1 = not correlated.
Between 1 and 5 = moderately correlated.
Greater than 5 = highly correlated.
"
Metrics,Regression,Explain Huber Loss,"Huber loss, also known as smooth L1 loss, is a loss function commonly used in regression problems, particularly in machine learning tasks involving regression tasks. It is a modified version of the Mean Absolute Error (MAE) and Mean Squared Error (MSE) loss functions, which combines the best properties of both.
Below are some advantages of Huber Loss –
Robustness to outliers
Differentiability
The balance between L1 and L2 loss
Smoother optimization landscape
Efficient optimization
User-defined threshold
Wide applicability
While there are also some disadvantages of using this loss function –
Hyperparameter tuning
Task-specific performance
Less emphasis on smaller errors
"
Metrics,Regression,Prediction Intervals in Forecasting: Quantile Loss Function,"In most real world prediction problems, the uncertainty in our predictions provides significant value. Knowing about the range of predictions as opposed to only point estimates can significantly improve decision making processes for many commercial applications.
A prediction interval is a quantification of the uncertainty on a prediction. It provides a probabilistic upper and lower bounds on the estimate of an outcome variable.
Although, most model outputs are accurate and close to the observed value, the outputs are themselves random variables, and thus have a distribution. Prediction intervals are necessary to get an idea about the likeliness of the correctness of our results. This likeliness determines an interval of possible values.
Machine learning models work by minimizing (or maximizing) an objective function. An objective function translates the problem we are trying to solve into a mathematical formula to be minimized by the model. As the name suggests, the quantile regression loss function is applied to predict quantiles. A quantile is the value below which a fraction of observations in a group falls. For example, a prediction for quantile 0.9 should over-predict 90% of the times.
Given a prediction yi^p and outcome yi, the mean regression loss for a quantile q is
For a set of predictions, the loss will be its average.
Intuitive Understanding
In the regression loss equation above, as q has a value between 0 and 1, the first term will be positive and dominate when over predicting, yip > yi, and the second term will dominate when under-predicting, yip < yi. For q equal to 0.5, under-prediction and over-prediction will be penalized by the same factor, and the median is obtained. The larger the value of q, the more over-predictions are penalized compared to under-predictions. For q equal to 0.75, over-predictions will be penalized by a factor of 0.75, and under-predictions by a factor of 0.25. The model will then try to avoid over-predictions approximately three times as hard as under-predictions, and the 0.75 quantile will be obtained.
Why use Quantile Loss?
Prediction interval from least square regression is based on an assumption that residuals (y — y_hat) have constant variance across values of independent variables. We can not trust linear regression models which violate this assumption. We can not also just throw away the idea of fitting linear regression model as baseline by saying that such situations would always be better modeled using non-linear functions or tree based models. This is where quantile loss and quantile regression come to rescue as regression based on quantile loss provides sensible prediction intervals even for residuals with non-constant variance or non-normal distribution.
"
Metrics,Regression,What Loss Function and Activation to Use?,"Regression Problem
A problem where you predict a real-value quantity.
Output Layer Configuration: One node with a linear activation unit.
Loss Function: Mean Squared Error (MSE).
Binary Classification Problem
A problem where you classify an example as belonging to one of two classes.
The problem is framed as predicting the likelihood of an example belonging to class one, e.g. the class that you assign the integer value 1, whereas the other class is assigned the 0value 0.
Output Layer Configuration: One node with a sigmoid activation unit.
Loss Function: Cross-Entropy, also referred to as Logarithmic loss.
Multi-Class Classification Problem
A problem where you classify an example as belonging to one of more than two classes.
The problem is framed as predicting the likelihood of an example belonging to each class.
Output Layer Configuration: One node for each class using the softmax activation function.
Loss Function: Cross-Entropy, also referred to as Logarithmic loss.
Categorical Cross-Entropy loss or Softmax Loss is a Softmax activation plus a Cross-Entropy loss. If we use this loss, we will train a CNN to output a probability over the C classes for each image. It is used for multi-class classification.
Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss. Unlike Softmax loss it is independent for each vector component (class), meaning that the loss computed for every CNN output vector component is not affected by other component values. That’s why it is used for multi-label classification, where the insight of an element belonging to a certain class should not influence the decision for another class.
"
Metrics,Decision Trees,Gini impurity,"The gini impurity is calculated using the following formula:
GiniIndex=1–∑jpj^2 = ∑pj(1 – pj)
Where pj is the probability of class j.
The gini impurity measures the frequency at which any element of the dataset will be mislabelled when it is randomly labeled.
The minimum value of the Gini Index is 0. This happens when the node is pure, this means that all the contained elements in the node are of one unique class. Therefore, this node will not be split again. Thus, the optimum split is chosen by the features with less Gini Index. Moreover, it gets the maximum value when the probability of the two classes are the same.
"
Metrics,Decision Trees,Entropy,"The entropy is calculated using the following formula:
Entropy=–∑jpj⋅log2pj
Where, as before, pj is the probability of class j.
Entropy is a measure of information that indicates the disorder of the features with the target. Similar to the Gini Index, the optimum split is chosen by the feature with less entropy. It gets its maximum value when the probability of the two classes is the same and a node is pure when the entropy has its minimum value, which is 0:
Entropy_min=−1⋅log2(1)=0
Entropy_max=–0.5⋅log2(0.5)–0.5⋅log2(0.5)=log(2)
"
Metrics,Decision Trees,Gini vs Entropy,"The Gini Index and the Entropy have two main differences:
Gini is the probability of misclassifying a randomly chosen element in a set while entropy measures the amount of uncertainty or randomness in a set.
The range of the Gini index is [0, 0.5], where 0 indicates perfect purity and 0.5 indicates maximum impurity. The range of entropy is [0, 1].
Gini index is a linear measure.	Entropy is a logarithmic measure.
Gini can be interpreted as the expected error rate in a classifier. Entropy can be interpreted as the average amount of information needed to specify the class of an instance.
Gini is sensitive to the distribution of classes in a set. Entropy is sensitive to the number of classes.
The computational complexity of the Gini index is O(c). Computational complexity of entropy is O(c * log(c)).
Entropy is more robust than Gini index and comparatively less sensitive.
Formula for the Gini index is Gini(P) = 1 – ∑(Px)^2 , where Pi is the proportion of the instances of class x in a set. Formula for entropy is Entropy(P) = -∑(Px)log(Px), where pi is the proportion of the instances of class x in a set.
Gini  has a bias toward selecting splits that result in a more balanced distribution of classes. Entropy has a bias toward selecting splits that result in a higher reduction of uncertainty.
Gini index is typically used in CART (Classification and Regression Trees) algorithms. Entropy is typically used in ID3 and C4.5 algorithms
"
Metrics,Decision Trees,Mean Decrease in Impurity,"Mean decrease in impurity is a metric used to evaluate the importance of a feature in decision tree algorithms, calculated as the average reduction in impurity brought by a feature across all trees in the model. This measure helps in understanding how well a feature can split the data into distinct classes, contributing to better model interpretation and explainability. The lower the impurity after a split, the more informative that feature is considered for making decisions.
"
Metrics,Decision Trees,Permutation feature importance,"Permutation feature importance is a model inspection technique that measures the contribution of each feature to a fitted model’s statistical performance on a given tabular dataset. This technique is particularly useful for non-linear or opaque estimators, and involves randomly shuffling the values of a single feature and observing the resulting degradation of the model’s score [1]. By breaking the relationship between the feature and the target, we determine how much the model relies on such particular feature.
Inputs: fitted predictive model m, tabular dataset (training or validation) D.
Compute the reference score s of the model m on data D (for instance the accuracy for a classifier or the R2 for a regressor).
For each feature j (column of D):
For each repetition k in 1,...,K:
Randomly shuffle column j of dataset D to generate a corrupted version of the data named D~k,j.
Compute the score sk,j of model m on corrupted data D~k,j.
Compute importance ij for feature fj defined as:
"
Metrics,Clustering,Accuracy metrics for Clustering,"As opposed to classification, it is difficult to assess the quality of results from clustering. Here, a metric cannot depend on the labels but only on the goodness of split. Secondly, we do not usually have true labels of the observations when we use clustering.
There are internal and external goodness metrics. External metrics use the information about the known true split while internal metrics do not use any external information and assess the goodness of clusters based only on the initial data. The optimal number of clusters is usually defined with respect to some internal metrics.
The following metrics are used:
Adjusted Rand Index
Adjusted Mutual Information
Homogeneity
Completeness
V-measure
Silhouette
"
Metrics,Clustering,Adjusted Rand Index (ARI),"Here, we assume that the true labels of objects are known. This metric does not depend on the labels’ values but on the data cluster split. Let N be the number of observations in a sample. Let a to be the number of observation pairs with the same labels and located in the same cluster, and let b to be the number of observation pairs with different labels and located in different clusters. The Rand Index can be calculated using the following formula:
RI=2(a+b) / n(n−1).
In other words, it evaluates a share of observation pairs for which these splits (initial and clustering result) are consistent. The Rand Index (RI) evaluates the similarity of the two splits of the same sample. In order for this index to be close to zero for any clustering outcomes with any n and number of clusters, it is essential to scale it, hence the Adjusted Rand Index:
ARI=(RI−E[RI]) / (max(RI)−E[RI]).
This metric is symmetric and does not depend in the label permutation. Therefore, this index is a measure of distances between different sample splits. ARI takes on values in the [−1,1] range. Negative values indicate the independence of splits, and positive values indicate that these splits are consistent (they match ARI=1).
"
Metrics,Clustering,Adjusted Mutual Information (AMI),"This metric is similar to ARI. It is also symmetric and does not depend on the labels’ values and permutation. It is defined by the entropy function and interprets a sample split as a discrete distribution (likelihood of assigning to a cluster is equal to the percent of objects in it). The MI index is defined as the mutual information for two distributions, corresponding to the sample split into clusters. Intuitively, the mutual information measures the share of information common for both clustering splits i.e. how information about one of them decreases the uncertainty of the other one.
Similarly to the ARI, the AMI is defined. This allows us to get rid of the MI index’s increase with the number of clusters. The AMI lies in the [0,1] range. Values close to zero mean the splits are independent, and those close to 1 mean they are similar (with complete match at AMI=1).
"
Metrics,Clustering,"Homogeneity, completeness, V-measure","Formally, these metrics are also defined based on the entropy function and the conditional entropy function, interpreting the sample splits as discrete distributions:
h=1−H(C∣K) / H(C)
c=1−H(K∣C) / H(K),
where K is a clustering result and C is the initial split. Therefore, h evaluates whether each cluster is composed of same class objects, and c measures how well the same class objects fit the clusters. These metrics are not symmetric. Both lie in the [0,1] range, and values closer to 1 indicate more accurate clustering results. These metrics’ values are not scaled as the ARI or AMI metrics are and thus depend on the number of clusters. A random clustering result will not have metrics’ values closer to zero when the number of clusters is big enough and the number of objects is small. In such a case, it would be more reasonable to use ARI. However, with a large number of observations (more than 100) and the number of clusters less than 10, this issue is less critical and can be ignored.
V-measure is a combination of h, and c and is their harmonic mean:
v=2hc / (h+c).
It is symmetric and measures how consistent two clustering results are.
"
Metrics,Clustering,Silhouette,"This coefficient does not imply the knowledge about the true labels of the objects. It lets us estimate the quality of the clustering using only the initial, unlabeled sample and the clustering result. To start with, for each observation, the silhouette coefficient is computed. Let a be the mean of the distance between an object and other objects within one cluster and b be the mean distance from an object to objects from the nearest cluster (different from the one the object belongs to). Then the silhouette measure for this object is
s=(b−a)/max(a,b).
The silhouette of a sample is a mean value of silhouette values from this sample. Therefore, the silhouette distance shows to which extent the distance between the objects of the same class differ from the mean distance between the objects from different clusters. This coefficient takes values in the [−1,1] range. Values close to -1 correspond to bad clustering results while values closer to 1 correspond to dense, well-defined clusters. Therefore, the higher the silhouette value is, the better the results from clustering.
With the help of silhouette, we can identify the optimal number of clusters k (if we don’t know it already from the data) by taking the number of clusters that maximizes the silhouette coefficient.
"
Metrics,Recommender Systems,MAP@K and MAR@K,"A recommender system typically produces an ordered list of recommendations for each user in the test set. MAP@K gives insight into how relevant the list of recommended items are, whereas MAR@K gives insight into how well the recommender is able to recall all the items the user has rated positively in the test set.
"
Metrics,Recommender Systems,Coverage,"Coverage is the percent of items in the training data the model is able to recommend on a test set. The random recommender has nearly 100% coverage as expected. Surprisingly, the collaborative filter is only able to recommend 8.42% of the items it was trained on.
"
Metrics,Recommender Systems,Diversity,"Recommendation diversity assesses how varied the recommended items are for each user. It reflects the breadth of item types or categories to which each user is exposed.
To compute this metric, you can measure the intra-list diversity by evaluating the average Cosine Distance between pairs of items inside the list. Then, you can average it across all users.   
Diversity is helpful if you expect users to have a better experience when they receive recommendations that span a diverse range of topics, genres, or characteristics. 
However, while diversity helps check if a system can show a varied mix of items, it does not consider relevance. You can use this metric with ranking or predictive metrics to get a complete picture.
"
Metrics,Recommender Systems,Novelty ,"Novelty assesses how unique the recommended items are to users. It measures the degree to which the suggested items differ from popular ones.  
You can compute novelty as the negative logarithm (base 2) of the probability of encountering a given item in a training set. High novelty corresponds to long-tail items that few users interacted with, and low novelty corresponds to popular items. Then, you can average the novelty inside the list and across users. 
Novelty reflects the system's ability to recommend items that are not well-known in the dataset. It is helpful for scenarios when you expect users to get new and unusual recommendations to stay engaged.
"
Metrics,Information Retrieval,Metrics in Information Retrieval,"Evaluation measures for IR systems can be split into two categories: online or offline metrics.
Online metrics are captured during actual usage of the IR system when it is online. These consider user interactions like whether a user clicked on a recommended show from Netflix or if a particular link was clicked from an email advertisement (the click-through rate or CTR). There are many online metrics, but they all relate to some form of user interaction.
Offline metrics are measured in an isolated environment before deploying a new IR system. These look at whether a particular set of relevant results are returned when retrieving items with the system.
Organizations often use both offline and online metrics to measure the performance of their IR systems. It begins, however, with offline metrics to predict the system’s performance before deployment.
We will focus on the most useful and popular offline metrics:
Recall@K
Mean Reciprocal Rank (MRR)
Mean Average Precision@K (MAP@K)
Normalized Discounted Cumulative Gain (NDCG@K)
These metrics are deceptively simple yet provide invaluable insight into the performance of IR systems.
We have two more subdivisions for these metrics; order-aware and order-unaware. This refers to whether the order of results impacts the metric score. If so, the metric is order-aware. Otherwise, it is order-unaware.
"
Metrics,Information Retrieval,Recall@K,"Recall@K is one of the most interpretable and popular offline evaluation metrics. It measures how many relevant items were returned against how many relevant items exist in the entire dataset. 
Recall@K=truePositives/(truePositives+falseNegatives)
The K in this and all other offline metrics refers to the number of items returned by the IR system. In our example, we have a total number of N = 8 items in the entire dataset, so K can be any value between [1,...,N][1,...,N].
Pros and Cons
Recall@K is undoubtedly one of the most easily interpretable evaluation metrics. We know that a perfect score indicates that all relevant items are being returned. We also know that a smaller k value makes it harder for the IR system to score well with recall@K.
Still, there are disadvantages to using recall@K. By increasing K to N or near N, we can return a perfect score every time, so relying solely on recall@K can be deceptive.
Another problem is that it is an order-unaware metric. That means if we used recall@4 and returned one relevant result at rank one, we would score the same as if we returned the same result at rank four. Clearly, it is better to return the actual relevant result at a higher rank, but recall@K cannot account for this.
"
Metrics,Information Retrieval,Mean Reciprocal Rank (MRR),"The Mean Reciprocal Rank (MRR) is an order-aware metric, which means that, unlike recall@K, returning an actual relevant result at rank one scores better than at rank four.
Another differentiator for MRR is that it is calculated based on multiple queries. It is calculated as:
RR=1/Q*∑1/rank_q
Q is the number of queries, q a specific query, and rank_q the rank of the first *actual relevant* result for query q. 
Pros and Cons
MRR has its own unique set of advantages and disadvantages. It is order-aware, a massive advantage for use cases where the rank of the first relevant result is important, like chatbots or question-answering.
On the other hand, we consider the rank of the first relevant item, but no others. That means for use cases where we’d like to return multiple items like recommendation or search engines, MRR is not a good metric. For example, if we’d like to recommend ~10 products to a user, we ask the IR system to retrieve 10 items. We could return just one actual relevant item in rank one and no other relevant items. Nine of ten irrelevant items is a terrible result, but MRR would score a perfect 1.0.
Another minor disadvantage is that MRR is less readily interpretable compared to a simpler metric like recall@K. However, it is still more interpretable than many other evaluation metrics.
"
Metrics,Information Retrieval,Mean Average Precision (MAP),"Mean Average Precision@K (MAP@K) is another popular order-aware metric.
There are a few steps to calculating MAP@K. We start with another metric called precision@K:
Precision@K=truePositives/(truePositives+falsePositives)
Note that the denominator in precision@K always equals KK. Now that we have the precision@K value, we move on to the next step of calculating the Average Precision@K (AP@K):
AP@K=∑(Precision@k∗relevance_k)/(number of relevant results)
Pros and Cons
MAP@K is a simple offline metric that allows us to consider the order of returned items. Making this ideal for use cases where we expect to return multiple relevant items.
The primary disadvantage of MAP@K is the relKrelK relevance parameter is binary. We must either view items as relevant or irrelevant. It does not allow for items to be slightly more/less relevant than others
"
Metrics,Information Retrieval,Normalized Discounted Cumulative Gain (NDCG@K),"Normalized Discounted Cumulative Gain @K (NDCG@KNDCG@K) is another order-aware metric that we can derive from a few simpler metrics. Starting with Cumulative Gain calculated like so:
CG@K=∑relevance_k
The relevance_k variable is a range of relevance ranks where *0* is the least relevant, and some higher value is the most relevant.
To handle this lack of order awareness, we modify the metric to create DCG@K, adding a penalty in the form of log2(1+k) to the formula:
DCG@2=∑relevance_k / log2(1+k)
Using the order-aware DCG@KDCG@K metric means the preferred swapped results returns a better score.
Unfortunately, DCG@K scores are very hard to interpret as their range depends on the variable relkrelk range we chose for our data. We use the Normalized DCG@K (NDCG@K) metric to fix this.
NDCG@K is a special modification of standard NDCG that cuts off any results whose rank is greater than K. This modification is prevalent in use-cases measuring search performance.
NDCG@K normalizes DCG@K using the Ideal DCG@K (IDCG@K) rankings. For IDCG@K, we assume that the most relevant items are ranked highest and in order of relevance.
Pros and Cons
NDCG@K is one of the most popular offline metrics for evaluating IR systems, in particular web search engines. That is because NDCG@K optimizes for highly relevant documents, is order-aware, and is easily interpretable.
However, there is a significant disadvantage to NDCG@K. Not only do we need to know which items are relevant for a particular query, but we need to know whether each item is more/less relevant than other items; the data requirements are more complex.
"
Metrics,Digital Metrics,Digital Metrics,"Different kinds of metrics can be used to measure a website efficacy. With discrete metrics, also called binomial metrics, only the two values 0 and 1 are possible. The following are examples of popular discrete metrics.
Click-through rate — if a user is shown an advertisement, do they click on it?
Conversion rate — if a user is shown an advertisement, do they convert into customers?
Bounce rate — if a user is visits a website, is the following visited page on the same website?
With continuous metrics, also called non-binomial metrics, the metric may take continuous values that are not limited to a set two discrete states. The following are examples of popular continuous metrics.
Average revenue per user — how much revenue does a user generate in a month?
Average session duration — for how long does a user stay on a website in a session?
Average order value — what is the total value of the order of a user?
"
Metrics,Digital Metrics,"Define the terms KPI, lift, model fitting, robustness and DOE. ","KPI: KPI stands for Key Performance Indicator that measures how well the business achieves its objectives.
Lift: This is a performance measure of the target model measured against a random choice model. Lift indicates how good the model is at prediction versus if there was no model.
Model fitting: This indicates how well the model under consideration fits given observations.
Robustness: This represents the system’s capability to handle differences and variances effectively.
DOE: stands for the design of experiments, which represents the task design aiming to describe and explain information variation under hypothesized conditions to reflect variables.
Design of experiments (DOE) is a systematic, efficient method that enables scientists and engineers to study the relationship between multiple input variables (aka factors) and key output variables (aka responses). It is a structured approach for collecting data and making discoveries.
"
Metrics,Other,Triplet Loss,"Triplet loss is a way to teach a machine-learning model how to recognize the similarity or differences between items. It uses groups of three items, called triplets, which consist of an anchor item, a similar item (positive), and a dissimilar item (negative). 
The goal is to make the model understand that the anchor is closer to the positive than the negative item. This helps the model distinguish between similar and dissimilar items more effectively.
In face recognition, for example, the model compares two unfamiliar faces and determines if they belong to the same person. 
This scenario uses triplet loss to learn embeddings for every face. Faces from the same individual should be close together again and form well-separated clusters in the embedding space.
The objective of triplet loss is to build a representation space where the gap between similar samples is smaller than between different examples. By enforcing the order of distances, triplet loss models are embedded so that samples with identical labels appear nearer than those with other labels. 
Hence, the triplet loss architecture helps us learn distributed embedding through the concept of similarity and dissimilarity. The mathematical depiction is shown below:
The goal is to minimize the above equation by minimizing the first term and maximizing the second term, and bias acts as a threshold.
An anchor (with fixed identity) negative is an image that doesn’t share the class with the anchor—so, with a greater distance. In contrast, a positive is a point closer to the anchor, displaying a similar image. The model attempts to diminish the difference between similar classes while increasing the difference between different classes.
"
Metrics,Other,Triplet loss vs. contrastive loss,"Although both triplet loss and contrastive loss are loss functions used in siamese networks—deep learning models for measuring the similarity of two inputs—they have particular distinctions.
The critical distinction between triplet and contrastive loss is how similarity is defined and the number of samples used to compute the loss. The following pointers indicate the key differences.
Input: The number of inputs used to compute the loss differs. Triplet loss requires three inputs (anchor, positive, and negative), whereas contrastive loss requires only two (positive and negative) inputs.
Distance: The goal of triplet loss is to minimize the distance between the anchor and the positive example while raising the gap between the anchor and the negative example. The purpose of contrastive loss is to minimize the distance between the positive (similar) examples while increasing the distance between the negative (dissimilar) examples
Use cases: Triplet loss is used in problems that aim to acquire a representation space where similar cases are close together, and different examples are far apart—such as facial recognition. Contrastive loss is commonly employed in applications such as picture categorization.
Sensitivity: The margin parameter specifies the minimum distance that has to be kept between the anchor and the positive example and the maximum distance that has to be retained between both the anchor and the negative example, which is more dependent upon the selection of triplet loss. The margin parameter has less of an effect on contrast loss.
"
Probability and Statistics,Probability ,Probability,"Probability is the branch of mathematics and statistics concerning events and numerical descriptions of how likely they are to occur. The probability of an event is a number between 0 and 1; the larger the probability, the more likely an event is to occur. A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the two outcomes (""heads"" and ""tails"") are both equally probable; the probability of ""heads"" equals the probability of ""tails""; and since no other outcomes are possible, the probability of either ""heads"" or ""tails"" is 1/2 (which could also be written as 0.5 or 50%). 
"
Probability and Statistics,Probability ,PDF (Probability Density Function),"A function f : RD → R is called a probability density function (pdf ) if probability density function 
1. ∀x ∈ R pdf D : f(x) ⩾ 0 
2. Its integral exists and Z RD f(x)dx = 1.
For probability mass functions (pmf) of discrete random variables, the integral in is replaced with a sum
"
Probability and Statistics,Probability ,PMF (Probability Mass Function),"PMF is a statistical term that describes the probability distribution of the Discrete random variable
People often get confused between PDF and PMF. The PDF is applicable for continues random variable while PMF is applicable for discrete random variable For e.g, Throwing a dice (You can only select 1 to 6 numbers (countable) )
A probability mass function (PMF) is a function that gives the probability that a discrete random variable is exactly equal to some value. The PMF does not work for continuous random variables, because for a continuous random variable P(X=x)=0 for all x∈R. Instead, we can usually define the probability density function (PDF). The PDF is the density of probability rather than the probability mass:
"
Probability and Statistics,Probability ,CDF (Cumulative Distribution Function),"A cumulative distribution function (cdf) of a multivariate real-valued random variable X with states x ∈ RD is given by FX(x) = P(X1 ⩽ x1, . . . , XD ⩽ xD), where X = [X1, . . . , XD] ⊤, x = [x1, . . . , xD] ⊤, and the right-hand side represents the probability that random variable Xi takes the value smaller than or equal to xi . There are cdfs, which do not have corresponding pdfs. The cdf can be expressed also as the integral of the probability density function f(x)
"
Probability and Statistics,Probability ,Discrete and Continuous Probabilities ,"Depending on whether the target space is discrete or continuous, the natural way to refer to distributions is different. When the target space T is discrete, we can specify the probability that a random variable X takes a particular value x ∈ T , denoted as P(X = x). The expression P(X = x) for a discrete random variable X is known as the probability mass function. When the target space T is continuous, e.g., function the real line R, it is more natural to specify the probability that a random variable X is in an interval, denoted by P(a ⩽ X ⩽ b) for a < b. By convention, we specify the probability that a random variable X is less than a particular value x, denoted by P(X ⩽ x). The expression P(X ⩽ x) for cumulative a continuous random variable X is known as the cumulative distribution function
"
Probability and Statistics,Probability ,"Joint, marginal and conditional probabilities ","For two random variables X and Y , the probability that X = x and Y = y is (lazily) written as p(x, y) and is called the joint probability. One can think of a probability as a function that takes state x and y and returns a real number, which is the reason we write p(x, y). The marginal probability that X takes the value x irrespective of the value of random variable Y is (lazily) written as p(x). We write X ∼ p(x) to denote that the random variable X is distributed according to p(x). If we consider only the instances where X = x, then the fraction of instances (the conditional probability) for which Y = y is written (lazily) as p(y | x).
"
Probability and Statistics,Probability ,What is difference between Probability and Statistics?,"Probability theory and statistics are often presented together, but they concern different aspects of uncertainty. One way of contrasting them is by the kinds of problems that are considered. Using probability, we can consider a model of some process, where the underlying uncertainty is captured by random variables, and we use the rules of probability to derive what happens. In statistics, we observe that something has happened and try to figure out the underlying process that explains the observations. In this sense, machine learning is close to statistics in its goals to construct a model that adequately represents the process that generated the data. We can use the rules of probability to obtain a “best-fitting” model for some data.
"
Probability and Statistics,Probability ,Are there any differences between the expected value and mean value?,"Expected value is used when we want to calculate the mean of a probability distribution. This represents the average value we expect to occur before collecting any data. Mean is typically used when we want to calculate the average value of a given sample.

"
Probability and Statistics,Bayesian Probability,Bayesian and frequentist probabilities,"The Bayesian interpretation uses probability to specify the degree of uncertainty that the user has about an event. It is sometimes referred to as “subjective probability” or “degree of belief”. The frequentist interpretation considers the relative frequencies of events of interest to the total number of events that occurred. The probability of an event is defined as the relative frequency of the event in the limit when one has infinite data.
"
Probability and Statistics,Bayesian Probability,Bayes’ Theorem,"In machine learning and Bayesian statistics, we are often interested in making inferences of unobserved (latent) random variables given that we have observed other random variables. Let us assume we have some prior knowledge p(x) about an unobserved random variable x and some relationship p(y | x) between x and a second random variable y, which we can observe. If we observe y, we can use Bayes’ theorem to draw some conclusions about x given the observed values of y.
p(x) is the prior, which encapsulates our subjective prior prior knowledge of the unobserved (latent) variable x before observing any data. We can choose any prior that makes sense to us, but it is critical to ensure that the prior has a nonzero pdf (or pmf) on all plausible x, even if they are very rare.
The likelihood p(y | x) describes how x and y are related, and in the case of discrete probability distributions, it is the probability of the data y if we were to know the latent variable x. Note that the likelihood is not a distribution in x, but only in y. We call p(y | x) either the “likelihood of x (given y)” or the “probability of y given x” but never the likelihood of y. 
The posterior p(x | y) is the quantity of interest in Bayesian statistics posterior because it expresses exactly what we are interested in, i.e., what we know about x after having observed y.
p(y) := Z p(y | x)p(x)dx = EX[p(y | x)] is the marginal likelihood/evidence. 
The marginal likelihood is independent of x, and it ensures that the posterior p(x | y) is normalized. The marginal likelihood can also be interpreted as the expected likelihood where we take the expectation with respect to the prior p(x).
"
Probability and Statistics,Bayesian Probability,"Explain Prior, Posterior, Likelihood","Likelihood function (often simply called the likelihood) measures how well a statistical model explains observed data by calculating the probability of seeing that data under different parameter values of the model. – p(X| θ)
A prior probability distribution of an uncertain quantity, often simply called the prior, is its assumed probability distribution before some evidence is taken into account – p(θ)
The posterior probability is a type of conditional probability that results from updating the prior probability with information summarized by the likelihood via an application of Bayes' rule. – p(θ | x)
Bayes rule:
P(θ|x) = p(x| θ) * p(θ) / p(x)
"
Probability and Statistics,Bayesian Probability,"What Is the Difference Between ""Likelihood"" and ""Probability""?","In statistics, ""likelihood"" refers to the chance of observing data given a particular model or hypothesis, while ""probability"" represents the chance of an event occurring beforehand.
Likelihood vs Probability: Comparison
 p ( x ) = ∫ p ( x | θ ) p ( θ ) d θ 
"
Probability and Statistics,Bayesian Probability,Conjugate Prior,"According to Bayes’ theorem, the posterior is proportional to the product of the prior and the likelihood. The specification of the prior can be tricky for two reasons: First, the prior should encapsulate our knowledge about the problem before we see any data. This is often difficult to describe. Second, it is often not possible to compute the posterior distribution analytically. However, there are some priors that are computationally: conjugate priors.
A prior is conjugate for the likelihood function if the posterior is of the same form/type as the prior
Conjugacy is particularly convenient because we can algebraically calculate our posterior distribution by updating the parameters of the prior distribution.
The Beta distribution is the conjugate prior for the parameter µ in both the Binomial and the Bernoulli likelihood. For a Gaussian likelihood function, we can place a conjugate Gaussian prior on the mean. The reason why the Gaussian likelihood appears twice in the table is that we need to distinguish the univariate from the multivariate case. In the univariate (scalar) case, the inverse Gamma is the conjugate prior for the variance. In the multivariate case, we use a conjugate inverse Wishart distribution as a prior on the covariance matrix. The Dirichlet distribution is the conjugate prior for the multinomial likelihood function.
"
Probability and Statistics,Bayesian Probability,Bayesian Inference,"Focusing solely on some statistic of the posterior distribution (such as the parameter θ ∗ that maximizes the posterior) leads to loss of information, which can be critical in a system uses the prediction p(x | θ ∗ ) to make decisions. These decision-making systems typically have different objective functions than the likelihood, a squared-error loss or a mis-classification error. Therefore, having the full posterior distribution around can be extremely useful and leads to more robust decisions. Bayesian inference is about finding this posterior distribution. For a dataset X , a parameter prior p(θ), and a likelihood function, the posterior is obtained by applying Bayes’ theorem. 
Parameter estimation via maximum likelihood or MAP estimation yields a consistent point estimate θ∗ of the parameters, and the key computational problem to be solved is optimization. In contrast, Bayesian inference yields a (posterior) distribution, and the key computational problem to be solved is integration. Predictions with point estimates are straightforward, whereas predictions in the Bayesian framework require solving another integration problem. However, Bayesian inference gives us a principled way to incorporate prior knowledge, account for side information, and incorporate structural knowledge, all of which is not easily done in the context of parameter estimation. Moreover, the propagation of parameter uncertainty to the prediction can be valuable in decision-making systems for risk assessment and exploration in the context of data-efficient learning.
If we do not choose a conjugate prior on the parameters, the integrals are not analytically tractable, and we cannot compute the posterior in closed form.
"
Probability and Statistics,Bayesian Probability,Latent-Variable Models,"In practice, it is sometimes useful to have additional latent variables z (besides the model parameters θ) as part of the model. These latent variables are different from the model parameters θ as they do not parametrize the model explicitly. Latent variables may describe the data-generating process, thereby contributing to the interpretability of the model. They also often simplify the structure of the model and allow us to define simpler and richer model structures. Simplification of the model structure often goes hand in hand with a smaller number of model parameters. Learning in latent-variable models (at least via maximum likelihood) can be done in a principled way using the expectation maximization (EM) algorithm.
Denoting data by x, the model parameters by θ and the latent variables by z, we obtain the conditional distribution 
p(x | z, θ)
"
Probability and Statistics,Variance,"Covariance, Variance, Correlation","The covariance between two univariate random variables X, Y ∈ R is given by the expected product of their deviations from their respective means, i.e., CovX,Y [x, y] := EX,Y [(x − EX[x])(y − EY [y])]
By using the linearity of expectations, the expression in Definition 6.5 can be rewritten as the expected value of the product minus the product of the expected values, i.e., Cov[x, y] = E[xy] − E[x]E[y] .
The covariance of a variable with itself Cov[x, x] is called the variance is denoted by VX[x]. The square root of the variance is called the standard deviation and is often denoted by σ(x). The notion of covariance can be generalized to multivariate random variables.
The correlation between two random variables X, Y is given by corr[x, y] = Cov[x, y] /sqrt(V[x]V[y]) ∈ [−1, 1] . The correlation matrix is the covariance matrix of standardized random variables, x/σ(x). In other words, each random variable is divided by its standard deviation (the square root of the variance) in the correlation matrix. The covariance (and correlation) indicate how two random variables are related. Positive correlation corr[x, y] means that when x grows, then y is also expected to grow. Negative correlation means that as x increases, then y decreases.
"
Probability and Statistics,Variance,Bessel's correction,"In statistics, Bessel's correction is the use of n − 1 instead of n in the formula for the sample variance and sample standard deviation, where n is the number of observations in a sample. This method corrects the bias in the estimation of the population variance. It also partially corrects the bias in the estimation of the population standard deviation. However, the correction often increases the mean squared error in these estimations.  
"
Probability and Statistics,Variance,What is the standard error? What is the standard error of mean?,"Standard error of a statistic is the standard deviation of its sampling distribution or an estimate of that standard deviation.
Using CLM, we can estimate the standard error of mean by using population standard deviation divided by the square root of sample size n. If the population standard deviation is unknown, we can use the sample standard deviation as an estimation.
"
Probability and Statistics,Variance,Standard Error of the Mean vs. Standard Deviation: What's the Difference?,"The standard deviation (SD) measures the amount of variability, or dispersion, from the individual data values to the mean. SD is a frequently-cited statistic in many applications from math and statistics to finance and investing.
Standard error of the mean (SEM) measures how far the sample mean (average) of the data is likely to be from the true population mean. The SEM is always smaller than the SD.
SEM is calculated simply by taking the standard deviation and dividing it by the square root of the sample size.
"
Probability and Statistics,Variance,What does it mean if a model is heteroscedastic? what about homoscedastic?,"A model is heteroscedastic when the variance in errors is not consistent. Conversely, a model is homoscedastic when the variances in errors is consistent.
"
Probability and Statistics,Distributions,Uniform distribution,"Uniform Distribution is the probability distribution that represents equal likelihood of all outcomes within a specific range. i.e. the probability of each outcome occurring is the same. Whether dealing with a simple roll of a fair die or selecting a random number from a continuous interval, uniform distribution provides a straightforward yet powerful model for understanding randomness. 
Probability density function(pdf):  f(x) = 1/( b – a), a ≤ x ≤ b
Mean: μ =  (a + b)/2
Variance: σ2 = (b – a)^2 /12
"
Probability and Statistics,Distributions,What's the difference between Binomial Distribution and Geometric Distribution?,"The Binomial distribution describes the probability of obtaining k successes in n Bernoulli experiments, i.e an experiment which has only two possible outcomes, often call them success and failure. Its probability function describes the probability of getting exactly k successes in n independent Bernoulli trials:
The Geometric distribution describes the probability of experiencing a certain amount of failures before experiencing the first success in a series of Bernoulli experiments. This probability is given by:
P(X=k)=pk(1−p)n−k
So as we can see, the key difference is that in a binomial distribution, there is a fixed number of trials meanwhile in a geometric distribution, we’re interested in the number of trials required until we obtain a success.
"
Probability and Statistics,Distributions,Exponential Family,"An exponential family is a family of probability distributions, parameterized by θ ∈ RD, of the form 
p(x | θ) = h(x) exp (⟨θ, ϕ(x)⟩ − A(θ)) , 
where ϕ(x) is the vector of sufficient statistics. In general, any inner product can be used in (6.107), and for concreteness we will use the standard dot product here (⟨θ, ϕ(x)⟩ = θ ⊤ϕ(x)). Note that the form of the exponential family is essentially a particular expression of gθ(ϕ(x)) in the Fisher-Neyman theorem
Exponential families include many of the most common distributions. Among many others, exponential families includes the following:[6]
normal
exponential
gamma
chi-squared
beta
Dirichlet
Bernoulli
categorical
Poisson
Wishart
inverse Wishart
geometric
"
Probability and Statistics,Distributions,Gamma Distribution,"The gamma distribution is another widely used distribution. Its importance is largely due to its relation to exponential and normal distributions. 
Gamma function: The gamma function, shown by Γ(x), is an extension of the factorial function to real (and complex) numbers. Specifically, if n∈{1,2,3,...}, then 
Γ(n)=(n−1)!
More generally, for any positive real number α, Γ(α) is defined as 
Γ(α)=∫∞0xα−1e−xdx, for α>0.
Gamma Distribution:
We now define the gamma distribution by providing its PDF: 
A continuous random variable X is said to have a gamma distribution with parameters α>0 and λ>0, shown as X∼Gamma(α,λ), if its PDF is given by  fX(x)= λαxα−1e−λx/Γ(α) if x>0, 0 otherwise
Consider a sequence of events, with the waiting time for each event being an exponential distribution with rate λ. Then the waiting time for the n-th event to occur is the gamma distribution with integer shape α = n.
"
Probability and Statistics,Distributions,Poisson distribution,"The Poisson distribution is a discrete distribution that gives the probability of the number of independent events occurring in a fixed time. An example of when you would use this is if you want to determine the likelihood of X patients coming into a hospital in a given hour.
The mean and variance are both equal to λ.
Poisson Distribution Formula
Poisson distribution is characterized by a single parameter, lambda (λ), which represents the average rate of occurrence of the events. The probability mass function of the Poisson distribution is given by:
P(X = k) = e−λλk / k!
Where,
P(X = k) is the Probability of Observing k Events
e is the Base of the Natural Logarithm (approximately 2.71828)
λ is the Average Rate of Occurrence of Events
k is the Number of Events that Occur
In the Poisson distribution, both the mean (average) and variance are equal and are denoted by the parameter λ (lambda). This property of equal mean and variance is a distinctive characteristic of the Poisson distribution and simplifies its statistical analysis.
"
Probability and Statistics,Distributions,Beta Distribution,"We may wish to model a continuous random variable on a finite interval. The Beta distribution is a distribution over a continuous random variable µ ∈ [0, 1], which is often used to represent the probability for some binary event (e.g., the parameter governing the Bernoulli distribution). The Beta distribution Beta(α, β) (illustrated in Figure 6.11) itself is governed by two parameters α > 0, β > 0 and is defined as
Intuitively, α moves probability mass toward 1, whereas β moves probability mass toward 0. There are some special cases: For α = 1 = β, we obtain the uniform distribution U[0, 1]. For α, β < 1, we get a bimodal distribution with spikes at 0 and 1. For α, β > 1, the distribution is unimodal. For α, β > 1 and α = β, the distribution is unimodal, symmetric, and centered in the interval [0, 1], i.e., the mode/mean is at 1/2 .
"
Probability and Statistics,Distributions,Laplace Distribution,"The Laplace distribution, one of the earliest known probability distributions, is a continuous probability distribution named after the French mathematician Pierre-Simon Laplace. Like the normal distribution, this distribution is unimodal (one peak) and it is also a symmetrical distribution. However, it has a sharper peak than the normal distribution.
The Laplace distribution is the distribution of the difference of two independent random variables with identical exponential distributions (Leemis, n.d.). It is often used to model phenomena with heavy tails or when data has a higher peak than the normal distribution.
This distribution is the result of two exponential distributions, one positive and one negative; It is sometimes called the double exponential distribution, because it looks like two exponential distributions spliced together back-to-back.
PDF
The general formula for the probability density function (PDF) is:


where:
μ (any real number) is the location parameter and
β (must be > 0) is the scale parameter; this is sometimes called the diversity.
The shape of the Laplace distribution is defined by the location parameter and scale parameter. 

"
Probability and Statistics,Distributions,Cauchy distribution,"The Cauchy distribution, sometimes called the Lorentz distribution Cauchy–Lorentz distribution, Lorentz(ian) function, or Breit–Wigner distribution, is a family of continuous probably distributions which resemble the normal distribution family of curves. While the resemblance is there, it has a taller peak than a normal. And unlike the normal distribution, it’s fat tails decay much more slowly.
The distribution, named after 18th century mathematician Augustin-Louis Cauchy, is well known for the fact that it’s expected value and other moments do not exist. The median and mode do exist. And for the Cauchy, they are equal. Together, they tell you where the line of symmetry is. However, the Central Limit theorem doesn’t work for the limiting distribution of the mean.
It models the ratio of two normal random variables.
The Cauchy distribution has two main parts: a scale parameter (λ) and a location parameter (x0).
The location parameter (x0) tells us where the peak is.
The scale parameter is half the width of the PDF at half the maximum height.
In other words, the location parameter x0 shifts the graph along the x-axis and the scale parameter λ results in a shorter or taller graph. The smaller the scale parameter, the taller and thinner the curve. In the image below, the smaller value for lambda (0.5) gives the tallest and thinnest graph, shown in orange.
The Cauchy family. [5]
The standard Cauchy distribution (shown in purple on the above graph) has a location parameter of 0 and a scale parameter of 1; the notation for the standard distribution is X ~ Cauchy(1,0) or more simply, C(1,0).
"
Probability and Statistics,Distributions,Normal Distribution vs. t-Distribution: What’s the Difference?,"A Z-test is a hypothesis test with a normal distribution that uses a z-statistic. A z-test is used when you know the population variance or if you don’t know the population variance but have a large sample size.
A T-test is a hypothesis test with a t-distribution that uses a t-statistic. You would use a t-test when you don’t know the population variance and have a small sample size.
"
Probability and Statistics,Distributions,"Given draws from a normal distribution with known parameters, how can you simulate draws from a uniform distribution?","Plug in the value to the CDF of the same random variable
"
Probability and Statistics,Distributions,Gaussian Mixture,"Consider a mixture of two univariate Gaussian densities p(x) = αp1(x) + (1 − α)p2(x), (6.80) where the scalar 0 < α < 1 is the mixture weight, and p1(x) and p2(x) are univariate Gaussian densities with different parameters, i.e., (µ1, σ2 1 ) ̸= (µ2, σ2 2 ). Then the mean of the mixture density p(x) is given by the weighted sum of the means of each random variable: E[x] = αµ1 + (1 − α)µ2 . The variance of the mixture density p(x) is given by V[x] =  ασ2 1 + (1 − α)σ 2 2  +  αµ2 1 + (1 − α)µ 2 2  − [αµ1 + (1 − α)µ2] 2 .
"
Probability and Statistics,Distributions,You have an 50-50 mixture of two normal distributions with the same standard deviation. How far apart do the means need to be in order for this distribution to be bimodal?,"More than two standard deviations
"
Probability and Statistics,Distributions,What is a Skewed Distribution?,"A skewed distribution occurs when one tail is longer than the other. Skewness defines the asymmetry of a distribution. Unlike the familiar normal distribution with its bell-shaped curve, these distributions are asymmetric. The two halves of the distribution are not mirror images because the data are not distributed equally on both sides of the distribution’s peak.
"
Probability and Statistics,Distributions,Examples of Right-Skewed Distributions,"Right skewed distributions are the more common form. These distributions tend to occur when there is a lower limit, and most values are relatively close to the lower bound. Values can’t be less than this bound but can fall far from the peak on the high end, causing them to skew positively.
For example, right skewed distributions can occur in the following cases:
Time to failure cannot be less than zero, but there is no upper bound.
Wait and response times cannot be less than zero, but there are no upper limits.
Sales data cannot be less than zero but can have unusually large values.
Humans have a minimum viable weight but can have large extreme values.
Income cannot be less than zero, but there are some extremely high incomes.
"
Probability and Statistics,Distributions,How do you transform a Skewed Distribution into a Normal Distribution?,"To transform a Skewed Distribution into a Normal Distribution we apply some linearized function on it. Some common functions that achieve this goal are:
Logarithmic function: We can use it to make extremely skewed distributions less skewed, especially for right-skewed distributions. The only condition is that this function is defined only for strictly positive numbers. $$ f(x) = ln(x) $$
Square root transformation n: this one has an average effect on distribution shape: it’s weaker than logarithmic transformation, and it’s also used for reducing right-skewed distributions, but is defined only for positive numbers. f(x) = \sqrt{x}
Reciprocal transformation: this one reverses the order among values of the same sign, so large values become smaller, but the negative reciprocal preserves the order among values of the same sign. The only condition is that this function is not defined for zero values. f(x) = 1/x
Exponential or Power transformation: has a reasonable effect on distribution shape; generally, we apply power transformation (power of two usually) to reduce left skewness. We could also try any exponent to see which one provides better results. f(x) = x^n
Box-Cox Transformation: in this transformation, we’re searching and evaluating all the other transformations and choosing the best one. It's defined as:
The exponent here is a variable called lambda (λ) that varies over the range of -5 to 5, and in the process of searching, we examine all values of λ. Finally, we choose the optimal value (resulting in the best approximation to a normal distribution) for the variable.
"
Probability and Statistics,Central Limit Theorem,What is the Central Limit Theorem (CLM)?,"The Central Limit Theorem states that no matter what is the population’s original distribution, when taking random samples from the population, the distribution of the means or sums from the random samples approaches a normal distribution, with mean equals to the population mean, as the random sample size gets larger
"
Probability and Statistics,Central Limit Theorem,What general conditions must be satisfied for the central limit theorem to hold?,"The data must be sampled randomly
The sample values must be independent of each other
The sample size must be sufficiently large, generally it should be greater or equal than 30
"
Probability and Statistics,Central Limit Theorem,Sampling Distributions and the Standard Error of the Mean,"Imagine you draw a random sample of 50 from a population, measure a property, and calculate the mean. Now, suppose you repeat that study many times. You repeatedly draw random samples of the same size, calculate the mean for each sample, and graph all the means on a histogram. Ultimately, the histogram displays the distribution of sample means for random samples of size 50 for the characteristic you’re measuring.
Statisticians call this type of distribution a sampling distribution. And, because we’re calculating the mean, it’s the sampling distribution of the mean. There’s a different sampling distribution for each sample size.
This distribution is the sampling distribution for the above experiment. Remember that the curve describes the distribution of sample means and not individual observations. Like other distributions, sampling distributions have a central location and variability around that center.
The center falls on the population mean because random sampling tends to converge on this value.
The variability, or spread, describes how far sample means tend to fall from the population mean.
The wider the distribution, the further the sample means tend to fall from the population mean. That’s not good when you’re using sample means to estimate population means! You want narrow sampling distributions where sample means fall near the population mean.
The variability of the sampling distribution is the standard error of the mean! More specifically, the SEM is the standard deviation of the sampling distribution. For the example sampling distribution, the SEM is 3. 
The mean of the sampling distribution of sample means is mean.
"
Probability and Statistics,Central Limit Theorem,"If you had draws from a normal distribution with known parameters, how would you simulate draws from a uniform distribution?","A question like this tests your knowledge of the concepts of uniform and normal distributions.
There’s a simple answer to this. To simulate draws from a uniform distribution, you would plug the values into the normal cumulative distribution function (CDF) for the same random variable.
This is known as the Universality of the Uniform or Probability Integral Transform.
"
Probability and Statistics,Parameter Estimation,Empirical Risk Minimization,"For a given training set {(x1, y1), . . . ,(xN , yN )}, we introduce the notation of an example matrix X := [x1, . . . , xN ] ⊤ ∈ RN×D and a label vector y := [y1, . . . , yN ] ⊤ ∈ RN . Using this matrix notation the average loss is given by
R (f, X, y) = 1 / N * sum n=1..N ℓ(yn, yˆn),
where yˆn = f(xn, θ). Equation is called the empirical risk and depends on three arguments, the predictor f and the data X, y.
"
Probability and Statistics,Parameter Estimation,Maximum Likelihood Estimation,"The idea behind maximum likelihood estimation (MLE) is to define a function of the parameters that enables us to find a model that fits the data well. The estimation problem is focused on the likelihood function, or more precisely its negative logarithm. For data represented by a random variable x and for a family of probability densities p(x | θ) parametrized by θ, the negative log-likelihood is given by
Lx(θ) = − log p(x | θ)
Let us interpret what the probability density p(x | θ) is modeling for a fixed value of θ. It is a distribution that models the uncertainty of the data for a given parameter setting. For a given dataset x, the likelihood allows us to express preferences about different settings of the parameters θ, and we can choose the setting that more “likely” has generated the data.
We assume that the set of examples (x1, y1), . . . ,(xN , yN ) are independent and identically distributed (i.i.d.). Hence, in machine learning we often consider the negative log-likelihood 
L(θ) = − sum n=1..N log p(yn | xn, θ).
We often assume that we can explain our observation uncertainty by independent Gaussian noise with zero mean. We further assume that the linear model <x⊤n, θ> is used for prediction. This means we specify a Gaussian likelihood for each example label pair (xn, yn), 
p(yn | xn, θ) = N
In this case, minimizing the negative log-likelihood corresponds to solving the least-squares problem.
For other likelihood functions, i.e., if we model our noise with non-Gaussian distributions, maximum likelihood estimation may not have a closed-form analytic solution. In this case, we resort to numerical optimization methods.
MLE can be seen as a special case of the maximum a posteriori estimation (MAP) that assumes a uniform prior distribution of the parameters, or as a variant of the MAP that ignores the prior and which therefore is unregularized.
For gaussian mixtures, non parametric models, it doesn’t exist.
MLE is a random variable as it is calculated on a random sample. MLE is a consistent estimator and under certain conditions, it asymptotically converges to a normal distribution with true parameter as mean and variance equal to the inverse of the Fisher information matrix.
"
Probability and Statistics,Parameter Estimation,Maximum A Posteriori Estimation,"If we have prior knowledge about the distribution of the parameters θ, we can multiply an additional term to the likelihood. This additional term is a prior probability distribution on parameters p(θ). Bayes’ theorem gives us a principled tool to update our probability distributions of random variables. It allows us to compute a posterior distribution p(θ | x) (the more specific knowledge) on the parameters θ from general prior statements (prior distribution) p(θ) and the function p(x | θ) that links the parameters θ and the observed data x (called the likelihood): 
p(θ | x) = p(x | θ)p(θ) p(x)
Since the distribution p(x) does not depend on θ, we can ignore the value of the denominator for the optimization.
Instead of estimating the minimum of the negative log-likelihood, we now estimate the minimum of the negative log-posterior, which is referred to as maximum a posteriori estimation (MAP estimation).
In addition to the assumption of Gaussian likelihood in the previous example, we assume that the parameter vector is distributed as a multivariate Gaussian with zero mean. Note that the conjugate prior of a Gaussian is also a Gaussian, and therefore we expect the posterior distribution to also be a Gaussian.
"
Probability and Statistics,Parameter Estimation,Moments,"Let X be a random variable and c ∈ R a scalar. Then: The kth moment of X is:
E [X^k]
Let X be a random variable, and c ∈ R a scalar. Let x1, . . . , xn be iid realizations (samples) from X.
The kth sample moment of X is
1/n∑xi^k
"
Probability and Statistics,Parameter Estimation,Explain Method of Moments (MOM),"According to the Law of Large Numbers (LLN), the average converges to the expectation as the sample size tends to infinity. Using this law, the population moments which are a function of parameters are set equal to sample moments to solve for the parameters. For a normal distribution, both MLE and MOM produce sample mean as an estimate to the population mean.
We then define the method of moments (MoM) estimator ˆθM oM of θ = (θ1, . . . , θk) to be
a solution (if it exists) to the k simultaneous equations where, for j = 1, . . . , k, we set the jth (true)
moment equal to the jth sample moment:
E [X] = 1/n∑xi
…
E [X^k] = 1/n∑xi^k
"
Probability and Statistics,Parameter Estimation,Explain Kernel Density Estimation (KDE),"KDE is a non-parametric method to estimate pdf of data generating distribution. KDE allocates high density to certain x if sample data has many datapoints around it. A datapoint’s contribution to certain x depends on its distance to x and bandwidth. As the sample size increases, KDE approximation under certain conditions approaches true pdf.
In practice, we use the t-distribution most often when performing hypothesis tests or constructing confidence intervals.
"
Probability and Statistics,Parameter Estimation,Principle of maximum entropy,"The principle of maximum entropy states that the probability distribution which best represents the current state of knowledge about a system is the one with largest entropy, in the context of precisely stated prior data (such as a proposition that expresses testable information).
Another way of stating this: Take precisely stated prior data or testable information about a probability distribution function. Consider the set of all trial probability distributions that would encode the prior data. According to this principle, the distribution with maximal information entropy is the best choice.
"
Probability and Statistics,Parameter Estimation,Koopman-Pitman-Darmois (KPD) theorem,"If a family of probability distributions has a sufficient statistic of fixed finite dimension (no matter the sample size), then the family must be an exponential family.
In simple terms:
A sufficient statistic summarizes all the information in the data relevant to estimating the parameters.
If you can summarize the data with a simple, finite-sized statistic, then the distributions in the family have to follow a specific form (the exponential family).
"
Probability and Statistics,Hypothesis Testing,Statistical Hypothesis Testing,"A statistical hypothesis test makes an assumption about the outcome, called the null hypothesis.
For example, the null hypothesis for the Pearson’s correlation test is that there is no relationship between two variables. The null hypothesis for the Student’s t test is that there is no difference between the means of two populations.
The test is often interpreted using a p-value, which is the probability of observing the result given that the null hypothesis is true, not the reverse, as is often the case with misinterpretations.
p-value (p): Probability of obtaining a result equal to or more extreme than was observed in the data.
"
Probability and Statistics,Hypothesis Testing,P-value,"The p-value in a statistical test helps you determine whether to reject or support the null hypothesis. It's a metric that argues against the null hypothesis and relies on the alpha value, critical value and probability. Measuring a smaller p-value suggests the rejection of the null hypothesis, whereas a higher p-value indicates stronger evidence for supporting the null hypothesis.
The p-value is a probability measure and uses the degree of freedom and estimation based on the alpha value of a t-test. Taking the sample size n, subtract one to get the degree of freedom (n - 1). Comparing the result to a respective alpha level gives you the estimate for the p-value. It's important to note that p-values depend on the results t-tests give you and can change according to different t-statistics.
"
Probability and Statistics,Hypothesis Testing,How to interpret P-value?,"In interpreting the p-value of a significance test, you must specify a significance level, often referred to as the Greek lower case letter alpha (a). A common value for the significance level is 5% written as 0.05.
The p-value is interested in the context of the chosen significance level. A result of a significance test is claimed to be “statistically significant” if the p-value is less than the significance level. This means that the null hypothesis (that there is no result) is rejected.
p <= alpha: reject H0, different distribution.
p > alpha: fail to reject H0, same distribution.
Where:
Significance level (alpha): Boundary for specifying a statistically significant finding when interpreting the p-value.
We can see that the p-value is just a probability and that in actuality the result may be different. The test could be wrong. Given the p-value, we could make an error in our interpretation.
"
Probability and Statistics,Hypothesis Testing,Types of Errors,"Type I Error. Reject the null hypothesis when there is in fact no significant effect (false positive). The p-value is optimistically small.  α = probability of a Type I error.
Type II Error. Not reject the null hypothesis when there is a significant effect (false negative). The p-value is pessimistically large. β = probability of a Type II error.
"
Probability and Statistics,Hypothesis Testing,What is alpha- and beta-values?,"Alpha is also known as the level of significance. This represents the probability of obtaining your results due to chance. The smaller this value is, the more “unusual” the results, indicating that the sample is from a different population than it’s being compared to, for example. Commonly, this value is set to .05 (or 5%), but can take on any value chosen by the research not exceeding .05.
Alpha also represents your chance of making a Type I Error. What’s that? The chance that you reject the null hypothesis when in reality, you should fail to reject the null hypothesis. In other words, your sample data indicates that there is a difference when in reality, there is not. Like a false positive.
The other key-value relates to the power of your study. Power refers to your study’s ability to find a difference if there is one. It logically follows that the greater the power, the more meaningful your results are. Beta = 1 – Power. Values of beta should be kept small, but do not have to be as small as alpha values. Values between .05 and .20 are acceptable.
Beta also represents the chance of making a Type II Error. As you may have guessed, this means that you came to the wrong conclusion in your study, but it’s the opposite of a Type I Error. With a Type II Error, you incorrectly fail to reject the null. In simpler terms, the data indicates that there is not a significant difference when in reality there is. Your study failed to capture a significant finding. Like a false negative.
"
Probability and Statistics,Hypothesis Testing,Significance level and confidence level,"Alpha is the significance level used to compute the confidence level. The confidence level equals 100*(1 - alpha)%, or in other words, an alpha of 0.05 indicates a 95 percent confidence level. Standard_dev is the population standard deviation for the data range and is assumed to be known.
The confidence level in hypothesis testing is the probability of not rejecting the null hypothesis when the null hypothesis is True:
P(Not Rejecting H0|H0 is True) = 1 - P(Rejecting H0|H0 is True)
The default confidence level is set at 95%.
"
Probability and Statistics,Hypothesis Testing,What is the statistical power of a test? ,"β = probability of a Type II error, known as a ""false negative""
1 − β = probability of a ""true positive"", i.e., correctly rejecting the null hypothesis. ""1 − β"" is also known as the power of the test.
 α = probability of a Type I error, known as a ""false positive""
1 − α = probability of a ""true negative"", i.e., correctly not rejecting the null hypothesis
The power of a test is the probability of rejecting the null hypothesis when it’s false. It’s also equal to 1 minus the beta.
It is generally accepted we should aim for a power of 0.8 or greater.
"
Probability and Statistics,Hypothesis Testing,What are two ways to increase the power of a test?,"To increase the power of the test, you can do two things:
You can increase alpha, but it also increases the chance of a type 1 error
Increase the sample size, n. This maintains the type 1 error but reduces type 2.
"
Probability and Statistics,Power Analysis,Power Analysis,"Statistical power is one piece in a puzzle that has four related parts; they are:
Effect Size. The quantified magnitude of a result present in the population. Effect size is calculated using a specific statistical measure, such as Pearson’s correlation coefficient for the relationship between variables or Cohen’s d for the difference between groups.
Sample Size. The number of observations in the sample.
Significance. The significance level used in the statistical test, e.g. alpha. Often set to 5% or 0.05.
Statistical Power. The probability of accepting the alternative hypothesis if it is true.
All four variables are related. For example, a larger sample size can make an effect easier to detect, and the statistical power can be increased in a test by increasing the significance level.
A power analysis involves estimating one of these four parameters given values for three other parameters. This is a powerful tool in both the design and in the analysis of experiments that we wish to interpret using statistical hypothesis tests.
For example, the statistical power can be estimated given an effect size, sample size and significance level. Alternately, the sample size can be estimated given different desired levels of significance.
Perhaps the most common use of a power analysis is in the estimation of the minimum sample size required for an experiment.
As a practitioner, we can start with sensible defaults for some parameters, such as a significance level of 0.05 and a power level of 0.80. We can then estimate a desirable minimum effect size, specific to the experiment being performed. A power analysis can then be used to estimate the minimum sample size required.
In general, large effect sizes require smaller sample sizes because they are “obvious” for the analysis to see/find. As we decrease in effect size we required larger sample sizes as smaller effect sizes are harder to find. This works in our favor as the larger the effect size the more important our results and fewer participants we need to recruit for our study.
"
Probability and Statistics,Power Analysis,Effect Size,"Effect size tries to answer the question of “Are these differences large enough to be meaningful despite being statistically significant?”.
Effect size addresses the concept of “minimal important difference” which states that at a certain point a significant difference (ie p≤ 0.05) is so small that it wouldn’t serve any benefits in the real world. Keep in mind, by small we do not mean a small p-value.
A different way to look at effect size is the quantitative measure of how much the IV affected the DV. A high effect size would indicate a very important result as the manipulation on the IV produced a large effect on the DV.
Effect size is typically expressed as Cohen’s d. Cohen described a small effect = 0.2, medium effect size = 0.5 and large effect size = 0.8
Smaller p-values (0.05 and below) don’t suggest the evidence of large or important effects, nor do high p-values (0.05+) imply insignificant importance and/or small effects. Given a large enough sample size, even very small effect sizes can produce significant p-values (0.05 and below). In other words, statistical significance explores the probability our results were due to chance and effect size explains the importance of our results.
"
Probability and Statistics,Power Analysis,Cohen's d ,"Cohen’s d is an effect size used to indicate the standardized difference between two means. It can be used, for example, to accompany reporting of t-test and ANOVA results. It is also widely used in meta-analysis.
Cohen’s D , or standardized mean difference, is one of the most common ways to measure effect size. An effect size is how large an effect is. For example, medication A has a larger effect than medication B. While a p-value can tell you if there is an effect, it won’t tell you how large that effect is.
Cohen’s D specifically measures the effect size of the difference between two means.

spooled = pooled standard deviations for the two groups. The formula is: √[(s12+ s22) / 2]
"
Probability and Statistics,Power Analysis,What are the confidence intervals of the coefficients?,"Confidence interval (CI) is a type of interval estimate (of a population parameter) that is computed from the observed data. The confidence level is the frequency (i.e., the proportion) of possible confidence intervals that contain the true value of their corresponding parameter. In other words, if confidence intervals are constructed using a given confidence level in an infinite number of independent experiments, the proportion of those intervals that contain the true value of the parameter will match the confidence level.
Confidence intervals consist of a range of values (interval) that act as good estimates of the unknown population parameter. However, the interval computed from a particular sample does not necessarily include the true value of the parameter. Since the observed data are random samples from the true population, the confidence interval obtained from the data is also random. If a corresponding hypothesis test is performed, the confidence level is the complement of the level of significance, i.e. a 95% confidence interval reflects a significance level of 0.05. If it is hypothesized that a true parameter value is 0 but the 95% confidence interval does not contain 0, then the estimate is significantly different from zero at the 5% significance level.
The desired level of confidence is set by the researcher (not determined by data). Most commonly, the 95% confidence level is used. However, other confidence levels can be used, for example, 90% and 99%.
Factors affecting the width of the confidence interval include the size of the sample, the confidence level, and the variability in the sample. A larger sample size normally will lead to a better estimate of the population parameter. A Confidence Interval is a range of values we are fairly sure our true value lies in.
X ± Z*s/√(n), X is the mean, Z is the chosen Z-value from the table, s is the standard deviation, n is the number of samples. The value after the ± is called the margin of error.
"
Probability and Statistics,Power Analysis,Confidence Interval ,"CI = x  +/-  t1-α/2, n-1*(s/√n)
where:
x: sample mean
t: the critical t-value, based on the significance level α and sample size n
s: sample standard deviation
n: sample size
"
Probability and Statistics,Statistical Tests,What are Statistical Tests?,"Statistical tests are a way of mathematically determining whether two sets of data are significantly different from each other. To do this, statistical tests use several statistical measures, such as the mean, standard deviation, and coefficient of variation. Once the statistical measures are calculated, the statistical test will then compare them to a set of predetermined criteria. If the data meet the criteria, the statistical test will conclude that there is a significant difference between the two sets of data.
There are various statistical tests that can be used, depending on the type of data being analyzed. However, some of the most common statistical tests are t-tests, chi-squared tests, and ANOVA tests.
"
Probability and Statistics,Statistical Tests,Null Hypothesis,"In scientific research, the null hypothesis (often denoted H0) is the claim that the effect being studied does not exist. The null hypothesis can also be described as the hypothesis in which no relationship exists between two sets of data or variables being analyzed. If the null hypothesis is true, any experimentally observed effect is due to chance alone, hence the term ""null"". In contrast with the null hypothesis, an alternative hypothesis is developed, which claims that a relationship does exist between two variables. 
Null hypotheses start as research questions that the investigator rephrases as a statement indicating there is no effect or relationship.
"
Probability and Statistics,Statistical Tests,Types of Statistical Tests,"When working with statistical data, several tools can be used to analyze the information.
1. Parametric Statistical Tests
Parametric statistical tests have precise requirements compared with non-parametric tests. Also, they make a strong inference from the data. Furthermore, they can only be conducted with data that adhere to common assumptions of statistical tests. Some common types of parametric tests are regression tests, comparison tests, and correlation tests.
1.1. Regression Tests
Regression tests determine cause-and-effect relationships. They can be used to estimate the effect of one or more continuous variables on another variable.
Simple linear regression is a type of test that describes the relationship between a dependent and an independent variable using a straight line. This test determines the relationship between two quantitative variables.
Multiple linear regression measures the relationship between a quantitative dependent variable and two or more independent variables, again using a straight line.
Logistic regression predicts and classifies the research problem. Logistic regression helps identify data anomalies, which could be predictive fraud.
1.2. Comparison Tests
Comparison tests determine the differences among the group means. They can be used to test the effect of a categorical variable on the mean value of other characteristics.
T-test
One of the most common statistical tests is the t-test, which is used to compare the means of two groups (e.g. the average heights of men and women). You can use the t-test when you are not aware of the population parameters (mean and standard deviation).
Paired T-test
It tests the difference between two variables from the same population (pre-and post-test scores). For example, measuring the performance score of the trainee before and after the completion of the training program.
Independent T-test
The independent t-test is also called the two-sample t-test. It is a statistical test that determines whether there is a statistically significant difference between the means in two unrelated groups. For example, comparing cancer patients and pregnant women in a population.
One Sample T-test
In this test, the mean of a single group is compared with the given mean. For example, determining the increase and decrease in sales in the given average sales.
ANOVA
ANOVA (Analysis of Variance) analyzes the difference between the means of more than two groups. One-way ANOVAs determine how one factor impacts another, whereas two-way analyses compare samples with different variables. It determines the impact of one or more factors by comparing the means of different samples.
MANOVA
MANOVA, which stands for Multivariate Analysis of Variance, provides regression analysis and analysis of variance for multiple dependent variables by one or more factor variables or covariates. Also, it examines the statistical difference between one continuous dependent variable and an independent grouping variable.
Z-test
It is a statistical test that determines whether two population means are different, provided the variances are known and the sample size is large. 
1.3. Correlation Tests
Correlation tests check if the variables are related without hypothesizing a cause-and-effect relationship. These tests can be used to check if the two variables you want to use in a multiple regression test are correlated.
Pearson Correlation Coefficient
It is a common way of measuring the linear correlation. The coefficient is a number between -1 and 1 and determines the strength and direction of the relationship between two variables. The change in one variable changes the course of  another variable change in the same direction.
2. Non-parametric Statistical Tests
Non-parametric tests do not make as many assumptions about the data  compared to parametric tests. They are useful when one or more of the common statistical assumptions are violated. However, these inferences are not as accurate as with parametric tests.
Chi-square test
The chi-square test compares two categorical variables. Furthermore, calculating the chi-square statistic value and comparing it with a critical value from the chi-square distribution allows you to assess whether the observed frequency is significantly different from the expected frequency.
"
Probability and Statistics,Statistical Tests,How to choose statistical test?,"1. Research Question
The decision for a statistical test depends on the research question that needs to be answered. Additionally, the research questions will help you formulate the data structure and research design.
2. Formulation of Null Hypothesis
After defining the research question, you could develop a null hypothesis. A null hypothesis suggests that no statistical significance exists in the expected observations.
3. Level of Significance in Study Protocol
Before performing the study protocol, a level of significance is specified. The level of significance determines the statistical importance, which defines the acceptance or rejection of the null hypothesis.
4. The Decision Between One-tailed and Two-tailed
You must decide if your study should be a one-tailed or two-tailed test. If you have clear evidence where the statistics are leading in one direction,  you must perform one-tailed tests. However, if there is no particular direction of the expected difference, you must perform a two-tailed test.
5. The Number of Variables to Be Analyzed
Statistical tests and procedures are divided according to the number of variables that are designed to analyze. Therefore, while choosing the test , you must consider how many variables you want to analyze.
6. Type of Data
It is important to define whether your data is continuous, categorical, or binary. In the case of continuous data, you must also check if the data are normally distributed or skewed, to further define which statistical test to consider.
7. Paired and Unpaired Study Designs
A paired design includes comparison studies where the two population means are compared when the two samples depend on each other. In an unpaired or independent study design, the results of the two samples are grouped and then compared.
"
Probability and Statistics,Statistical Tests,What is Pearson Correlation?,"Correlation between sets of data is a measure of how well they are related. The most common measure of correlation in stats is the Pearson Correlation. The full name is the Pearson Product Moment Correlation (PPMC). It shows the linear relationship between two sets of data. In simple terms, it answers the question, Can I draw a line graph to represent the data? Two letters are used to represent the Pearson correlation: Greek letter rho (ρ) for a population and the letter “r” for a sample.
ρ = cov(X, Y)/ σX* σY,
where cov is the covariance,
σ is the standard deviation.
"
Probability and Statistics,Statistical Tests,Potential problems with Pearson correlation.,"The PPMC is not able to tell the difference between dependent variables and independent variables. For example, if you are trying to find the correlation between a high calorie diet and diabetes, you might find a high correlation of .8. However, you could also get the same result with the variables switched around. In other words, you could say that diabetes causes a high calorie diet. That obviously makes no sense. Therefore, as a researcher you have to be aware of the data you are plugging in. In addition, the PPMC will not give you any information about the slope of the line; it only tells you whether there is a relationship.
"
Probability and Statistics,Statistical Tests,What is Spearman Correlation?,"The Spearman correlation between two variables is equal to the Pearson correlation between the rank values of those two variables; while Pearson's correlation assesses linear relationships, Spearman's correlation assesses monotonic relationships (whether linear or not). If there are no repeated data values, a perfect Spearman correlation of +1 or −1 occurs when each of the variables is a perfect monotone function of the other. 
Intuitively, the Spearman correlation between two variables will be high when observations have a similar (or identical for a correlation of 1) rank (i.e. relative position label of the observations within the variable: 1st, 2nd, 3rd, etc.) between the two variables, and low when observations have a dissimilar (or fully opposed for a correlation of −1) rank between the two variables. 
"
Probability and Statistics,Statistical Tests,A comparison of the Pearson and Spearman correlation methods,"A correlation coefficient measures the extent to which two variables tend to change together. The coefficient describes both the strength and the direction of the relationship. 
The Pearson correlation evaluates the linear relationship between two continuous variables. A relationship is linear when a change in one variable is associated with a proportional change in the other variable.
The Spearman correlation evaluates the monotonic relationship between two continuous or ordinal variables. In a monotonic relationship, the variables tend to change together, but not necessarily at a constant rate. The Spearman correlation coefficient is based on the ranked values for each variable rather than the raw data.
"
Probability and Statistics,Statistical Tests,Pearson’s chi-squared test,"Fisher’s exact test has the important advantage of computing exact p-values. But if we have a large sample size, it may be computationally inefficient. In this case, we can use Pearson’s chi-squared test to compute an approximate p-value.
Let us call Oij the observed value of the contingency table at row i and column j. Under the null hypothesis of independence of rows and columns, i.e. assuming that A and B have same efficacy, we can easily compute corresponding expected values Eij. Moreover, if the observations are normally distributed, then the χ2 statistic follows exactly a chi-square distribution with 1 degree of freedom.
In fact, this test can also be used with non-normal observations if the sample size is large enough, thanks to the central limit theorem.
Example: Handedness and nationality
A chi-square test (a test of independence) can test whether these observed frequencies are significantly different from the frequencies expected if handedness is unrelated to nationality.
"
Probability and Statistics,Statistical Tests,Z-test (Z-value),"The Z-test can be applied under the following assumptions.
The observations are normally distributed (or the sample size is large).
The sampling distributions have known variance σX and σY.
Under the above assumptions, the Z-test exploits the fact that the following Z statistic has a standard normal distribution.
The z-value is another common test statistic where the null hypothesis suggests the means of two populations are equal. This metric goes beyond the t-value, which tests only a sample of the population. The z-score is also important for calculating the probability of a data value appearing within the normal distribution for a specific standard. This allows for the comparison of two z-values from different sample groups with varying standard deviations and mean values. To get the z-value, you can use the formula:
z = (X - μ) / σ, where X represents the raw data or score, μ is the mean of the population and σ is the standard deviation for the population. 
"
Probability and Statistics,Statistical Tests,Student’s t-test (T-value),"In most cases, the variances of the sampling distributions are unknown, so that we need to estimate them. Student’s t-test can then be applied under the following assumptions.
The observations are normally distributed (or the sample size is large).
The sampling distributions have “similar” variances σX ≈ σY.
Under the above assumptions, Student’s t-test relies on the observation that the following t statistic has a Student’s t distribution.
The t-value is one type of test statistic that results from performing either t-tests or regression tests. Evaluating the t-value requires testing a null hypothesis where the means of both test samples are equal. If you perform a t-test or regression rest and find the means are not equal, you reject the null hypothesis for the alternative hypothesis. You can calculate a t-value using a common t-test with the formula:
t = (X‾ - μ0) / (s / √n), where X‾ is the sample mean, μ0 represents the population mean, s is the standard deviation of the sample and n stands for the size of the sample.
"
Probability and Statistics,Statistical Tests,F-value,"An f-value is a test statistic that you can get from an analysis of variance (ANOVA). This statistical test measures the difference in means for two or more independent samples. The f-value shows the significance of the mean differences, indicating whether the variance between the groups forms a relationship.
If the f-value is greater than or equal to the variation between the groups, the null hypothesis holds true. If the f-value is less than the variation between the sample groups, it rejects the null hypothesis. Calculating the f-value relies on sophisticated computations, which many data scientists perform with computer software.
"
Probability and Statistics,Statistical Tests,X2 value,"The X2 value comes from non-parametric correlation tests that measure whether there is a causal relationship between variables. This value can also tell you whether the two variables you want to use in a statistical analysis already display a relationship. This test statistic becomes useful when preparing variables for testing in regression analysis, as the null hypothesis for the X2 value indicates independent samples.
"
Probability and Statistics,Statistical Tests,Welch’s t-test,"In most cases Student’s t test can be effectively applied with good results. However, it may rarely happen that its second assumption (similar variance of the sampling distributions) is violated. In that case, we cannot compute a pooled variance and rather than Student’s t test we should use Welch’s t-test.
This test operates under the same assumptions of Student’s t-test but removes the requirement on the similar variances. Then, we can use a slightly different t statistic, which also has a Student’s t distribution, but with a different number of degrees of freedom ν.
"
Probability and Statistics,Statistical Tests,Chi-square tests vs t-tests,"Both chi-square tests and t tests can test for differences between two groups. However, a t test is used when you have a dependent quantitative variable and an independent categorical variable (with two groups). A chi-square test of independence is used when you have two categorical variables.
"
Probability and Statistics,Statistical Tests,Continuous non-normal metrics,"In the previous section on continuous metrics, we assumed that our observations came from normal distributions. But non-normal distributions are extremely common when dealing with per-user monthly revenues etc. There are several ways in which normality is often violated:
zero-inflated distributions — most user don’t buy anything at all, so lots of zero observations;
multimodal distributions — a market segment tends purchases cheap products, while another segment purchases more expensive products.
However, if we have enough samples, tests derived under normality assumptions like Z-test, Student’s t-test, and Welch’s t-test can still be applied for observations that signficantly deviate from normality. Indeed, thanks to the central limit theorem, the distribution of the test statistics tends to normality as the sample size increases. In the zero-inflated and multimodal example we are considering, even a sample size of 40 produces a distribution that is well approximated by a normal distribution.
But if the sample size is still too small to assume normality, we have no other choice than using a non-parametric approach such as the Mann-Whitney U test.
"
Probability and Statistics,Statistical Tests,Mann–Whitney U test,"This test makes no assumption on the nature of the sampling distributions, so it is fully nonparametric. The idea of Mann-Whitney U test is to compute the following U statistic.
The values of this test statistic are tabulated, as the distribution can be computed under the null hypothesis that, for random samples X and Y from the two populations, the probability P(X < Y) is the same as P(X > Y).
"
Probability and Statistics,Statistical Tests,You are told that your regression model is suffering from multicollinearity. How do verify this is true and build a better model?,"You should create a correlation matrix to identify and remove variables with a correlation above 75%. Keep in mind that our threshold here is subjective.
You could also calculate VIF (variance inflation factor) to check for the presence of multicollinearity. 
You can’t just remove variables, so you should use a penalized regression model or add random noise in the correlated variables, but this approach is less ideal.
"
Probability and Statistics,Normality Tests,How do we check if a variable follows the normal distribution? ,"Plot a histogram out of the sampled data. If you can fit the bell-shaped ""normal"" curve to the histogram, then the hypothesis that the underlying random variable follows the normal distribution can not be rejected.
Check Skewness and Kurtosis of the sampled data. Skewness = 0 and kurtosis = 3 are typical for a normal distribution, so the farther away they are from these values, the more non-normal the distribution.
Use Kolmogorov-Smirnov or/and Shapiro-Wilk tests for normality. They take into account both Skewness and Kurtosis simultaneously.
Check for Quantile-Quantile plot. It is a scatterplot created by plotting two sets of quantiles against one another. Normal Q-Q plot place the data points in a roughly straight line.
"
Probability and Statistics,Normality Tests,Statistical Tests for Normality,"You’ve got lots of options to test for normality. Most of these are included with statistical packages like SPSS.
Chi-square normality test. You can use a chi square test for normality. The advantage is that it’s relatively easy to use, but it isn’t a very strong test. If you have a small sample (under 20), it may be the only test you can use. For larger samples, you’re much better off choosing another option.
D’Agostino-Pearson Test. This uses skewness and kurtosis to see if your data matches normal data. It requires your sample size to be over 20.
Jarque-Bera Test. This common test is also relatively straightforward. Like the D’Agostino-Pearson, the basic idea is that it tests the skew and kurtosis of your data to see if it matches what you would expect from a normal distribution. The larger the JB statistic, the more the data deviates from the normal.
Kolmogorov-Smirnov Goodness of Fit Test. This compares your data with a known distribution (i.e. a normal distribution).
Lilliefors Test. The Lilliefors test calculates a test statistic T which you can compare to a critical value. If the test statistic is bigger than the critical value, it’s a sign that your data isn’t normal. It also computes a p-value for your distribution, which you compare to a significance level.
Shapiro-Wilk Test This test will tell you if a random sample came from a normal distribution. The test gives you a W value; small values indicate your sample is not normally distributed. 
"
Probability and Statistics,Normality Tests,Shapiro-Wilk test ,"Shapiro-Wilk test is a test of normality, it determines whether the given sample comes from the normal distribution or not. Shapiro-Wilk’s test or Shapiro test is a normality test in frequentist statistics. The null hypothesis of Shapiro’s test is that the population is distributed normally.
Suppose a sample, say x1,x2…….xn,  has come from a normally distributed population. Then according to the Shapiro-Wilk’s tests null hypothesis test
W = (sum_i=1..n(aixi))^2 / (sum_i=1..n xi – mean(x))^2
where,
x(i) : it is the ith smallest number in the given sample.
mean(x) : ( x1+x2+……+xn) / n i.e the sample mean.
ai : coefficient that can be calculated as (a1,a2,….,an) = (mT V-1)/C . Here V is the covariance matrix, m and C are the vector norms that can be calculated as C= || V-1 m || and m = (m1, m2,……, mn ).
Output:
(0.9977102279663086, 0.7348126769065857)
Output Interpretation:
Since in the above example, the p-value is 0.73 which is more than the threshold(0.05) which is the alpha(0.05) then we fail to reject the null hypothesis i.e. we do not have sufficient evidence to say that sample does not come from a normal distribution.
"
Probability and Statistics,A/B testing,What is A/B testing?,"A/B testing is one of the most popular controlled experiments used to optimize web marketing strategies. It allows decision makers to choose the best design for a website by looking at the analytics results obtained with two possible alternatives A and B.
To understand what A/B testing is about, let’s consider two alternative designs: A and B. Visitors of a website are randomly served with one of the two. Then, data about their activity is collected by web analytics. Given this data, one can apply statistical tests to determine whether one of the two designs has better efficacy.
With the data we collected from the activity of users of our website, we can compare the efficacy of the two designs A and B. Simply comparing mean values wouldn’t be very meaningful, as we would fail to assess the statistical significance of our observations. It is indeed fundamental to determine how likely it is that the observed discrepancy between the two samples originates from chance.
In order to do that, we will use a two-sample hypothesis test. Our null hypothesis H0 is that the two designs A and B have the same efficacy, i.e. that they produce an equivalent click-through rate, or average revenue per user, etc. The statistical significance is then measured by the p-value, i.e. the probability of observing a discrepancy between our samples at least as strong as the one that we actually observed.
Now, some care has to be applied to properly choose the alternative hypothesis Ha. This choice corresponds to the choice between one- and two- tailed tests .
A two-tailed test is preferable in our case, since we have no reason to know a priori whether the discrepancy between the results of A and B will be in favor of A or B. This means that we consider the alternative hypothesis Ha the hypothesis that A and B have different efficacy.
The p-value is therefore computed as the area under the the two tails of the probability density function p(x) of a chosen test statistic on all x’ s.t. p(x’) <= p(our observation). The computation of such p-value clearly depends on the data distribution. So we will first see how to compute it for discrete metrics, and then for continuous metrics.
"
Probability and Statistics,A/B testing,When A/B testing can't be used?,"A/B testing can't be used effectively in the following situations:
Small Sample Sizes
If your sample size is too small, the results may lack statistical significance, leading to unreliable conclusions.
Long Feedback Loops
When the effect of a change takes a long time to manifest (e.g., brand reputation changes), A/B tests may be impractical.
Ethical Concerns
In scenarios involving healthcare, finance, or safety-critical systems, it may be unethical to expose one group to a potentially harmful variation.
Highly Interconnected Features
If changes in one feature affect other parts of the system, isolating the impact of the variation becomes challenging.
Low Traffic or Engagement
For websites or platforms with very little traffic, achieving statistically significant results may take too long or be impossible.
Short-Term vs. Long-Term Effects
A/B tests focus on short-term impacts, so they may not capture long-term behavioral changes.
Non-Randomizable Audiences
If you cannot randomly assign users to different groups (e.g., geographic restrictions or personalized content), A/B testing may lead to biased results.
Continuous or Dynamic Environments
In rapidly changing systems, like stock markets or dynamic pricing algorithms, the environment may shift during testing, invalidating results.
Non-Measurable Outcomes
When outcomes cannot be directly measured (e.g., customer satisfaction or brand perception), A/B testing may not be feasible.
Testing Multiple Variations (Multivariate Tests Needed)
When testing interactions between multiple variables, multivariate testing is more appropriate than simple A/B testing.
"
Probability and Statistics,Miscellaneous,Simpson’s paradox,"Simpson’s paradox refers to the situations in which a trend or relationship that is observed within multiple groups disappears or reverses when the groups are combined. The quick answer to why there is Simpson’s paradox is the existence of confounding variables.
"
Probability and Statistics,Miscellaneous,What Is a Statistical Interaction?,"A statistical interaction is when two or more variables interact, and this results in a third variable being affected. 
Examples. Real-world examples of interaction include: Interaction between adding sugar to coffee and stirring the coffee. Neither of the two individual variables has much effect on sweetness but a combination of the two does.
"
Probability and Statistics,Miscellaneous,How do you identify if a coin is biased?,"We collect data by flipping the coin 200 times. 
To perform a chi-square test (or any other statistical test), we first must establish our null hypothesis. In this example, our null hypothesis is that the coin should be equally likely to land head-up or tails-up every time. The null hypothesis allows us to state expected frequencies. For 200 tosses, we would expect 100 heads and 100 tails.
The Observed values are those we gather ourselves. The expected values are the frequencies expected, based on our null hypothesis. We total the rows and columns as indicated. It's a good idea to make sure that the row totals equal the column totals (both total to 400 in this example).
Using probability theory, statisticians have devised a way to determine if a frequency distribution differs from the expected distribution. To use this chi-square test, we first have to calculate chi-squared.
Chi-squared = (observed-expected)2/(expected)
We have two classes to consider in this example, heads and tails.
Now we have to consult a table of critical values of the chi-squared distribution. 
The left-most column list the degrees of freedom (df). We determine the degrees of freedom by subtracting one from the number of classes. In this example, we have two classes (heads and tails), so our degrees of freedom is 1. Our chi-squared value is 1.28. Move across the row for 1 df until we find critical numbers that bound our value. In this case, 1.07 (corresponding to a probability of 0.30) and 1.64 (corresponding to a probability of 0.20). We can interpolate our value of 1.24 to estimate a probability of 0.27. This value means that there is a 73% chance that our coin is biased. In other words, the probability of getting 108 heads out of 200 coin tosses with a fair coin is 27%. In biological applications, a probability � 5% is usually adopted as the standard. This value means that the chances of an observed value arising by chance is only 1 in 20. Because the chi-squared value we obtained in the coin example is greater than 0.05 (0.27 to be precise), we accept the null hypothesis as true and conclude that our coin is fair.
"
Probability and Statistics,Miscellaneous,What does Design of Experiments mean?,"Design of experiments also known as DOE, it is the design of any task that aims to describe and explain the variation of information under conditions that are hypothesized to reflect the variable. In essence, an experiment aims to predict an outcome based on a change in one or more inputs (independent variables).
"
Probability and Statistics,Miscellaneous,"Given uniform distribution X and Y (mean 0, SD 1), what’s the probability of 2X > Y?","0.5
"
