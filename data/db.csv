section,question,answer
Classic_models,"What are various ways to predict a binary response variable? Can you compare two of them and tell me when one would be more appropriate? What’s the difference between these? (SVM, Logistic Regression, Naive Bayes, Decision Tree, etc.)","Things to look at: N, P, linearly seperable?, features independent?, likely to overfit?, speed, performance, memory usage
Logistic Regression
features roughly linear, problem roughly linearly separable
robust to noise, use l1, l2 regularization for model selection, avoid overfitting
the output come as probabilities
efficient and the computation can be distributed
can be used as a baseline for other algorithms
(-) can hardly handle categorical features
SVM
with a nonlinear kernel, can deal with problems that are not linearly separable
(-) slow to train, for most industry scale applications, not really efficient
Naive Bayes
computationally efficient when P is large by alleviating the curse of dimensionality
works surprisingly well for some cases even if the condition doesn’t hold
with word frequencies as features, the independence assumption can be seen reasonable. So the algorithm can be used in text categorization
(-) conditional independence of every other feature should be met
Tree Ensembles
good for large N and large P, can deal with categorical features very well
non parametric, so no need to worry about outliers
GBT’s work better but the parameters are harder to tune
RF works out of the box, but usually performs worse than GBT
Deep Learning
works well for some classification tasks (e.g. image)
used to squeeze something out of the problem
"
Classic_models,What methods for solving linear regression do you know?,"To solve linear regression, you need to find the coefficients which minimize the sum of squared errors.
Matrix Algebra method: Let's say you have X, a matrix of features, and y, a vector with the values you want to predict. After going through the matrix algebra and minimization problem, you get this solution:
But solving this requires you to find an inverse, which can be time-consuming, if not impossible. Luckily, there are methods like Singular Value Decomposition (SVD) or QR Decomposition that can reliably calculate this part  (called the pseudo-inverse) without actually needing to find an inverse. The popular python ML library sklearn uses SVD to solve least squares.
Alternative method: Gradient Descent. 
"
Classic_models,Maximum Likelihood Estimation for Linear Regression,"Linear regression can be written as a CPD in the following manner:
For linear regression we assume that μ(x) is linear and so μ(x)=βTx. We must also assume that the variance in the model is fixed (i.e. that it doesn't depend on x) and as such σ2(x)=σ2, a constant. This then implies that our parameter vector θ=(β,σ2).
In linear regression problems we need to make the assumption that the feature vectors are all independent and identically distributed (iid). This makes it far simpler to solve the log-likelihood problem, using properties of natural logarithms. Since we will be differentiating these values it is far easier to differentiate a sum than a product, hence the logarithm:
As I also mentioned in the article on Deep Learning/Logistic Regression, for reasons of increased computational ease, it is often easier to minimise the negative of the log-likelihood rather than maximise the log-likelihood itself. Hence, we can ""stick a minus sign in front of the log-likelihood"" to give us the negative log-likelihood (NLL):
Where RSS(β):=∑i=1N(yi−βTxi)2 is the Residual Sum of Squares, also known as the Sum of Squared Errors (SSE).
Since the first term in the equation is a constant we simply need to concern ourselves with minimising the RSS, which will be sufficient for producing the optimal parameter estimate.
To simply the notation we can write this latter term in matrix form. By defining the N×(p+1) matrix X we can write the RSS term as:
At this stage we now want to differentiate this term w.r.t. the parameter variable 

There is an extremely key assumption to make here. We need XTX to be positive-definite, which is only the case if there are more observations than there are dimensions. If this is not the case (which is extremely common in high-dimensional settings) then it is not possible to find a unique set of β coefficients and thus the following matrix equation will not hold.
Under the assumption of a positive-definite XTX we can set the differentiated equation to zero and solve for β:
The solution to this matrix equation provides 
"
Classic_models,MAP Estimation as Regularization,"Maximum a-posteriori (MAP) method adds a prior distribution of the parameters θ:
θMAP=argmaxθp(x|y,θ)p(θ)
The optimal solution must still match the data but it has also to conform to your prior knowledge about the parameter distribution.
How is this related to adding a regularizer term to a loss function?
Instead of optimizing the posterior directly, one often optimizes negative of the logarithm instead:
θMAP=argminθ−logp(x|y,θ)p(θ)=argminθ−(logp(x|y,θ)+logp(θ))
Assuming you want the parameters θ to be normally distributed around zero, you get logp(θ)∝||θ||^2
Gaussian prior p(θ) = N(0, b^2I) is equivalent to the loss function ∥y − Φθ∥^2 + λ ∥θ∥^2, λ = σ 2/ b 2.
Minimizing the regularized least-squares loss function yields θRLS = (Φ ⊤Φ + λI) ^−1Φ ⊤ y
"
Classic_models,Bayesian Linear Regression,"Bayesian linear regression pushes the idea of the parameter prior a step regression further and does not even attempt to compute a point estimate of the parameters, but instead the full posterior distribution over the parameters is taken into account when making predictions. This means we do not fit any parameters, but we compute a mean over all plausible parameters settings (according to the posterior).
In Bayesian linear regression, we consider the model 
prior p(θ) = N(m0, S0),
likelihood p(y|x, θ) = N(y| ϕ ⊤ (x)θ, σ2)
where we now explicitly place a Gaussian prior on θ, which turns the parameter vector into a random variable.
In practice, we are usually not so much interested in the parameter values θ themselves. Instead, our focus often lies in the predictions we make with those parameter values. In a Bayesian setting, we take the parameter distribution and average over all plausible parameter settings when we make predictions. More specifically, to make predictions at an input x∗, we integrate out θ.
"
Classic_models,What is Logistic Regression?,"Logistic regression is used for binary classification where we use sigmoid function, that takes input as independent variables and produces a probability value between 0 and 1.
The logistic regression model transforms the linear regression function continuous value output into categorical value output using a sigmoid function, which maps any real-valued set of independent variables input into a value between 0 and 1. This function is known as the logistic function.
Let the independent input features be X  and the dependent variable is Y having only binary value i.e. 0 or 1, then, apply the multi-linear function to the input variables X.
z=w⋅X+b
Now we use the sigmoid function where the input will be z and we find the probability between 0 and 1. i.e. predicted y.
P(y=1)=σ(z)
P(y=0)=1−σ(z)
The odd is the ratio of something occurring to something not occurring. it is different from probability as the probability is the ratio of something occurring to everything that could possibly occur. so odd will be:
p(x)/(1−p(x)) =e^z
The final logistic regression equation will be:
p(X;b,w)=1/(1+e^(-w⋅X+b))

"
Classic_models,What is the difference between odds and probability?,"The probability that an event will occur is the fraction of times you expect to see that event in many trials. Probabilities always range between 0 and 1.
The odds are defined as the probability that the event will occur divided by the probability that the event will not occur.
"
Classic_models,Logistic Regression Parameter Interpretation,"log  p / 1−p  = α + β1x1 + β2x2, where x1 is binary (as before) and x2 is a continuous predictor.
e^β1 is the relative increase in the odds, going from x1 = 0 to x1 = 1 holding x2 fixed
If you increase x2 from k to k + ∆, then odds increase e^β2∆ times.
"
Classic_models,Assumptions of Logistic Regression,"We will explore the assumptions of logistic regression as understanding these assumptions is important to ensure that we are using appropriate application of the model. The assumption include:
Independent observations: Each observation is independent of the other, meaning there is no correlation between any input variables.
Binary dependent variables: It takes the assumption that the dependent variable must be binary or dichotomous, meaning it can take only two values. For more than two categories SoftMax functions are used.
Linearity relationship between independent variables and log odds: The relationship between the independent variables and the log odds of the dependent variable should be linear.
No outliers: There should be no outliers in the dataset.
Large sample size: The sample size is sufficiently large
"
Classic_models,What distinguishes Logistic Regression from Linear Regression?,"While Linear Regression is used to predict continuous outcomes, Logistic Regression is used to predict the likelihood of an observation falling into a specific category. Logistic Regression employs an S-shaped logistic function to map predicted values between 0 and 1.
"
Classic_models,Nearest Neighbors Method,"The nearest neighbors method (k-Nearest Neighbors, or k-NN) is another very popular classification method that is also sometimes used in regression problems. This, like decision trees, is one of the most comprehensible approaches to classification. The underlying intuition is that you look like your neighbors. More formally, the method follows the compactness hypothesis: if the distance between the examples is measured well enough, then similar examples are much more likely to belong to the same class.

To classify each sample from the test set, one needs to perform the following operations in order:
Calculate the distance to each of the samples in the training set.
Select k samples from the training set with the minimal distance to them.
The class of the test sample will be the most frequent class among those k nearest neighbors.
"
Classic_models,Nearest Neighbors Method in Real Applications,"k-NN can serve as a good starting point (baseline) in some cases;
In Kaggle competitions, k-NN is often used for the construction of meta-features (i.e. k-NN predictions as input to other models) or for stacking/blending;
The nearest neighbors method extends to other tasks like recommendation systems. The initial decision could be a recommendation of a product (or service) that is popular among the closest neighbors of the person for whom we want to make a recommendation;
In practice, on large datasets, approximate methods of search are often used for nearest neighbors. There is a number of open source libraries that implement such algorithms; check out Spotify’s library Annoy.
The quality of classification/regression with k-NN depends on several parameters:
The number of neighbors k.
The distance measure between samples (common ones include Hamming, Euclidean, cosine, and Minkowski distances). Note that most of these metrics require data to be scaled. Simply speaking, we do not want the “salary” feature, which is on the order of thousands, to affect the distance more than “age”, which is generally less than 100.
Weights of neighbors (each neighbor may contribute different weights; for example, the further the sample, the lower the weight).
"
Classic_models,Pros and Cons of The nearest neighbors method,"Pros:
Simple implementation;
Well studied;
Typically, the method is a good first solution not only for classification or regression, but also recommendations;
It can be adapted to a certain problem by choosing the right metrics or kernel (in a nutshell, the kernel may set the similarity operation for complex objects such as graphs while keeping the k-NN approach the same). By the way, Alexander Dyakonov, a former top-1 kaggler, loves the simplest k-NN but with the tuned object similarity metric;
Good interpretability. There are exceptions: if the number of neighbors is large, the interpretability deteriorates (“We did not give him a loan, because he is similar to the 350 clients, of which 70 are the bad, and that is 12% higher than the average for the dataset”).
Cons:
Method considered fast in comparison with compositions of algorithms, but the number of neighbors used for classification is usually large (100-150) in real life, in which case the algorithm will not operate as fast as a decision tree.
If a dataset has many variables, it is difficult to find the right weights and to determine which features are not important for classification/regression.
Dependency on the selected distance metric between the objects. Selecting the Euclidean distance by default is often unfounded. You can find a good solution by grid searching over parameters, but this becomes very time consuming for large datasets.
There are no theoretical ways to choose the number of neighbors – only grid search (though this is often true for all hyperparameters of all models). In the case of a small number of neighbors, the method is sensitive to outliers, that is, it is inclined to overfit.
As a rule, it does not work well when there are a lot of features due to the “curse of dimensionality”. Professor Pedro Domingos, a well-known member in the ML community, talks about it here in his popular paper, “A Few Useful Things to Know about Machine Learning”; also “the curse of dimensionality” is described in the Deep Learning book in this chapter.
"
Classic_models,Tree-building Algorithm,"At the heart of the popular algorithms for decision tree construction, such as ID3 or C4.5, lies the principle of greedy maximization of information gain: at each step, the algorithm chooses the variable that gives the greatest information gain upon splitting. Then the procedure is repeated recursively until the entropy is zero (or some small value to account for overfitting). Different algorithms use different heuristics for “early stopping” or “cut-off” to avoid constructing an overfitted tree.
An attribute with the highest information gain from a set should be selected as the parent (root) node.
Build child nodes for every value of attribute A.
Repeat iteratively until you finish constructing the whole tree.
"
Classic_models,Information Gain,"We can define information gain as a measure of how much information a feature provides about a class. Information gain helps to determine the order of attributes in the nodes of a decision tree.
The main node is referred to as the parent node, whereas sub-nodes are known as child nodes. We can use information gain to determine how good the splitting of nodes in a decision tree.
It can help us determine the quality of splitting, as we shall soon see. The calculation of information gain should help us understand this concept better.
Gain=Eparent−Echildren
The term Gain represents information gain. EparentEparent is the entropy of the parent node and E_{children} is the average entropy of the child nodes. Let’s use an example to visualize information gain and its calculation.
The more the entropy removed, the greater the information gain. The higher the information gain, the better the split.
"
Classic_models,Pros and Cons of Decision trees,"Pros:
Generation of clear human-understandable classification rules, e.g. “if age <25 and is interested in motorcycles, deny the loan”. This property is called interpretability of the model.
Decision trees can be easily visualized, i.e. both the model itself (the tree) and prediction for a certain test object (a path in the tree) can “be interpreted”.
Fast training and forecasting.
Small number of model parameters.
Supports both numerical and categorical features.
Cons:
The trees are very sensitive to the noise in input data; the whole model could change if the training set is slightly modified (e.g. remove a feature, add some objects). This impairs the interpretability of the model.
Separating border built by a decision tree has its limitations – it consists of hyperplanes perpendicular to one of the coordinate axes, which is inferior in quality to some other methods, in practice.
We need to avoid overfitting by pruning, setting a minimum number of samples in each leaf, or defining a maximum depth for the tree. Note that overfitting is an issue for all machine learning methods.
Instability. Small changes to the data can significantly change the decision tree. This problem is tackled with decision tree ensembles (discussed next time).
The optimal decision tree search problem is NP-complete. Some heuristics are used in practice such as greedy search for a feature with maximum information gain, but it does not guarantee finding the globally optimal tree.
Difficulties to support missing values in the data. Friedman estimated that it took about 50% of the code to support gaps in data in CART (an improved version of this algorithm is implemented in sklearn).
The model can only interpolate but not extrapolate (the same is true for random forests and tree boosting). That is, a decision tree makes constant prediction for the objects that lie beyond the bounding box set by the training set in the feature space. In our example with the yellow and blue balls, it would mean that the model gives the same predictions for all balls with positions >19 or <0.
"
Classic_models,Random Forest,"The random forest algorithm is an expansion of decision tree, in that you first construct a multitude of decision trees with training data, then fit your new data within one of the trees as a “random forest.”
It, essentially, averages your data to connect it to the nearest tree on the data scale. Random forest models are helpful as they remedy for the decision tree’s problem of “forcing” data points within a category unnecessarily. 
While an individual tree is overfit to the training data and is likely to have large error, bagging (Bootstrap Aggregating) uses the insight that a suitably large number of uncorrelated errors average out to zero to solve this problem. Bagging chooses multiple random samples of observations from the training data, with replacement, constructing a tree from each one. Since each tree learns from different data, they are fairly uncorrelated from one another. Plotting the R² of our model as we increase the number of “bagged” trees ( scikit-learn calls these trees estimators) illustrates the power of this technique.
"
Classic_models,What clustering algorithms do you know? ,"k-medoids: Takes the most central point instead of the mean value as the center of the cluster. This makes it more robust to noise.
Agglomerative Hierarchical Clustering (AHC): hierarchical clusters combining the nearest clusters starting with each point as its own cluster.
DIvisive ANAlysis Clustering (DIANA): hierarchical clustering starting with one cluster containing all points and splitting the clusters until each point describes its own cluster.
Density-Based Spatial Clustering of Applications with Noise (DBSCAN): Cluster defined as maximum set of density-connected points.
"
Classic_models,How does DBScan work?,"Two input parameters epsilon (neighborhood radius) and minPts (minimum number of points in an epsilon-neighborhood)
Cluster defined as maximum set of density-connected points.
Points p_j and p_i are density-connected w.r.t. epsilon and minPts if there is a point o such that both, i and j are density-reachable from o w.r.t. epsilon and minPts.
p_j is density-reachable from p_i w.r.t. epsilon, minPts if there is a chain of points p_i -> p_i+1 -> p_i+x = p_j such that p_i+x is directly density-reachable from p_i+x-1.
p_j is a directly density-reachable point of the neighborhood of p_i if dist(p_i,p_j) <= epsilon.
"
Classic_models,When would you choose K-means and when DBScan? ,"DBScan is more robust to noise.
DBScan is better when the amount of clusters is difficult to guess.
K-means has a lower complexity, i.e. it will be much faster, especially with a larger amount of points.
"
Classic_models,What reduction techniques do you know? ,"Singular Value Decomposition (SVD)
Principal Component Analysis (PCA)
Linear Discriminant Analysis (LDA)
T-distributed Stochastic Neighbor Embedding (t-SNE)
Autoencoders
Fourier and Wavelet Transforms
"
Classic_models,What’s singular value decomposition? How is it typically used for machine learning? ,"Singular Value Decomposition (SVD) is a general matrix decomposition method that factors a matrix X into three matrices L (left singular values), Σ (diagonal matrix) and R^T (right singular values).
For machine learning, Principal Component Analysis (PCA) is typically used. It is a special type of SVD where the singular values correspond to the eigenvectors and the values of the diagonal matrix are the squares of the eigenvalues. We use these features as they are statistically descriptive.
Having calculated the eigenvectors and eigenvalues, we can use the Kaiser-Guttman criterion, a scree plot or the proportion of explained variance to determine the principal components (i.e. the final dimensionality) that are useful for dimensionality reduction
"
Classic_models,Which models do you know for solving time series problems? ,"Simple Exponential Smoothing: approximate the time series with an exponential function
Trend-Corrected Exponential Smoothing (Holt‘s Method): exponential smoothing that also models the trend
Trend- and Seasonality-Corrected Exponential Smoothing (Holt-Winter‘s Method): exponential smoothing that also models trend and seasonality
Time Series Decomposition: decomposed a time series into the four components trend, seasonal variation, cycling variation and irregular component
Autoregressive models: similar to multiple linear regression, except that the dependent variable y_t depends on its own previous values rather than other independent variables.
Deep learning approaches (RNN, LSTM, etc.)
"
Classic_models,What are the problems with using trees for solving time series problems? ,"Random Forest models are not able to extrapolate time series data and understand increasing/decreasing trends. It will provide us with average data points if the validation data has values greater than the training data points.
"
Classic_models,Why might it be preferable to include fewer predictors over many?,"When we add irrelevant features, it increases model's tendency to overfit because those features introduce more noise. When two variables are correlated, they might be harder to interpret in case of regression, etc.
curse of dimensionality
adding random noise makes the model more complicated but useless
computational cost
"
Classic_models,What are some ways I can make my model more robust to outliers?,"We can have regularization such as L1 or L2 to reduce variance (increase bias).
Changes to the algorithm:
Use tree-based methods instead of regression methods as they are more resistant to outliers. For statistical tests, use non parametric tests instead of parametric ones.
Use robust error metrics such as MAE or Huber Loss instead of MSE.
Changes to the data:
Winsorizing the data
Transforming the data (e.g. log)
Remove them only if you’re certain they’re anomalies not worth predicting
"
Classic_models,Your linear regression didn’t run and communicates that there are an infinite number of best estimates for the regression coefficients. What could be wrong?,"p > n.
If some of the explanatory variables are perfectly correlated (positively or negatively) then the coefficients would not be unique. 
"
Classic_models,"You have several variables that are positively correlated with your response, and you think combining all of the variables could give you a good prediction of your response. However, you see that in the multiple linear regression, one of the weights on the predictors is negative. What could be the issue?","Multicollinearity refers to a situation in which two or more explanatory variables in a multiple regression model are highly linearly related. 
Leave the model as is, despite multicollinearity. The presence of multicollinearity doesn't affect the efficiency of extrapolating the fitted model to new data provided that the predictor variables follow the same pattern of multicollinearity in the new data as in the data on which the regression model is based.
principal component regression
"
Classic_models,What is the effect on the coefficients of logistic regression if two predictors are highly correlated? ,"When predictor variables are correlated, the estimated regression coefficient of any one variable depends on which other predictor variables are included in the model. When predictor variables are correlated, the precision of the estimated regression coefficients decreases as more predictor variables are added to the model.
In statistics, multicollinearity (also collinearity) is a phenomenon in which two or more predictor variables in a multiple regression model are highly correlated, meaning that one can be linearly predicted from the others with a substantial degree of accuracy. In this situation the coefficient estimates of the multiple regression may change erratically in response to small changes in the model or the data. Multicollinearity does not reduce the predictive power or reliability of the model as a whole, at least within the sample data set; it only affects calculations regarding individual predictors. That is, a multiple regression model with correlated predictors can indicate how well the entire bundle of predictors predicts the outcome variable, but it may not give valid results about any individual predictor, or about which predictors are redundant with respect to others.
The consequences of multicollinearity:
Ratings estimates remain unbiased.
Standard coefficient errors increase.
The calculated t-statistics are underestimated.
Multicollinearity reduces the precision of the estimated coefficients, which weakens the statistical power of your regression model. You might not be able to trust the p-values to identify independent variables that are statistically significant.
Estimates become very sensitive to changes in specifications and changes in individual observations.
The overall quality of the equation, as well as estimates of variables not related to multicollinearity, remain unaffected.
The closer multicollinearity to perfect (strict), the more serious its consequences.
Indicators of multicollinearity:
High R2 and negligible odds.
Strong pair correlation of predictors.
Strong partial correlations of predictors.
High VIF - variance inflation factor.
"
Classic_models,What’s the difference between Gaussian Mixture Model and K-Means?,"Let's says we are aiming to break them into three clusters. K-means will start with the assumption that a given data point belongs to one cluster.
Choose a data point. At a given point in the algorithm, we are certain that a point belongs to a red cluster. In the next iteration, we might revise that belief, and be certain that it belongs to the green cluster. However, remember, in each iteration, we are absolutely certain as to which cluster the point belongs to. This is the ""hard assignment"".
What if we are uncertain? What if we think, well, I can't be sure, but there is 70% chance it belongs to the red cluster, but also 10% chance its in green, 20% chance it might be blue. That's a soft assignment. The Mixture of Gaussian model helps us to express this uncertainty. It starts with some prior belief about how certain we are about each point's cluster assignments. As it goes on, it revises those beliefs. But it incorporates the degree of uncertainty we have about our assignment.
Kmeans: find kk to minimize (x−μk)^2
Gaussian Mixture (EM clustering) : find kk to minimize (x−μk)^2/σ^2
The difference (mathematically) is the denominator “σ^2”, which means GM takes variance into consideration when it calculates the measurement. Kmeans only calculates conventional Euclidean distance. In other words, Kmeans calculate distance, while GM calculates “weighted” distance.
K means:
Hard assign a data point to one particular cluster on convergence.
It makes use of the L2 norm when optimizing (Min {Theta} L2 norm point and its centroid coordinates).
EM:
Soft assigns a point to clusters (so it give a probability of any point belonging to any centroid).
It doesn't depend on the L2 norm, but is based on the Expectation, i.e., the probability of the point belonging to a particular cluster. This makes K-means biased towards spherical clusters.
"
Classic_models,Bootstrapping,"The bootstrap method goes as follows. Let there be a sample X of size N. We can make a new sample from the original sample by drawing N elements from the latter randomly and uniformly, with replacement. In other words, we select a random element from the original sample of size 
 and do this N times. All elements are equally likely to be selected, thus each element is drawn with the equal probability 1/N.
By repeating this procedure M times, we create M bootstrap samples X1,…XM. In the end, we have a sufficient number of samples and can compute various statistics of the original distribution.
"
Classic_models,Bagging,"Suppose that we have a training set X. Using bootstrapping, we generate samples X1,…,XM. Now, for each bootstrap sample, we train its own classifier ai(x). The final classifier will average the outputs from all these individual classifiers. In the case of classification, this technique corresponds to voting: 
Bagging reduces the variance of a classifier by decreasing the difference in error when we train the model on different datasets. In other words, bagging prevents overfitting. The efficiency of bagging comes from the fact that the individual models are quite different due to the different training data and their errors cancel each other out during voting. Additionally, outliers are likely omitted in some of the training bootstrap samples.
Bagging is effective on small datasets. Dropping even a small part of training data leads to constructing substantially different base classifiers. If you have a large dataset, you would generate bootstrap samples of a much smaller size.
"
Classic_models,Out-of-bag error,"Looking ahead, in case of Random Forest, there is no need to use cross-validation or hold-out samples in order to get an unbiased error estimation. Why? Because, in ensemble techniques, the error estimation takes place internally.
Random trees are constructed using different bootstrap samples of the original dataset. Approximately 37% of inputs are left out of a particular bootstrap sample and are not used in the construction of the k-th tree.
This is easy to prove. Suppose there are ℓ examples in our dataset. At each step, each data point has equal probability of ending up in a bootstrap sample with replacement, probability 1/ℓ. The probability that there is no such bootstrap sample that contains a particular dataset element (i.e. it has been omitted ℓ times) equals (1−1/ℓ)^ℓ. When ℓ→+∞, it becomes equal to the Second Remarkable Limit 1e. Then, the probability of selecting a specific example is ≈1−1e≈63%.
The Out-of-Bag error is then computed in the following way:
take all instances that have been chosen as a part of test set for some tree (in the picture above that would be all instances in the lower-right picture). All together, they form an Out-of-Bag dataset;
take a specific instance from the Out-of-Bag dataset and all models (trees) that were not trained with this instance;
compare the majority vote of these trees’ classifications and compare it with the true label for this instance;
do this for all instances in the Out-of-Bag dataset and get the average OOB error.
"
Classic_models,Difference between AdaBoost and XGBoost.,"Both methods combine weak learners into one strong learner. For example, one decision tree is a weak learner, and an emsemble of them would be a random forest model, which is a strong learner.
Both methods in the learning process will increase the ensemble of weak-trainers, adding new weak learners to the ensemble at each training iteration, i.e. in the case of the forest, the forest will grow with new trees. The only difference between AdaBoost and XGBoost is how the ensemble is replenished.
AdaBoost works by weighting the observations, putting more weight on difficult to classify instances and less on those already handled well. New weak learners are added sequentially that focus their training on the more difficult patterns. AdaBoost at each iteration changes the sample weights in the sample. It raises the weight of the samples in which more mistakes were made. The sample weights vary in proportion to the ensemble error. We thereby change the probabilistic distribution of samples - those that have more weight will be selected more often in the future. It is as if we had accumulated samples on which more mistakes were made and would use them instead of the original sample. In addition, in AdaBoost, each weak learner has its own weight in the ensemble (alpha weight) - this weight is higher, the “smarter” this weak learner is, i.e. than the learner least likely to make mistakes.
XGBoost does not change the selection or the distribution of observations at all. XGBoost builds the first tree (weak learner), which will fit the observations with some prediction error. A second tree (weak learner) is then added to correct the errors made by the existing model. Errors are minimized using a gradient descent algorithm. Regularization can also be used to penalize more complex models through both Lasso and Ridge regularization.
In short, AdaBoost- reweighting examples. Gradient boosting - predicting the loss function of trees. Xgboost - the regularization term was added to the loss function (depth + values in leaves).
"
Classic_models,Kernel function,"Kernel functions are generalized dot product functions used for the computing dot product of vectors x and y in high dimensional feature space. Kernal trick method is used for solving a non-linear problem by using a linear classifier by transforming linearly inseparable data into separable ones in higher dimensions.
If the data is not linearly separable in the original, or input, space then we apply transformations to the data, which map the data from the original space into a higher dimensional feature space. The goal is that after the transformation to the higher dimensional space, the classes are now linearly separable in this higher dimensional feature space. We can then fit a decision boundary to separate the classes and make predictions. The decision boundary will be a hyperplane in this higher dimensional space.
"
Classic_models,How are the time series problems different from other regression problems?,"Time series data can be thought of as an extension to linear regression which uses terms like autocorrelation, movement of averages for summarizing historical data of y-axis variables for predicting a better future.
Forecasting and prediction is the main goal of time series problems where accurate predictions can be made but sometimes the underlying reasons might not be known.
Having Time in the problem does not necessarily mean it becomes a time series problem. There should be a relationship between target and time for a problem to become a time series problem.
The observations close to one another in time are expected to be similar to the ones far away which provide accountability for seasonality. For instance, today’s weather would be similar to tomorrow’s weather but not similar to weather from 4 months from today. Hence, weather prediction based on past data becomes a time series problem.
Instead of adding fully connected layers on top of the feature maps, it takes the average of each feature map, and the resulting vector is fed directly into the softmax layer. One advantage of global average pooling over the fully connected layers is that it is more native to the convolution structure by enforcing correspondences between feature maps and categories.
Another advantage is that there is no parameter to optimize in the global average pooling thus overfitting is avoided at this layer. Global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input. We can see global average pooling as a structural regularizer that explicitly enforces feature maps to be confidence maps of concepts (categories).
"
Classic_models,What are various assumptions used in linear regression? What would happen if they are violated?,"Linear regression is done under the following assumptions:
The sample data used for modeling represents the entire population.
There exists a linear relationship between the X-axis variable and the mean of the Y variable.
The residual variance is the same for any X values. This is called homoscedasticity. Residual Variance (also called unexplained variance or error variance) is the variance of any error (residual).
The errors or residuals of the data are normally distributed and independent from each other. 
There is minimal multicollinearity between explanatory variables 
Extreme violations of the above assumptions lead to redundant results. Smaller violations of these result in greater variance or bias of the estimates.
"
Classic_models,What Is a Linear Regression Model? List Its Drawbacks.,"A linear regression model is a model in which there is a linear relationship between the dependent and independent variables. 
Here are the drawbacks of linear regression: 
Only the mean of the dependent variable is taken into consideration. 
It assumes that the data is independent. 
The method is sensitive to outlier data values. 
"
Classic_models,Decision Forest,"The algorithm for constructing a random forest of N trees goes as follows:
For each k=1,…,N:
Generate a bootstrap sample Xk.
Build a decision tree bk on the sample Xk:
Pick the best feature according to the given criteria. Split the sample by this feature to create a new tree level. Repeat this procedure until the sample is exhausted.
Building the tree until any of its leaves contains no more than nmin instances or until a certain depth is reached.
For each split, we first randomly pick m features from the d original ones and then search for the next best split only among the subset.
The final classifier is defined by:
a(x)=1N∑k=1Nbk(x)
We use the majority voting for classification and the mean for regression.
For classification problems, it is advisable to set m=d. For regression problems, we usually take m=d3, where d is the number of features. It is recommended to build each tree until all of its leaves contain only nmin=1 examples for classification and nmin=5 examples for regression.
You can see random forest as bagging of decision trees with the modification of selecting a random subset of features at each split.
The main difference between random forests and bagging is that, in a random forest, the best feature for a split is selected from a random subset of the available features while, in bagging, all features are considered for the next best split.
Decision boundary of the decision tree is quite jagged and has a lot of acute angles that suggest overfitting and a weak ability to generalize. We would have trouble making reliable predictions on new test data. In contrast, the bagging algorithm has a rather smooth boundary and has no obvious signs of overfitting.
"
Classic_models,Parameters of Random Forest,"n_estimators — the number of trees in the forest (default = 10)
criterion — the function used to measure the quality of a split. Supported criteria are “mse” for the mean squared error, which is equal to variance reduction as feature selection criterion, and “mae” for the mean absolute error (default = “mse”)
max_features — the number of features to consider when looking for the best split. You can specify the number or percentage of features, or choose from the available values: “auto” (all features), “sqrt”, “log2”. (default = “auto”)
max_depth — the maximum depth of the tree (default means that nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples)
min_samples_split — the minimum number of samples required to split an internal node. Can be specified as the number or as a percentage of a total number of samples (default = 2)
min_samples_leaf — the minimum number of samples required at a leaf node(default = 1)
min_weight_fraction_leaf — the minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided (default = 0)
max_leaf_nodes — the maximum number of leaves (default = no restrictions)
min_impurity_split — threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf (default = 1е-7)
bootstrap — whether bootstrap samples are used when building trees(default = True)
oob_score — whether to use out-of-bag samples to estimate the R^2 on unseen data (default = False)
n_jobs — the number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores (default = 1)
random_state — if int, random_state is the seed used by the random number generator; if RandomState instance, random_state is the random number generator; if None, the random number generator is the RandomState instance used by np.random (default = None)
verbose — controls the verbosity of the tree building process (default = 0)
warm_start — when set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest (default = False)
In case of classification, parameters are mostly the same. Only the following differ for RandomForestClassifier as compared to RandomForestRegressor:
criterion — the function used to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific (default = “gini”)
class_weight — the weight of each class (by default all weights equal to 1, but you can create a dictionary with weights or specify it as “balanced” - uses the values of classes to automatically adjust weights inversely proportional to class frequencies in the input data or as “balanced_subsample” - the same as “balanced” except that weights are computed based on the bootstrap sample for every tree grown)
Below are the parameters which we need to pay attention to when we are building a new model:
n_estimators — the number of trees in the forest;
criterion — the function used to measure the quality of a split;
max_features — the number of features to consider when looking for the best split;
min_samples_leaf — the minimum number of samples required to be at a leaf node;
max_depth — the maximum depth of the tree.
"
Classic_models,Variance and Decorrelation of Random Forests,"Let’s write the variance of a random forest as
Var f(x)=ρ(x)σ2(x)
ρ(x)=Corr[T(x1,Θ1(Z)),T(x2,Θ2(Z))],
where
ρ(x) is the sample correlation coefficient between any two trees used in averaging:
Θ1(Z) and Θ2(Z) are a randomly selected pair of trees on randomly selected elements of the sample Z;
T(x,Θi(Z)) is the output of the i-th tree classifier on an input vector x;
σ2(x) is the sample variance of any randomly selected tree:
σ2(x)=Var[T(x,Θ(X))]
It is easy to confuse ρ(x) with the average correlation between the trained trees in a given random forest when we consider trees as N-vectors and calculate the average pairwise correlation between them. But this is not the case.
In fact, this conditional correlation is not directly related to the averaging process, and the dependence of ρ(x) on x warns us of this difference. ρ(x) is the theoretical correlation between a pair of random trees estimated on the input x. Its value comes from the repeated sampling of the training set from the population Z and the subsequent random choice of a pair of trees. In statistics jargon, this is the correlation caused by the sampling distribution of Z and Θ.
The conditional covariance of any pair of trees is equal to 0 because bootstrapping and feature selection are independent and identically distributed.
If we consider the variance of a single tree, it barely depends on the parameters of the splitting (m). But they are crucial for ensembles. The variance of a tree is much higher than the one of an ensemble. The book The Elements of Statistical Learning (Trevor Hastie, Robert Tibshirani and Jerome Friedman) has a great example that demonstrates this fact:
Just as in bagging, the bias of a random forest is the same as the bias of a single tree T(x,Θ(Z)):
In absolute value, the bias is usually higher than that of an unpruned tree because randomization and sample space reduction impose their own restrictions on the model. Therefore, the improvements in prediction accuracy obtained by bagging and random forests are solely the result of variance reduction.
"
Classic_models,Pros and cons of random forests,"Pros:
High prediction accuracy; will perform better than linear algorithms in most problems; the accuracy is comparable with that of boosting.
Robust to outliers, thanks to random sampling.
Insensitive to the scaling of features as well as any other monotonic transformations due to the random subspace selection.
Doesn’t require fine-grained parameter tuning, works quite well out-of-the-box. With tuning, it is possible to achieve a 0.5–3% gain in accuracy, depending on the problem setting and data.
Efficient for datasets with a large number of features and classes.
Handles both continuous and discrete variables equally well.
Rarely overfits. In practice, an increase in the tree number almost always improves the composition. But, after reaching a certain number of trees, the learning curve is very close to the asymptote.
There are developed methods to estimate feature importance.
Works well with missing data and maintains good accuracy even when a large part of data is missing.
Provides means to weight classes on the whole dataset as well as for each tree sample.
Under the hood, calculates proximities between pairs of examples that can subsequently be used in clustering, outlier detection, or interesting data representations.
The above functionality and properties may be extended to unlabeled data to enable unsupervised clustering, data visualization, and outlier detection.
Easily parallelized and highly scalable.
Cons:
In comparison with a single decision tree, Random Forest’s output is more difficult to interpret.
There are no formal p-values for feature significance estimation.
Performs worse than linear methods in the case of sparse data: text inputs, bag of words, etc.
Unlike linear regression, Random Forest is unable to extrapolate. But, this can be also regarded as an advantage because outliers do not cause extreme values in Random Forests.
Prone to overfitting in some problems, especially, when dealing with noisy data.
In the case of categorical variables with varying level numbers, random forests favor variables with a greater number of levels. The tree will fit more towards a feature with many levels because this gains greater accuracy.
If a dataset contains groups of correlated features, preference might be given to groups of smaller size (“correlation bias”). See this work
The resulting model is large and requires a lot of RAM.
"
Classic_models,How can you select k for k-means? ,"We use the elbow method to select k for k-means clustering. The idea of the elbow method is to run k-means clustering on the data set where 'k' is the number of clusters.
Within the sum of squares (WSS), it is defined as the sum of the squared distance between each member of the cluster and its centroid. 
Siloutte score.
For each point compute the score. 
Score = (b - a)/ max(a,b)
a = intra cluster distance 
b = inter cluster distance for nearest cluster for that point. 
Do this for all points and average. 
Pick the one with max. average siloutte score
"
Classic_models,Describe Markov chains?,"Markov Chains defines that a state’s future probability depends only on its current state. 
Markov chains belong to the Stochastic process type category.
A perfect example of the Markov Chains is the system of word recommendation. In this system, the model recognizes and recommends the next word based on the immediately previous word and not anything before that. The Markov Chains take the previous paragraphs that were similar to training data-sets and generates the recommendations for the current paragraphs accordingly based on the previous word.
"
Classic_models,Difference between an error and a residual error,"The error of an observation is the deviation of the observed value from the true value of a quantity of interest (for example, a population mean). The residual is the difference between the observed value and the estimated value of the quantity of interest (for example, a sample mean).
"
Classic_models,What are the differences between Supervised and Unsupervised Learning?,"Supervised learning is a type of machine learning where a function is inferred from labeled training data. The training data contains a set of training examples.
Unsupervised learning, on the other hand, is when inferences are drawn from datasets containing input data without labeled responses.
The following are the various other differences between the two types of machine learning:
"
Classic_models,What is the Computational Graph?,"A computational graph is a graphical presentation that is based on TensorFlow. It has a wide network of different kinds of nodes wherein each node represents a particular mathematical operation. The edges in these nodes are called tensors. This is the reason the computational graph is called a TensorFlow of inputs. The computational graph is characterized by data flows in the form of a graph; therefore, it is also called the DataFlow Graph.
Forward pass is the procedure for evaluating the value of the mathematical expression represented by computational graphs. Doing forward pass means we are passing the value from variables in forward direction from the left (input) to the right where the output is.
In the backward pass, our intention is to compute the gradients for each input with respect to the final output. These gradients are essential for training the neural network using gradient descent.
"
Classic_models,What is the difference between a discriminative and a generative model?,"A discriminative model learns distinctions between different categories of data. A generative model learns categories of data. Discriminative models generally perform better on classification tasks.
In General, A Discriminative model ‌models the decision boundary between the classes. A Generative Model ‌explicitly models the actual distribution of each class. In final both of them is0 predicting the conditional probability P(Animal | Features). But Both models learn different probabilities.
A Generative Model ‌learns the joint probability distribution p(x,y). It predicts the conditional probability with the help of Bayes Theorem. A Discriminative model ‌learns the conditional probability distribution p(y|x). Both of these models were generally used in supervised learning problems.
Generative classifiers
Assume some functional form for P(Y), P(X|Y)
Estimate parameters of P(X|Y), P(Y) directly from training data
Use Bayes rule to calculate P(Y |X)
Discriminative Classifiers
Assume some functional form for P(Y|X)
Estimate parameters of P(Y|X) directly from training data
Generative classifiers
‌Naïve Bayes
Bayesian networks
Markov random fields
‌Hidden Markov Models (HMM)
Discriminative Classifiers
‌Logistic regression
Scalar Vector Machine
‌Traditional neural networks
‌Nearest neighbour
Conditional Random Fields (CRF)s
"
Classic_models,What are parametric models? Provide an example.,"Parametric models have a finite number of parameters. You only need to know the parameters of the model to make a data prediction. Common examples are as follows: 
Logistic Regression
Linear Discriminant Analysis
Perceptron
Naive Bayes
Simple Neural Networks
A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) is called a parametric model. No matter how much data you throw at a parametric model, it won’t change its mind about how many parameters it needs.
Non-parametric models have an unbounded number of parameters to offer flexibility. For data predictions, you need the parameters of the model and the state of the observed data. Common examples are as follows: 
k-Nearest Neighbors
Decision Trees like CART and C4.5
Support Vector Machines
Nonparametric methods are good when you have a lot of data and no prior knowledge, and when you don’t want to worry too much about choosing just the right features.
An easy to understand nonparametric model is the k-nearest neighbors algorithm that makes predictions based on the k most similar training patterns for a new data instance. The method does not assume anything about the form of the mapping function other than patterns that are close are likely to have a similar output variable.
"
Classic_models,Linear Discriminant Analysis,"Linear discriminant analysis is used as a tool for classification, dimension reduction, and data visualization. It has been around for quite some time now. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the benchmarking method before other more complicated and flexible ones are employed.
"
Classic_models,Hidden Markov Model vs Recurrent Neural Network,"Hidden Markov Models (HMMs) are much simpler than Recurrent Neural Networks (RNNs), and rely on strong assumptions which may not always be true. If the assumptions are true then you may see better performance from an HMM since it is less finicky to get working.
An RNN may perform better if you have a very large dataset, since the extra complexity can take better advantage of the information in your data. This can be true even if the HMMs assumptions are true in your case.
Finally, don't be restricted to only these two models for your sequence task, sometimes simpler regressions (e.g. ARIMA) can win out, and sometimes other complicated approaches such as Convolutional Neural Networks might be the best. (Yes, CNNs can be applied to some kinds of sequence data just like RNNs.)
As always, the best way to know which model is best is to make the models and measure performance on a held out test set.
Strong Assumptions of HMMs
State transitions only depend on the current state, not on anything in the past.
"
Classic_models,T-distributed Stochastic Neighbor Embedding,"t-SNE is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.
It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples. For more tips see Laurens van der Maaten’s FAQ [2].
"
Classic_models,What kind of regularization techniques are applicable to linear models? ,"AIC/BIC, Ridge regression, Lasso, Elastic Net, Basis pursuit denoising, Rudin–Osher–Fatemi model (TV), Potts model, RLAD, Dantzig Selector, SLOPE
"
Classic_models,When do we need to perform feature normalization for linear models? When it’s okay not to do it? ,"Feature normalization is necessary for L1 and L2 regularizations. The idea of both methods is to penalize all the features relatively equally. This can't be done effectively if every feature is scaled differently.
Linear regression without regularization techniques can be used without feature normalization. Also, regularization can help to make the analytical solution more stable, — it adds the regularization matrix to the feature matrix before inverting it.
"
Classic_models,"Which machine learning algorithm is known as the lazy learner, and why is it called so?","KNN is a Machine Learning algorithm known as a lazy learner. K-NN is a lazy learner because it doesn’t learn any machine-learned values or variables from the training data but dynamically calculates distance every time it wants to classify, hence memorizing the training dataset instead. 
"
Classic_models,"Is it possible to test for the probability of improving model accuracy without cross-validation techniques? If yes, please explain.","Yes, it is possible to test for the probability of improving model accuracy without cross-validation techniques. We can do so by running the ML model for say n number of iterations, recording the accuracy. Plot all the accuracies and remove the 5% of low probability values. Measure the left [low] cut off and right [high] cut off. With the remaining 95% confidence, we can say that the model can go as low or as high [as mentioned within cut off points]. 
"
Classic_models,Name and define techniques used to find similarities in the recommendation system. ,"Pearson correlation and Cosine correlation are techniques used to find similarities in recommendation systems. 
"
Classic_models,Random Forest Feature importance,"Permutation importance. The average reduction in accuracy caused by a variable is determined during the calculation of the out-of-bag error. The greater the reduction in accuracy due to an exclusion or permutation of the variable, the higher its importance score. For this reason, variables with a greater average reduction in accuracy are generally more significant for classification.
Sklearn library uses another approach to determine feature importance. The rationale for that method is that the more gain in information the node (with splitting feature Xj) provides, the higher its importance. The average reduction in the Gini impurity – or MSE for regression – represents the contribution of each feature to the homogeneity of nodes and leaves in the resulting Random Forest model. Each time a selected feature is used for splitting, the Gini impurity of the child nodes is calculated and compared with that of the original node.
"
Classic_models,What is difference between Inference and Prediction?,"Prediction:
- Evaluate a variety of models
- Select the best-performing model
- Empirically determine loss on test set
- Predict the outcome for new samples
- Model interpretability suffers
- Model validity shown for the test set
- Model may overfit if the test data are similar to the training data
Inference:
- Reason about the data generation process
- Select model whose assumptions seem most reasonable
- Use goodneess-of-fit tests
- Use the model to explain the data generation process
- High model interpretability
- Model validity is uncertain since predictive accuracy was not considered
- Overfitting prevented through simplified assumptions
"
Classic_models,State the limitations of Fixed Basis Function,"Linear separability in feature space doesn’t imply linear separability in input space. So, Inputs are non-linearly transformed using vectors of basic functions with increased dimensionality. Limitations of Fixed basis functions are:
Non-Linear transformations cannot remove overlap between two classes but they can increase overlap.
Often it is not clear which basis functions are the best fit for a given task. So, learning the basic functions can be useful over using fixed basis functions.
If we want to use only fixed ones, we can use a lot of them and let the model figure out the best fit but that would lead to overfitting the model thereby making it unstable. 
"
Classic_models,Define and explain the concept of Inductive Bias with some examples,"Inductive Bias is a set of assumptions that humans use to predict outputs given inputs that the learning algorithm has not encountered yet. When we are trying to learn Y from X and the hypothesis space for Y is infinite, we need to reduce the scope by our beliefs/assumptions about the hypothesis space which is also called inductive bias. Through these assumptions, we constrain our hypothesis space and also get the capability to incrementally test and improve on the data using hyper-parameters. Examples:
We assume that Y varies linearly with X while applying Linear regression.
We assume that there exists a hyperplane separating negative and positive examples.
"
Classic_models,Explain the term instance-based learning.,"Instance Based Learning is a set of procedures for regression and classification which produce a class label prediction based on resemblance to its nearest neighbors in the training data set. These algorithms just collects all the data and get an answer when required or queried. In simple words they are a set of procedures for solving new problems based on the solutions of already solved problems in the past which are similar to the current problem.
"
Classic_models,Tree’s Feature Importance from Mean Decrease in Impurity (MDI),"The impurity-based feature importance ranks the numerical features to be the most important features. 
This problem stems from two limitations of impurity-based feature importances:
impurity-based importances are biased towards high cardinality features;
impurity-based importances are computed on training set statistics and therefore do not reflect the ability of feature to be useful to make predictions that generalize to the test set (when the model has enough capacity).
"
Classic_models,Define Perceptron,"An artificial neuron is a mathematical function conceived as a model of biological neurons, that is, a neural network.
A Perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data. It is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).
”Perceptron Learning Rule states that the algorithm would automatically learn the optimal weight coefficients.
Single layer Perceptrons can learn only linearly separable patterns.
Multilayer Perceptron or feedforward neural network with two or more layers have the greater processing power and can process non-linear patterns as well.
Perceptrons can implement Logic Gates like AND, OR, or XOR.
"
Classic_models,Estimating the Noise Variance,"The maximum likelihood estimate of the noise variance is the empirical mean of the squared distances between the noise-free function values ϕ ⊤ (xn)θ and the corresponding noisy observations yn at input locations xn:
σ^2 = 1 / N * sum_n=1..N(yn − ϕ⊤(xn)θ)^2
"
Classic_models,Recommender Systems Approaches,"Like many machine learning techniques, a recommender system makes prediction based on users’ historical behaviors. Specifically, it’s to predict user preference for a set of items based on past experience. To build a recommender system, the most two popular approaches are Content-based and Collaborative Filtering.
Content-based approach requires a good amount of information of items’ own features, rather than using users’ interactions and feedbacks. For example, it can be movie attributes such as genre, year, director, actor etc., or textual content of articles that can extracted by applying Natural Language Processing.
Unlike collaborative methods that only rely on the user-item interactions, content based approaches use additional information about users and/or items. If we consider the example of a movies recommender system, this additional information can be, for example, the age, the sex, the job or any other personal information for users as well as the category, the main actors, the duration or other characteristics for the movies (items).
Collaborative Filtering, on the other hand, doesn’t need anything else except users’ historical preference on a set of items. Because it’s based on historical data, the core assumption here is that the users who have agreed in the past tend to also agree in the future. In terms of user preference, it usually expressed by two categories. Explicit Rating, is a rate given by a user to an item on a sliding scale, like 5 stars for Titanic. This is the most direct feedback from users to show how much they like an item. Implicit Rating, suggests users preference indirectly, such as page views, clicks, purchase records, whether or not listen to a music track, and so on. In this article, I will take a close look at collaborative filtering that is a traditional and powerful tool for recommender systems.
"
Classic_models,Time Series,"Time series is a sequence of observations of categorical or numeric variables indexed by a date, or timestamp. A clear example of time series data is the time series of a stock price. In the following table, we can see the basic structure of time series data. In this case the observations are recorded every hour.
Normally, the first step in time series analysis is to plot the series, this is normally done with a line chart.
The most common application of time series analysis is forecasting future values of a numeric value using the temporal structure of the data. This means, the available observations are used to predict values from the future.
The temporal ordering of the data, implies that traditional regression methods are not useful. In order to build robust forecast, we need models that take into account the temporal ordering of the data.
The most widely used model for Time Series Analysis is called Autoregressive Moving Average (ARMA). The model consists of two parts, an autoregressive (AR) part and a moving average (MA) part. The model is usually then referred to as the ARMA(p, q) model where p is the order of the autoregressive part and q is the order of the moving average part.
"
Classic_models,Components of Time Series Analysis,"Trend
Seasonality
Cyclical
Irregularity
Trend: In which there is no fixed interval and any divergence within the given dataset is a continuous timeline. The trend would be Negative or Positive or Null Trend
Seasonality: In which regular or fixed interval shifts within the dataset in a continuous timeline. Would be bell curve or saw tooth
Cyclical: In which there is no fixed interval, uncertainty in movement and its pattern
Irregularity: Unexpected situations/events/scenarios and spikes in a short time span.
"
Classic_models,What are the limitations of Time Series Analysis?,"Time series has the below-mentioned limitations, we have to take care of those during our analysis,
Similar to other models, the missing values are not supported by TSA
The data points must be linear in their relationship.
Data transformations are mandatory, so a little expensive.
Models mostly work on Uni-variate data.
"
Classic_models,Data Types of Time Series,"Let’s discuss the time series’ data types and their influence. While discussing TS data-types, there are two major types.
Stationary
Non- Stationary
Stationary: A dataset should follow the below thumb rules, without having Trend, Seasonality, Cyclical, and Irregularity component of time series
The MEAN value of them should be completely constant in the data during the analysis
The VARIANCE should be constant with respect to the time-frame
The COVARIANCE measures the relationship between two variables.
Non- Stationary: This is just the opposite of Stationary.
"
Classic_models,Methods to check Stationarity ,"During the TSA model preparation workflow, we must access if the given dataset is Stationary or NOT. Using Statistical and Plots test.
Statistical Test: There are two tests available to test if the dataset is Stationary or NOT.
Augmented Dickey-Fuller (ADF) Test
Kwiatkowski-Phillips-Schmidt-Shin (KPSS) Test
Augmented Dickey-Fuller (ADF) Test or Unit Root Test: The ADF test is the most popular statistical test and with the following assumptions.
Null Hypothesis (H0): Series is non-stationary
Alternate Hypothesis (HA): Series is stationary
p-value >0.05 Fail to reject (H0)
p-value <= 0.05 Accept (H1)
Kwiatkowski–Phillips–Schmidt–Shin (KPSS): these tests are used for testing a NULL Hypothesis (HO), that will perceive the time-series, as stationary around a deterministic trend against the alternative of a unit root. Since TSA looking for Stationary Data for its further analysis, we have to make sure that the dataset should be stationary.
"
Classic_models,Converting Non-stationary into stationary,"Let’s discuss quickly how to convert Non- stationary into stationary for effective time series modeling. There are two major methods available for this conversion.
Detrending
Differencing
Detrending: It involves removing the trend effects from the given dataset and showing only the differences in values from the trend. it always allows the cyclical patterns to be identified.
Differencing: This is a simple transformation of the series into a new time series, which we use to remove the series dependence on time and stabilize the mean of the time series, so trend and seasonality are reduced during this transformation.
Yt= Yt – Yt-1
Yt=Value with time
Transformation: This includes three different methods they are Power Transform, Square Root, and Log Transfer., most commonly used one is Log Transfer.
"
Classic_models,Moving Average Methodology,"The commonly used time series method is Moving Average. This method is slick with random short-term variations. Relatively associated with the components of time series.
The Moving Average (MA) (Or) Rolling Mean: In which MA has calculated by taking averaging data of the time-series, within k periods.
Let’s see the types of moving averages:
Simple Moving Average (SMA),
Cumulative Moving Average (CMA)
Exponential Moving Average (EMA)
Simple Moving Average (SMA)
The SMA is the unweighted mean of the previous M or N points. The selection of sliding window data points, depending on the amount of smoothing is preferred since increasing the value of M or N, improves the smoothing at the expense of accuracy.
Cumulative Moving Average (CMA)
The CMA is the unweighted mean of past values, till the current time.
Exponential Moving Average (EMA)
EMA is mainly used to identify trends and to filter out noise. The weight of elements is decreased gradually over time. This means It gives weight to recent data points, not historical ones. Compared with SMA, the EMA is faster to change and more sensitive.
α –>Smoothing Factor.
It has a value between 0,1.
Represents the weighting applied to the very recent period. 
Rather than using past values of the forecast variable in a regression, a moving average model uses past forecast errors in a regression-like model.
yt=c+εt+θ1εt−1+θ2εt−2+⋯+θqεt−q,
where εtεt is white noise. We refer to this as an MA(qq) model, a moving average model of order qq. Of course, we do not observe the values of εtεt, so it is not really a regression in the usual sense.
"
Classic_models,Time Series Analysis in Data Science and Machine Learning,"When dealing with TSA in Data Science and Machine Learning, there are multiple model options are available. In which the Autoregressive–Moving-Average (ARMA) models with [p, d, and q].
P==> autoregressive lags
q== moving average lags
d==> difference in the order
Before we get to know about Arima, first you should understand the below terms better.
Auto-Correlation Function (ACF)
Partial Auto-Correlation Function (PACF)
Auto-Correlation Function (ACF): ACF is used to indicate and how similar a value is within a given time series and the previous value. (OR) It measures the degree of the similarity between a given time series and the lagged version of that time series at different intervals that we observed.
Python Statsmodels library calculates autocorrelation. This is used to identify a set of trends in the given dataset and the influence of former observed values on the currently observed values.
Partial Auto-Correlation (PACF): PACF is similar to Auto-Correlation Function and is a little challenging to understand. It always shows the correlation of the sequence with itself with some number of time units per sequence order in which only the direct effect has been shown, and all other intermediary effects are removed from the given time series.
A partial autocorrelation is the amount of correlation between a variable and a lag of itself that is not explained by correlations at all lower-order-lags. The autocorrelation of a time series Y at lag 1 is the coefficient of correlation between Yt and Yt-1, which is presumably also the correlation between Yt-1 and Yt-2. But if Yt is correlated with Yt-1, and Yt-1 is equally correlated with Yt-2, then we should also expect to find correlation between Yt and Yt-2. In fact, the amount of correlation we should expect at lag 2 is precisely the square of the lag-1 correlation. Thus, the correlation at lag 1 ""propagates"" to lag 2 and presumably to higher-order lags. The partial autocorrelation at lag 2 is therefore the difference between the actual correlation at lag 2 and the expected correlation due to the propagation of correlation at lag 1.

The autocorrelations are significant for a large number of lags--but perhaps the autocorrelations at lags 2 and above are merely due to the propagation of the autocorrelation at lag 1. This is confirmed by the PACF plot:
Interpret ACF and PACF plots
Remember that both ACF and PACF require stationary time series for analysis.
"
Classic_models,Understanding ARMA and ARIMA ,"ARMA This is a combination of the Auto-Regressive and Moving Average model for forecasting. This model provides a weakly stationary stochastic process in terms of two polynomials, one for the Auto-Regressive and the second for the Moving Average.
ARMA is best for predicting stationary series. So ARIMA came in since it supports stationary as well as non-stationary.
AR ==> Uses the past values to predict the future
MA ==> Uses the past error terms in the given series to predict the future
I==> uses the differencing of observation and makes the stationary data
AR+I+MA= ARIMA
Understand the Signature of ARIMA
p==> log order => No of lag observations.
d==> degree of differencing => No of times that the raw observations are differenced.
q==>order of moving average => the size of the moving average window
Implementation steps for ARIMA
Step 1: Plot a time series format
Step 2: Difference to make stationary on mean by removing the trend
Step 3: Make stationary by applying log transform.
Step 4: Difference log transform to make as stationary on both statistic mean and variance
Step 5: Plot ACF & PACF, and identify the potential AR and MA model
Step 6: Discovery of best fit ARIMA model
Step 7: Forecast/Predict the value, using the best fit ARIMA model
Step 8: Plot ACF & PACF for residuals of the ARIMA model, and ensure no more information is left.
"
Data,"You run your regression on different subsets of your data, and find that in each subset, the beta value for a certain variable varies wildly. What could be the issue here?","The dataset might be heterogeneous. In which case, it is recommended to cluster datasets into different subsets wisely, and then draw different models for different subsets. Or, use models like non parametric models (trees) which can deal with heterogeneity quite nicely.
Heterogeneous data are any data with high variability of data types and formats. They are possibly ambiguous and low quality due to missing values, high data redundancy, and untruthfulness.

"
Data,Is more data always better?,"Statistically,
It depends on the quality of your data, for example, if your data is biased, just getting more data won’t help.
It depends on your model. If your model suffers from high bias, getting more data won’t improve your test results beyond a point. You’d need to add more features, etc.
Practically
Also there’s a tradeoff between having more data and the additional storage, computational power, memory it requires. Hence, always think about the cost of having more data.
"
Data,What are advantages of plotting your data before performing analysis?,"Data sets have errors.  You won't find them all but you might find some. That 212 year old man. That 9 foot tall woman.
Variables can have skewness, outliers etc.  Then the arithmetic mean might not be useful. Which means the standard deviation isn't useful.
Variables can be multimodal!  If a variable is multimodal then anything based on its mean or median is going to be suspect. 
"
Data,How can you determine which features are the most important in your model?,"run the features though a Gradient Boosting Machine or Random Forest to generate plots of relative importance and information gain for each feature in the ensembles.
Look at the variables added in forward variable selection 
"
Data,Define confounding variables.,"Confounding variables are also known as confounders. These variables are a type of extraneous variables that influence both independent and dependent variables causing spurious association and mathematical relationships between those variables that are associated but are not casually related to each other
"
Data,Define and explain selection bias,"The selection bias occurs in the case when the researcher has to make a decision on which participant to study. The selection bias is associated with those researches when the participant selection is not random. The selection bias is also called the selection effect. The selection bias is caused by as a result of the method of sample collection.
Four types of selection bias are explained below:
Sampling Bias: As a result of a population that is not random at all, some members of a population have fewer chances of getting included than others, resulting in a biased sample. This causes a systematic error known as sampling bias.
Time interval: 
Trials may be stopped early if we reach any extreme value but if all variables are similar invariance, the variables with the highest variance have a higher chance of achieving the extreme value. Early termination of a trial at a time when its results support the desired conclusion.
Data: It is when specific data is selected arbitrarily and the generally agreed criteria are not followed. Cherry picking, which actually is not selection bias, but confirmation bias, when specific subsets of data are chosen to support a conclusion (e.g. citing examples of plane crashes as evidence of airline flight being unsafe, while ignoring the far more common example of flights that complete safely. See: Availability heuristic)
Attrition: Attrition in this context means the loss of the participants. It is the discounting of those subjects that did not complete the trial.
"
Data,Types of sampling bias,"Self-selection
Non-response 
Undercoverage
Survivorship
Pre-screening or advertising
Healthy user
"
Data,What is Cross-Validation?,"Cross-Validation is a Statistical technique used for improving a model’s performance. Here, the model will be trained and tested with rotation using different samples of the training dataset to ensure that the model performs well for unknown data. The training data will be split into various groups and the model is run and validated against these groups in rotation.
The most commonly used techniques are:
K-Fold method
Leave p-out method
Leave-one-out method
Holdout method
"
Data,Dealing with outliers,"Univariate method: This method looks for data points with extreme values on one variable.
Multivariate method: Here, we look for unusual combinations of all the variables.
Minkowski error: This method reduces the contribution of potential outliers in the training process.
"
Data,The Curse of Dimensionality,"If we have more features than observations than we run the risk  of massively overfitting our model — this would generally result in terrible out of sample performance.
When we have too many features, observations become harder to cluster — believe it or not, too many dimensions causes every observation in your dataset to appear equidistant from all the others. And because clustering uses a distance measure such as Euclidean distance to quantify the similarity between observations, this is a big problem. If the distances are all approximately equal, then all the observations appear equally alike (as well as equally different), and no meaningful clusters can be formed.
All samples are close to the edge of the sample. And this is a bad news because prediction is much more difficult near the edges of the training sample.
The sampling density decreases exponentially as p increases and hence the data becomes much more sparse without significantly more data. 
We should conduct PCA to reduce dimensionality
The curse of dimensionality, first introduced by Bellman [1], indicates that the number of samples needed to estimate an arbitrary function with a given level of accuracy grows exponentially with respect to the number of input variables (i.e., dimensionality) of the function.
"
Data,What is a Box-Cox Transformation?,"The dependent variable for a regression analysis might not satisfy one or more assumptions of an ordinary least squares regression. The residuals could either curve as the prediction increases or follow the skewed distribution. In such scenarios, it is necessary to transform the response variable so that the data meets the required assumptions. A Box cox transformation is a statistical technique to transform non-normal dependent variables into a normal shape. If the given data is not normal then most of the statistical techniques assume normality. Applying a box cox transformation means that you can run a broader number of tests.
"
Data,What is the importance of dimensionality reduction?,"The process of dimensionality reduction constitutes reducing the number of features in a dataset to avoid overfitting and reduce the variance. There are mostly 4 advantages of this process:
This reduces the storage space and time for model execution.
Removes the issue of multi-collinearity thereby improving the parameter interpretation of the ML model.
Makes it easier for visualizing data when the dimensions are reduced.
Avoids the curse of increased dimensionality. 
"
Data,Methods of dimensionality reduction,"Feature Selection Methods
Matrix Factorization
Manifold Learning
Autoencoder Methods
Linear Discriminant Analysis (LDA)
Principal component analysis (PCA)
"
Data,What is the significance of using the Fourier transform in Deep Learning tasks?,"The Fourier transform function efficiently analyzes, maintains, and manages large datasets. You can use it to generate real-time array data that is helpful for processing multiple signals.
If the matrices of the input and filters in the CNN can be converted into the frequency domain to perform the multiplication and the outcome matrices of the multiplication in the frequency domain can be converted into the time domain will not perform any harm to the accuracy of the model. The conversion of matrices from the time domain to the frequency domain can be done by the Fourier transform or fast Fourier transform and conversion from the frequency domain to the time domain can be done by the inverse Fourier transform or inverse fast Fourier transform. 
"
Data,What if we want to build a model for predicting prices? Are prices distributed normally? Do we need to do any pre-processing for prices? ,"Data is not normal. Specially, real-world datasets or uncleaned datasets always have certain skewness. Same goes for the price prediction. Price of houses or any other thing under consideration depends on a number of factors. So, there's a great chance of presence of some skewed values i.e outliers if we talk in data science terms.
Yes, you may need to do pre-processing. Most probably, you will need to remove the outliers to make your distribution near-to-normal.
"
Data,How do we choose K in K-fold cross-validation? What’s your favorite K? ,"There are two things to consider while deciding K: the number of models we get and the size of validation set. We do not want the number of models to be too less, like 2 or 3. At least 4 models give a less biased decision on the metrics. On the other hand, we would want the dataset to be at least 20-25% of the entire data. So that at least a ratio of 3:1 between training and validation set is maintained.
I tend to use 4 for small datasets and 5 for large ones as K.
"
Data,If a weight for one variable is higher than for another  —  can we say that this variable is more important? ,"Yes - if your predictor variables are normalized.
Without normalization, the weight represents the change in the output per unit change in the predictor. If you have a predictor with a huge range and scale that is used to predict an output with a very small range - for example, using each nation's GDP to predict maternal mortality rates - your coefficient should be very small. That does not necessarily mean that this predictor variable is not important compared to the others.
"
Data,What do you mean by Associative Rule Mining (ARM)?,"Associative Rule Mining is one of the techniques to discover patterns in data like features (dimensions) which occur together and features (dimensions) which are correlated. It is mostly used in Market-based Analysis to find how frequently an itemset occurs in a transaction. Association rules have to satisfy minimum support and minimum confidence at the very same time. Association rule generation generally comprised of two different steps:
“A min support threshold is given to obtain all frequent item-sets in a database.”
“A min confidence constraint is given to these frequent item-sets in order to form the association rules.”
Support is a measure of how often the “item set” appears in the data set and Confidence is a measure of how often a particular rule has been found to be true.
"
Data,List Sampling Techniques,"It is the practice of selecting an individual group from a population in order to study the whole population.
Probability Sampling Techniques is one of the important types of sampling techniques. Probability sampling allows every member of the population a chance to get selected. It is mainly used in quantitative research when you want to produce results representative of the whole population.
1. Simple Random Sampling
2. Systematic Sampling
In systematic sampling, every population is given a number as well like in simple random sampling. However, instead of randomly generating numbers, the samples are chosen at regular intervals.
3. Stratified Sampling
In stratified sampling, the population is subdivided into subgroups, called strata, based on some characteristics (age, gender, income, etc.). After forming a subgroup, you can then use random or systematic sampling to select a sample for each subgroup. This method allows you to draw more precise conclusions because it ensures that every subgroup is properly represented.
4. Cluster Sampling
In cluster sampling, the population is divided into subgroups, but each subgroup has similar characteristics to the whole sample. Instead of selecting a sample from each subgroup, you randomly select an entire subgroup. This method is helpful when dealing with large and diverse populations.
Non-Probability Sampling Techniques is one of the important types of Sampling techniques. In non-probability sampling, not every individual has a chance of being included in the sample. This sampling method is easier and cheaper but also has high risks of sampling bias. It is often used in exploratory and qualitative research with the aim to develop an initial understanding of the population.
1. Convenience Sampling
In this sampling method, the researcher simply selects the individuals which are most easily accessible to them. This is an easy way to gather data, but there is no way to tell if the sample is representative of the entire population. The only criteria involved is that people are available and willing to participate.
2. Voluntary Response Sampling
Voluntary response sampling is similar to convenience sampling, in the sense that the only criterion is people are willing to participate. However, instead of the researcher choosing the participants, the participants volunteer themselves. 
3. Purposive Sampling
In purposive sampling, the researcher uses their expertise and judgment to select a sample that they think is the best fit. It is often used when the population is very small and the researcher only wants to gain knowledge about a specific phenomenon rather than make statistical inferences.
4. Snowball Sampling
In snowball sampling, the research participants recruit other participants for the study. It is used when participants required for the research are hard to find. It is called snowball sampling because like a snowball, it picks up more participants along the way and gets larger and larger.
"
Data,Feature selection methods ,"The main benefits of performing feature selection in advance, rather than letting the machine learning model figure out which features are most important, include:
simpler models: simple models are easy to explain - a model that is too complex and unexplainable is not valuable
shorter training times: a more precise subset of features decreases the amount of time needed to train a model
variance reduction: increase the precision of the estimates that can be obtained for a given simulation 
avoid the curse of high dimensionality: dimensionally cursed phenomena states that, as dimensionality and the number of features increases, the volume of space increases so fast that the available data become limited - PCA feature selection may be used to reduce dimensionality ,
Filter Methods:
Filter methods select features based on statistics rather than feature selection cross-validation performance. A selected metric is applied to identify irrelevant attributes and perform recursive feature selection. Filter methods are either univariate, in which an ordered ranking list of features is established to inform the final selection of feature subset; or multivariate, which evaluates the relevance of the features as a whole, identifying redundant and irrelevant features.
There are various filter methods such as the Chi-Square test, Fisher’s Score method, Correlation Coefficient, Variance Threshold, Mean Absolute Difference (MAD) method, Dispersion Ratios, etc.
Wrapper Methods:
Wrapper feature selection methods consider the selection of a set of features as a search problem, whereby their quality is assessed with the preparation, evaluation, and comparison of a combination of features to other combinations of features. This method facilitates the detection of possible interactions amongst variables. Wrapper methods focus on feature subsets that will help improve the quality of the results of the clustering algorithm used for the selection. 
There are three types of wrapper methods, they are:
Forward Selection: Here, one feature is tested at a time and new features are added until a good fit is obtained.
Backward Selection: Here, all the features are tested and the non-fitting ones are eliminated one by one to see while checking which works better.
Recursive Feature Elimination: The features are recursively checked and evaluated how well they perform.
These methods are generally computationally intensive and require high-end resources for analysis. But these methods usually lead to better predictive models having higher accuracy than filter methods.
Embedded Methods:
Embedded feature selection methods integrate the feature selection machine learning algorithm as part of the learning algorithm, in which classification and feature selection are performed simultaneously. The features that will contribute the most to each iteration of the model training process are carefully extracted. Random forest feature selection, decision tree feature selection, and LASSO feature selection are common embedded methods.
"
Data,Data Normalization,"There are different types of data normalization. Assume you have a dataset X, which has N rows(entries) and D columns(features). X[:,i] represent feature i and X[j,:] represent entry j. We have:
Z Normalization(Standardization):
I used to falsely think this method somehow yields a standard Gaussian result. In fact, standardization does not change the type of distribution:
This transformation sets the mean of data to 0 and the standard deviation to 1. In most cases, standardization is used feature-wise
Min-Max Normalization:
This method rescales the range of the data to [0,1]. In most cases, standardization is used feature-wise as well
StandardScaling and MinMax Scaling have similar applications and are often more or less interchangeable. However, if the algorithm involves the calculation of distances between points or vectors, the default choice is StandardScaling. But MinMax Scaling is useful for visualization by bringing features within the interval (0, 255).
Unit Vector Normalization:
Scaling to unit length shrinks/stretches a vector (a row of data can be viewed as a D-dimensional vector) to a unit sphere. When used on the entire dataset, the transformed data can be visualized as a bunch of vectors with different directions on the D-dimensional unit sphere.
"
Data,Why do we need Normalization?,"Monotonic feature transformation is critical for some algorithms and has no effect on others. This is one of the reasons for the increased popularity of decision trees and all its derivative algorithms (random forest, gradient boosting). Not everyone can or want to tinker with transformations, and these algorithms are robust to unusual distributions.
There are also purely engineering reasons: np.log is a way of dealing with large numbers that do not fit in np.float64. This is an exception rather than a rule; often it’s driven by the desire to adapt the dataset to the requirements of the algorithm. Parametric methods usually require the data distribution to be at least symmetric and unimodal, which is not always the case.
However, data requirements are imposed not only by parametric methods; K nearest neighbors will predict complete nonsense if features are not normalized e.g. when one distribution is located in the vicinity of zero and does not go beyond (-1, 1) while the other’s range is on the order of hundreds of thousands.
a). Standardization improves the numerical stability of your model
b). Standardization may speed up the training process
A corollary to the first ‘theorem’ is that if different features have drastically different ranges, the learning rate is determined by the feature with the largest range. This leads to another advantage of standardization: speeds up the training process.
Standardization gives ‘equal’ considerations for each feature.
Standardization is beneficial in many cases. It improves the numerical stability of the model and often reduces training time. However, standardization isn’t always great. It can harm the performance of distance-based clustering algorithms by assuming equal importance of features. If there are inherent importance differences between features, it’s generally not a good idea to do standardization.
"
Data,Imbalanced Training Data,"We now understand what class imbalance is and why it provides misleading classification accuracy.
1) Collect More Data
2) Try Changing Performance Metric
Accuracy is not the metric to use when working with an imbalanced dataset. We have seen that it is misleading.
There are metrics that have been designed to tell you a more truthful story when working with imbalanced classes.
I give more advice on selecting different performance measures in my post “Classification Accuracy is Not Enough: More Performance Measures You Can Use“.
In that post I look at an imbalanced dataset that characterizes the recurrence of breast cancer in patients.
From that post, I recommend looking at the following performance measures that can give more insight into the accuracy of the model than traditional classification accuracy:
Confusion Matrix: A breakdown of predictions into a table showing correct predictions (the diagonal) and the types of incorrect predictions made (what classes incorrect predictions were assigned).
Precision: A measure of a classifiers exactness.
Recall: A measure of a classifiers completeness
F1 Score (or F-score): A weighted average of precision and recall.
I would also advice you to take a look at the following:
Kappa (or Cohen’s kappa): Classification accuracy normalized by the imbalance of the classes in the data.
ROC Curves: Like precision and recall, accuracy is divided into sensitivity and specificity and models can be chosen based on the balance thresholds of these values.
You can learn a lot more about using ROC Curves to compare classification accuracy in our post “Assessing and Comparing Classifier Performance with ROC Curves“.
Still not sure? Start with kappa, it will give you a better idea of what is going on than classification accuracy.
3) Resampling Dataset
You can change the dataset that you use to build your predictive model to have more balanced data.
This change is called sampling your dataset and there are two main methods that you can use to even-up the classes:
You can add copies of instances from the under-represented class called over-sampling (or more formally sampling with replacement), or
You can delete instances from the over-represented class, called under-sampling.
These approaches are often very easy to implement and fast to run. They are an excellent starting point.
In fact, I would advise you to always try both approaches on all of your imbalanced datasets, just to see if it gives you a boost in your preferred accuracy measures.
You can learn a little more in the the Wikipedia article titled “Oversampling and undersampling in data analysis“.
Some Rules of Thumb
Consider testing under-sampling when you have an a lot data (tens- or hundreds of thousands of instances or more)
Consider testing over-sampling when you don’t have a lot of data (tens of thousands of records or less)
Consider testing random and non-random (e.g. stratified) sampling schemes.
Consider testing different resampled ratios (e.g. you don’t have to target a 1:1 ratio in a binary classification problem, try other ratios)
4) Try Generate Synthetic Samples
A simple way to generate synthetic samples is to randomly sample the attributes from instances in the minority class.
You could sample them empirically within your dataset or you could use a method like Naive Bayes that can sample each attribute independently when run in reverse. You will have more and different data, but the non-linear relationships between the attributes may not be preserved.
There are systematic algorithms that you can use to generate synthetic samples. The most popular of such algorithms is called SMOTE or the Synthetic Minority Over-sampling Technique.
5) Try Different Algorithms
As always, I strongly advice you to not use your favorite algorithm on every problem. You should at least be spot-checking a variety of different types of algorithms on a given problem.
6) Try Penalized Models
You can use the same algorithms but give them a different perspective on the problem.
Penalized classification imposes an additional cost on the model for making classification mistakes on the minority class during training. These penalties can bias the model to pay more attention to the minority class.
Now for handling class imbalance, you can use weighted Sigmoid Cross-Entropy loss. So you will penalize for wrong prediction based on the number/ratio of positive examples.
"
Data,Ways to Deal with the Lack of Data in Machine Learning ,"Use simpler model
Use ensemble methods
In general, the simpler the machine learning algorithm, the better it will learn from small data sets. From an ML perspective, small data requires models that have low complexity (or high bias) to avoid overfitting the model to the data. I noticed that the Naive Bayes algorithm is among the simplest classifiers and as a result learns remarkably well from relatively small data sets.
Transfer learning
Data Augmentation
Synthetic Data
"
Data,Log-normal distribution,"In probability theory, a log-normal (or lognormal) distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed. 
If we assume that some data is not normally distributed but is described by the log-normal distribution, it can easily be transformed to a normal distribution. The lognormal distribution is suitable for describing salaries, price of securities, urban population, number of comments on articles on the internet, etc. However, to apply this procedure, the underlying distribution does not necessarily have to be lognormal; you can try to apply this transformation to any distribution with a heavy right tail. Furthermore, one can try to use other similar transformations, formulating their own hypotheses on how to approximate the available distribution to a normal. Examples of such transformations are Box-Cox transformation (logarithm is a special case of the Box-Cox transformation) or Yeo-Johnson transformation (extends the range of applicability to negative numbers). In addition, you can also try adding a constant to the feature — np.log (x + const).
"
Data,How to Test for Normality,"Q Q plot compares two different distributions. If the two sets of data came from the same distribution, the points will fall on a 45 degree reference line. To use this type of graph for the assumption of normality, compare your data to data from a distribution with known normality.
Boxplot.
Draw a boxplot of your data. If your data comes from a normal distribution, the box will be symmetrical with the mean and median in the center. If the data meets the assumption of normality, there should also be few outliers.
Normal Probability Plot.
The normal probability plot was designed specifically to test for the assumption of normality. If your data comes from a normal distribution, the points on the graph will form a line.
Histogram.
The popular histogram can give you a good idea about whether your data meets the assumption. If your data looks like a bell curve: then it’s probably normal.
Check Skewness and Kurtosis of the sampled data. Skewness = 0 and kurtosis = 3 are typical for a normal distribution, so the farther away they are from these values, the more non-normal the distribution.
"
Data,Statistical Tests for Normality,"You’ve got lots of options to test for normality. Most of these are included with statistical packages like SPSS.
Chi-square normality test. You can use a chi square test for normality. The advantage is that it’s relatively easy to use, but it isn’t a very strong test. If you have a small sample (under 20), it may be the only test you can use. For larger samples, you’re much better off choosing another option.
D’Agostino-Pearson Test. This uses skewness and kurtosis to see if your data matches normal data. It requires your sample size to be over 20.
Jarque-Bera Test. This common test is also relatively straightforward. Like the D’Agostino-Pearson, the basic idea is that it tests the skew and kurtosis of your data to see if it matches what you would expect from a normal distribution. The larger the JB statistic, the more the data deviates from the normal.
Kolmogorov-Smirnov Goodness of Fit Test. This compares your data with a known distribution (i.e. a normal distribution).
Lilliefors Test. The Lilliefors test calculates a test statistic T which you can compare to a critical value. If the test statistic is bigger than the critical value, it’s a sign that your data isn’t normal. It also computes a p-value for your distribution, which you compare to a significance level.
Shapiro-Wilk Test This test will tell you if a random sample came from a normal distribution. The test gives you a W value; small values indicate your sample is not normally distributed. 
"
Data,Shapiro-Wilk test ,"Shapiro-Wilk test is a test of normality, it determines whether the given sample comes from the normal distribution or not. Shapiro-Wilk’s test or Shapiro test is a normality test in frequentist statistics. The null hypothesis of Shapiro’s test is that the population is distributed normally.
Suppose a sample, say x1,x2…….xn,  has come from a normally distributed population. Then according to the Shapiro-Wilk’s tests null hypothesis test
W = (sum_i=1..n(aixi))^2 / (sum_i=1..n xi – mean(x))^2
where,
x(i) : it is the ith smallest number in the given sample.
mean(x) : ( x1+x2+……+xn) / n i.e the sample mean.
ai : coefficient that can be calculated as (a1,a2,….,an) = (mT V-1)/C . Here V is the covariance matrix, m and C are the vector norms that can be calculated as C= || V-1 m || and m = (m1, m2,……, mn ).
Output:
(0.9977102279663086, 0.7348126769065857)
Output Interpretation:
Since in the above example, the p-value is 0.73 which is more than the threshold(0.05) which is the alpha(0.05) then we fail to reject the null hypothesis i.e. we do not have sufficient evidence to say that sample does not come from a normal distribution.
"
Deep Learning,How does a batch size influence a model?,"Increasing batch size drops the learners' ability to generalize. The idea is that smaller batches are more likely to push out local minima and find the Global Minima.
large batch size means the model makes very large gradient updates and very small gradient updates. The size of the update depends heavily on which particular samples are drawn from the dataset. On the other hand using small batch size means the model makes updates that are all about the same size. The size of the update only weakly depends on which particular samples are drawn from the dataset
Large Batch Training methods tend to overfit compared to the same network trained with smaller batch size.
Large Batch Training methods tend to get trapped or even attracted to potential saddle points in the loss landscape.
Large Batch Training methods tend to zoom in on the closest relative minima that it finds, whereas networks trained with a smaller batch size tend to “explore” the loss landscape before settling on a promising minimum.
Large Batch Training methods tend to converge to completely “different” minima points than networks trained with smaller batch sizes.
Furthermore, the authors tackled the Generalization Gap from the perspective of how Neural Networks navigate the loss landscape during training. Training with a relatively large batch size tends to converge to sharp minimizers, while reducing the batch size usually leads to falling into flat minimizers. A sharp minimizer can be thought of as a narrow and steep ravine, whereas a flat minimizer is analogous to a valley in a vast landscape of low and mild hill terrains. To phrase it in more rigorous terms:
Sharp minimizers are characterized by a significant number of large positive eigenvalues of the Hessian Matrix of f(x), while flat minimizers are characterized by a considerable number of smaller positive eigenvalues of the Hessian Matrix of f(x).
“Falling” into a sharp minimizer may produce a seemingly better loss than a flat minimizer, but it’s more prone to generalizing poorly to unseen datasets. The diagram below illustrates a simple 2-dimensional loss landscape from Keskar et al.
A sharp minimum compared to a flat minimum. From Keskar et al.
We assume that the relationship between features and labels of unseen data points is similar to that of the data points that we used for training but not exactly the same. As the example shown above, the “difference” between train and test can be a slight horizontal shift. The parameter values that result in a sharp minimum become a relative maximum when applied to unseen data points due to its narrow accommodation of minimum values. With a flat minimum, though, as shown in the diagram above, a slight shift in the “Testing Function” would still put the model at a relatively minimum point in the loss landscape.
Typically, adopting a small batch size adds noise to training compared to using a bigger batch size. Since the gradients were estimated with a smaller number of samples, the estimation at each batch update will be rather “noisy” relative to the “loss landscape” of the entire dataset. Noisy training in the early stages is helpful to the model as it encourages exploration of the loss landscape. Keskar et al. also stated that…
“We have observed that the loss function landscape of deep Neural Networks is such that large-batch methods are attracted to regions with sharp minimizers and that, unlike small-batch methods, are unable to escape basins of attraction of these minimizers.”
Although larger batch sizes are considered to bring more stability to training, the noisiness that small batch training provides is actually beneficial to explore and avoiding sharp minimizers. We can effectively utilize this fact to design a “batch size scheduler” where we start with a small batch size to allow for exploration of the loss landscape. Once a general direction is decided, we hone in on the (hopefully) flat minimum and increase the batch size to stabilize training. The details of how one can increase the batch size during training to obtain faster and better results are described in the following article.
"
Deep Learning,Why can’t I use Softmax on the hidden layer?,"The following steps explain why using the softmax function on the hidden layer is not a good idea:
1. Variables independence: A lot of regularization and effort is required to keep your variables independent, uncorrelated and quite sparse. If you use the softmax layer as a hidden layer, then you will keep all your nodes linearly dependent which may result in many problems and poor generalization.
2. Training issues:  if your network is working better, you have to make a part of activations from your hidden layer a little bit lower. Here automatically you are making the rest of them have mean activation on a higher level which might, in fact, increase the error and harm your training phase.
3. Mathematical issues: If you create constraints on activations of your model you decrease the expressive power of your model without any logical explanation. 
4. Batch normalization does it better: You may consider the fact that mean output from a network may be useful for training. But on the other hand, a technique called Batch Normalization has been already proven to work better, but it was reported that setting softmax as the activation function in a hidden layer may decrease the accuracy and speed of learning.
"
Deep Learning,Global Average Pooling,"The feature maps of the last convolutional layer are vectorized and fed into fully connected layers followed by a softmax logistic regression layer. This structure bridges the convolutional structure with traditional neural networks. It treats the convolutional layers as feature extractors, and the resulting feature is classified in a traditional way. 
The fully connected layers are prone to overfitting. You can use Dropout as a regularizer which randomly sets half of the activations to the fully connected layers to zero during training. It has improved the generalization ability and largely prevents overfitting. 
You can use another strategy called global average pooling to replace the Flatten layers in CNN. It generates one feature map for each corresponding category of the classification task in the last Conv layer.
"
Deep Learning,What are some of the uses of Autoencoders in Deep Learning?,"An autoencoder is a type of artificial neural network used to learn data encodings in an unsupervised manner.
The aim of an autoencoder is to learn a lower-dimensional representation (encoding) for a higher-dimensional data, typically for dimensionality reduction, by training the network to capture the most important parts of the input image.
Autoencoders are used to convert black and white images into colored images.
Autoencoder helps to extract features and hidden patterns in the data.
It is also used to reduce the dimensionality of data.
It can also be used to remove noises from images.
"
Deep Learning,Why is a convolutional neural network preferred over a dense neural network for an image classification task?,"The number of parameters in a convolutional neural network is much more diminutive than that of a Dense Neural Network. Hence, a CNN is less likely to overfit.
CNN allows you to look at the weights of a filter and visualize what the network learned. So, this gives a better understanding of the model.
CNN trains models in a hierarchical way, i.e., it learns the patterns by explaining complex patterns using simpler ones.
"
Deep Learning,Xavier (Glorot) initialization ,"Xavier initialization sets a layer’s weights to values chosen from a random uniform distribution that’s bounded between +-sqrt(6/(ni+ni+1))
where nᵢ is the number of incoming network connections, or “fan-in,” to the layer, and nᵢ₊₁ is the number of outgoing network connections from that layer, also known as the “fan-out.”
"
Deep Learning,What is an optimizer?,"Optimizers are algorithms or methods used to minimize an error function(loss function)or to maximize the efficiency of production. Optimizers are mathematical functions which are dependent on model’s learnable parameters i.e Weights & Biases. Optimizers help to know how to change weights and learning rate of neural network to reduce the losses.
"
Deep Learning,List Optimizer Types,"Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent, SGD with Momentum, Nesterov Accelerated Gradient, RMS-Prop, AdaGrad(Adaptive Gradient Descent), AdaDelta, Adam
"
Deep Learning,Gradient Descent,"We now consider the problem of solving for the minimum of a real-valued function min x f(x), where f : Rd → R is an objective function that captures the machine learning problem at hand. We assume that our function f is differentiable, and we are unable to analytically find a solution in closed form. Gradient descent is a first-order optimization algorithm. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point. Recall that the gradient points in the direction of steepest ascent.
Let us consider multivariate functions. Imagine a surface (described by the function f(x)) with a ball starting at a particular location x0. When the ball is released, it will move downhill in the direction of steepest descent. Gradient descent exploits the fact that f(x0) decreases fastest if one moves from x0 in the direction of the negative gradient −((∇f)(x0))⊤ of f at x0. We assume in this book that the functions are differentiable, and refer the reader to more general settings in Section 7.4. Then, if x1 = x0 − γ((∇f)(x0))⊤ (7.5) for a small step-size γ ⩾ 0, then f(x1) ⩽ f(x0). Note that we use the transpose for the gradient since otherwise the dimensions will not work out. This observation allows us to define a simple gradient descent algorithm: If we want to find a local optimum f(x∗) of a function f : Rn → R, x 7→ f(x), we start with an initial guess x0 of the parameters we wish to optimize and then iterate according to xi+1 = xi − γi((∇f)(xi))⊤ . (7.6) For suitable step-size γi , the sequence f(x0) ⩾ f(x1) ⩾ . . . converges to a local minimum.
In summary, Gradient Descent method’s steps are:
choose a starting point (initialisation)
calculate gradient at this point
make a scaled step in the opposite direction to the gradient (objective: minimise)
repeat points 2 and 3 until one of the criteria is met:
maximum number of iterations reached
step size is smaller than the tolerance.
Below there’s an exemplary implementation of the Gradient Descent algorithm (with steps tracking):
This function takes 5 parameters:
1. starting point - in our case, we define it manually but in practice, it is often a random initialisation
2. gradient function - has to be specified before-hand
3. learning rate - scaling factor for step sizes
4. maximum number of iterations
5. tolerance to conditionally stop the algorithm (in this case a default value is 0.01)
Advantages of Gradient Descent
Easy to understand
Easy to implement
Disadvantages of Gradient Descent
Because this method calculates the gradient for the entire data set in one update, the calculation is very slow.
It requires large memory and it is computationally expensive.
"
Deep Learning,Stochastic Gradient Descent,"It is a variant of Gradient Descent. If the model has 10K dataset SGD will update the model parameters 10k times.
Stochastic Gradient Descent
Advantages of Stochastic Gradient Descent
Frequent updates of model parameter
Requires less Memory.
Allows the use of large data sets as it has to update only one example at a time.
Disadvantages of Stochastic Gradient Descent
The frequent can also result in noisy gradients which may cause the error to increase instead of decreasing it.
High Variance.
Frequent updates are computationally expensive.
"
Deep Learning,Mini-Batch Gradient Descent,"It is a combination of the concepts of SGD and batch gradient descent. It simply splits the training dataset into small batches and performs an update for each of those batches. This creates a balance between the robustness of stochastic gradient descent and the efficiency of batch gradient descent. it can reduce the variance when the parameters are updated, and the convergence is more stable. It splits the data set in batches in between 50 to 256 examples, chosen at random.
Mini Batch Gradient Descent
Advantages of Mini Batch Gradient Descent:
It leads to more stable convergence.
more efficient gradient calculations.
Requires less amount of memory.
Disadvantages of Mini Batch Gradient Descent
Mini-batch gradient descent does not guarantee good convergence,
If the learning rate is too small, the convergence rate will be slow. If it is too large, the loss function will oscillate or even deviate at the minimum value.
"
Deep Learning,SGD with Momentum,"SGD with Momentum is a stochastic optimization method that adds a momentum term to regular stochastic gradient descent. Momentum simulates the inertia of an object when it is moving, that is, the direction of the previous update is retained to a certain extent during the update, while the current update gradient is used to fine-tune the final update direction. In this way, you can increase the stability to a certain extent, so that you can learn faster, and also have the ability to get rid of local optimization.
Advantages of SGD with momentum
Momentum helps to reduce the noise.
Exponential Weighted Average is used to smoothen the curve.
Disadvantage of SGD with momentum
Extra hyperparameter is added.
"
Deep Learning,Nesterov Accelerated Gradient,"in Nesterov Accelerated Gradient, we apply the velocity vt to the parameters θ to compute interim parameters θ. We then compute the gradient using the interim parameters
We can view Nesterov Accelerated Gradients as the correction factor for Momentum method. Consider the case when the velocity added to the parameters gives you immediate unwanted high loss, e.g., exploding gradient case. In this case, the Momentum method can be very slow since the optimization path taken exhibits large oscillations. In Nesterov Accelerated Gradient case, you can view it like peeking through the interim parameters where the added velocity will lead the parameters. If the velocity update leads to bad loss, then the gradients will direct the update back towards θ𝑡. This help Nesterov Accelerated Gradient to avoid the oscillations.
"
Deep Learning,AdaGrad(Adaptive Gradient Descent),"In all the algorithms that we discussed previously the learning rate remains constant. The intuition behind AdaGrad is can we use different Learning Rates for each and every neuron for each and every hidden layer based on different iterations.
Advantages of AdaGrad
Learning Rate changes adaptively with iterations.
It is able to train sparse data as well.
Disadvantage of AdaGrad
If the neural network is deep the learning rate becomes very small number which will cause dead neuron problem.
"
Deep Learning,RMS-Prop (Root Mean Square Propagation),"RMS-Prop is a special version of Adagrad in which the learning rate is an exponential average of the gradients instead of the cumulative sum of squared gradients. RMS-Prop basically combines momentum with AdaGrad.
or
Advantages of RMS-Prop
In RMS-Prop learning rate gets adjusted automatically and it chooses a different learning rate for each parameter.
Disadvantages of RMS-Prop
Slow Learning
"
Deep Learning,AdaDelta,"Adadelta is an extension of Adagrad and it also tries to reduce Adagrad’s aggressive, monotonically reducing the learning rate and remove decaying learning rate problem. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size w.
In Adadelta we do not need to set the default learning rate as we take the ratio of the running average of the previous parameters to the current gradient.
E – running average 
Advantages of Adadelta
The main advantage of AdaDelta is that we do not need to set a default learning rate.
Disadvantages of Adadelta
Computationally expensive
"
Deep Learning,Adam (Adaptive Moment Estimation),"Adam optimizer is one of the most popular and famous gradient descent optimization algorithms. It is a method that computes adaptive learning rates for each parameter. It stores both the decaying average of the past gradients, similar to momentum, and also the decaying average of the past squared gradients, similar to RMS-Prop and Adadelta. Thus, it combines the advantages of both the methods.
We compute the decaying averages of past and past squared gradients mt and vt respectively as follows:
They counteract these biases by computing bias-corrected first and second moment estimates:
Advantages of Adam
Easy to implement
Computationally efficient.
Little memory requirements.
"
Deep Learning,How to choose optimizers?,"If the data is sparse, use the self-applicable methods, namely Adagrad, Adadelta, RMSprop, Adam.
RMSprop, Adadelta, Adam have similar effects in many cases.
Adam just added bias-correction and momentum on the basis of RMSprop,
As the gradient becomes sparse, Adam will perform better than RMSprop.
"
Deep Learning,Step size in Gradient Descent,"Choosing a good step-size, or learning rate, is important in gradient descent. If the step-size is too small, gradient descent can be slow. If the step-size is chosen too large, gradient descent can overshoot, fail to converge, or even diverge
There are two simple heuristics: 
When the function value increases after a gradient step, the step-size was too large. Undo the step and decrease the step-size. 
When the function value decreases the step could have been larger. Try to increase the step-size.
"
Deep Learning,Solving a Linear Equation System with Gradient Descent,"When we solve linear equations of the form Ax = b, in practice we solve Ax−b = 0 approximately by finding x∗ that minimizes the squared error 
∥Ax − b∥^2 = (Ax − b) ⊤(Ax − b)
 if we use the Euclidean norm. The gradient of (7.9) with respect to x is 
∇x = 2(Ax − b) ⊤A
We can use this gradient directly in a gradient descent algorithm. However, for this particular special case, it turns out that there is an analytic solution, which can be found by setting the gradient to zero.
When applied to the solution of linear systems of equations Ax = b, gradient descent may converge slowly. The speed of convergence of gradient descent is dependent on the condition number κ = σ(A)/max σ(A)min, which is the ratio of the maximum to the minimum singular value of A. The condition number essentially measures the ratio of the most curved direction versus the least curved direction, which corresponds to our imagery that poorly conditioned problems are long, thin valleys: They are very curved in one direction, but very flat in the other. Instead of directly solving Ax = b, one could instead solve P −1 (Ax − b) = 0, where P is called the preconditioner. The goal is to design P −1 such that P −1A has a better condition number, but at the same time P −1 is easy to compute.
"
Deep Learning,Gradient Descent With Momentum,"Gradient descent with momentum (Rumelhart et al., 1986) is a method that introduces an additional term to remember what happened in the previous iteration. This memory dampens oscillations and smoothes out the gradient updates. Continuing the ball analogy, the momentum term emulates the phenomenon of a heavy ball that is reluctant to change directions. The idea is to have a gradient update with memory to implement a moving average. The momentum-based method remembers the update ∆xi at each iteration i and determines the next update as a linear combination of the current and previous gradients 
xi+1 = xi − γi((∇f)(xi))⊤ + α∆xi
∆xi = xi − xi−1 = α∆xi−1 − γi−1((∇f)(xi−1))⊤ ,
where α ∈ [0, 1]. Sometimes we will only know the gradient approximately. In such cases, the momentum term is useful since it averages out different noisy estimates of the gradient. One particularly useful way to obtain an approximate gradient is by using a stochastic approximation, which we discuss next
"
Deep Learning,Stochastic Gradient Descent,"Stochastic gradient descent descent (often shortened as SGD) is a stochastic approximation of the gradient descent method for minimizing an objective function that is written as a sum of differentiable functions. The word stochastic here refers to the fact that we acknowledge that we do not know the gradient precisely, but instead only know a noisy approximation to it. By constraining the probability distribution of the approximate gradients, we can still theoretically guarantee that SGD will converge.
Why should one consider using an approximate gradient? A major reason is practical implementation constraints, such as the size of central processing unit (CPU)/graphics processing unit (GPU) memory or limits on computational time. We can think of the size of the subset used to estimate the gradient in the same way that we thought of the size of a sample when estimating empirical means. Large mini-batch sizes will provide accurate estimates of the gradient, reducing the variance in the parameter update. Furthermore, large mini-batches take advantage of highly optimized matrix operations in vectorized implementations of the cost and gradient. The reduction in variance leads to more stable convergence, but each gradient calculation will be more expensive.
In contrast, small mini-batches are quick to estimate. If we keep the mini-batch size small, the noise in our gradient estimate will allow us to get out of some bad local optima, which we may otherwise get stuck in. In machine learning, optimization methods are used for training by minimizing an objective function on the training data, but the overall goal is to improve generalization performance. Since the goal in machine learning does not necessarily need a precise estimate of the minimum of the objective function, approximate gradients using mini-batch approaches have been widely used. Stochastic gradient descent is very effective in large-scale machine learning problems.
"
Deep Learning,Constrained Optimization and Lagrange Multipliers,"we consider the constrained optimization problem 
min x f(x) 
subject to gi(x) ⩽ 0 for all i = 1, . . . , m .
We associate to problem the Lagrangian by introducing the Lagrange multipliers λi ⩾ 0 corresponding to each inequality constraint respectively so that
L(x,λ) = f(x) +sum_i=1..m(λi * gi(x)) = f(x) + λ ⊤ g(x), 
where in the last line we have concatenated all constraints gi(x) into a vector g(x), and all the Lagrange multipliers into a vector λ ∈ Rm. 
The associated Lagrangian dual problem is given by problem 
max λ∈Rm D(λ) 
subject to λ ⩾ 0 , 
where λ are the dual variables and D(λ) = minx∈Rd L(x,λ).
In contrast to the original optimization problem, which has constraints, minx∈Rd L(x,λ) is an unconstrained optimization problem for a given value of λ. If solving minx∈Rd L(x,λ) is easy, then the overall problem is easy to solve. We can see this by observing from that L(x,λ) is affine with respect to λ. Therefore minx∈Rd L(x,λ) is a pointwise minimum of affine functions of λ, and hence D(λ) is concave even though f(·) and gi(·) may be nonconvex. The outer problem, maximization over λ, is the maximum of a concave function and can be efficiently computed.
Assuming f(·) and gi(·) are differentiable, we find the Lagrange dual problem by differentiating the Lagrangian with respect to x, setting the differential to zero, and solving for the optimal value.
"
Deep Learning,What is an activation function?,"Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron. That is exactly what an activation function does in an ANN as well. It takes in the output signal from the previous cell and converts it into some form that can be taken as input to the next cell.
"
Deep Learning,Why do we need activation functions?,"They help in keeping the value of the output from the neuron restricted to a certain limit as per our requirement. This is important because input into the activation function is W*x + b where W is the weights of the cell and the x is the inputs and then there is the bias b added to that. This value if not restricted to a certain limit can go very high in magnitude especially in case of very deep neural networks that have millions of parameters. This will lead to computational issues. For example, there are some activation functions (like softmax) that out specific values for different values of input (0 or 1).
The most important feature in an activation function is its ability to add non-linearity into a neural network. 
What if we use an ANN with a single cell but without an activation function. So our output is basically W*x + b. But this is no good because W*x also has a degree of 1, hence linear and this is basically identical to a linear classifier.
What if we stack multiple layers. Let’s represent nᵗʰ layer as a function fₙ(x). So we have:
o(x) = fₙ(fₙ₋₁(….f₁(x))
However, this is also not complex enough especially for problems with very high patterns such as that faced in computer vision or natural language processing.
In order to make the model get the power (aka the higher degree complexity) to learn the non-linear patterns, specific non-linear layers (activation functions) are added in between.
"
Deep Learning,Desirable features of an activation function,"Vanishing Gradient problem: Neural Networks are trained using the process gradient descent. The gradient descent consists of the backward propagation step which is basically chain rule to get the change in weights in order to reduce the loss after every epoch. Consider a two-layer network and the first layer is represented as f₁(x) and the second layer is represented as f₂(x). The overall network is o(x) = f₂(f₁(x)). If we calculate weights during the backward pass, we get o`(x) = f₂(x)*f₁`(x). Here f₁(x) is itself a compound function consisting of Act(W₁*x₁ + b₁) where Act is the activation function after layer 1. Applying chain rule again, we clearly see that f₁`(x) = Act(W₁*x₁ + b₁)*x₁ which means it also depends directly on the activation value. Now imagine such a chain rule going through multiple layers while backpropagation. If the value of Act() is between 0 and 1, then several such values will get multiplied to calculate the gradient of the initial layers. This reduces the value of the gradient for the initial layers and those layers are not able to learn properly. In other words, their gradients tend to vanish because of the depth of the network and the activation shifting the value to zero. This is called the vanishing gradient problem. So we want our activation function to not shift the gradient towards zero.
Zero-Centered: Output of the activation function should be symmetrical at zero so that the gradients do not shift to a particular direction.
Computational Expense: Activation functions are applied after every layer and need to be calculated millions of times in deep networks. Hence, they should be computationally inexpensive to calculate.
Differentiable: As mentioned, neural networks are trained using the gradient descent process, hence the layers in the model need to differentiable or at least differentiable in parts. This is a necessary requirement for a function to work as activation function layer.
"
Deep Learning,List Activation functions,"Sigmoid
σ(x) = 1 / (1 + e^-x)
0 <= σ(x) <= 1
Softmax - The softmax is a more generalised form of the sigmoid. It is used in multi-class classification problems. Similar to sigmoid, it produces values in the range of 0–1 therefore it is used as the final layer in classification models.
Softmax(xi) = e^xi / sum_j(e^xj)
Tanh - If you compare it to sigmoid, it solves just one problem of being zero-centred.
ReLU: ReLU (Rectified Linear Unit) is defined as f(x) = max(0,x):
This is a widely used activation function, especially with Convolutional Neural networks. It is easy to compute and does not saturate and does not cause the Vanishing Gradient Problem. It has just one issue of not being zero centred. It suffers from “dying ReLU” problem. Since the output is zero for all negative inputs. It causes some nodes to completely die and not learn anything.
Another problem with ReLU is of exploding the activations since it higher limit is, well, inf. This sometimes leads to unusable nodes.
Leaky ReLU and Parametric ReLU: It is defined as f(x) = max(αx, x)
the figure is for α = 0.1
Here α is a hyperparameter generally set to 0.01. Clearly, Leaky ReLU solves the “dying ReLU” problem to some extent. Note that, if we set α as 1 then Leaky ReLU will become a linear function f(x) = x and will be of no use. Hence, the value of α is never set close to 1. If we set α as a hyperparameter for each neuron separately, we get parametric ReLU or PReLU.
ReLU6: It is basically ReLU restricted on the positive side and it is defined as f(x) = min(max(0,x),6)
This helps to stop blowing up the activation thereby stopping the gradients to explode(going to inf) as well another of the small issues that occur with normal ReLUs.
Notable non-linear activations coming out of latest research
Swish: This was proposed in 2017 by Ramachandran et.al. It is defined as f(x) = x*sigmoid(x).
It is slightly better in performance as compared to ReLU since its graph is quite similar to ReLU. However, because it does not change abruptly at a point as ReLU does at x = 0, this makes it easier to converge while training.
But, the drawback of Swish is that it is computationally expensive. To solve that we come to the next version of Swish.
Hard-Swish or H-Swish: This is defined as:
The best part is that it is almost similar to swish but it is less expensive computationally since it replaces sigmoid (exponential function) with a ReLU (linear type).
"
Deep Learning,Applications of Sigmoid ,"The sigmoid can be used simply as an activation function throughout a neural network, applying it to the outputs of each network layer. It isn’t used as much nowadays, however, because it has a couple of inefficiencies. 
The first is the problem of saturating gradients. Looking at its graph, we can see that the sigmoid has a strong slope in the middle, but at the ends, its slope is very shallow. This is a problem for learning. At a high level, when we run gradient descent, many of the neurons in our network will be outputting values in the shallow regions of the sigmoid. Changing the network weights will then have little effect on its overall output, and learning comes to a halt.
In a little more detail, to run backpropagation and learn, we must take the gradient of the loss function with respect to each parameter in our network. At first, some neurons may be outputting values in the middle of the sigmoid range, where the slope is strong. But as we make updates, we move up or down this slope and quickly end up in a shallow region. The magnitude of our gradient then becomes smaller and smaller, meaning we take smaller and smaller learning steps. Learning is not very efficient this way.
The other problem with the sigmoid is that it’s not symmetric about the origin. In the brain, neurons either fire or don’t, so we may have the intuition that neuron activations should be zero or one. Despite this, researchers have actually found that neural networks learn better when activations are centered around zero. This is one of the reasons it’s a good idea to standardize your data (i.e., shift it to have mean zero) before feeding it into a neural network. It’s also one of the reasons for batch normalization, a similar process where we standardize our network activations at intermediate layers rather than just at the start.
If you look at the beginning of the previous section, you’ll see that the tanh function ranges from -1 to one and is centered around zero. For this reason, it’s often preferable to the sigmoid. It also has the problem of saturating gradients, though. The most common activation function nowadays is the rectified linear unit (ReLU):
This function has a strong slope everywhere to the right of zero, although it’s obviously not symmetric around zero. So, tanh has saturating gradients, and ReLU is non-symmetric. In practice, the former is a bigger problem than the latter. The moral here, though, is that the sigmoid is the worst of both worlds on these fronts.
Despite all this, the sigmoid still has a place in modern machine learning: binary classification. In binary classification, we categorize inputs as one of two classes. If we’re using neural networks, the output of our network must be a number between zero and one, representing the probability that the input belongs to class one (with the probability for class two being immediately inferable).
The output layer of such a network consists of a single neuron. Consider the output value of this neuron. Before applying any activation function, it can be any real number, which is no good. If we apply a ReLU, it will be positive (or zero). If we use tanh, it will be between -1 and one. None of these work. We must apply a sigmoid to this last neuron. We need a number between zero and one, and we still need the activation function to be smooth for the purposes of training. The sigmoid is the right choice.
"
Deep Learning,What are convolutional neural networks?,"Neural networks are a subset of machine learning, and they are at the heart of deep learning algorithms. They are comprised of node layers, containing an input layer, one or more hidden layers, and an output layer. Each node connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.
While we primarily focused on feedforward networks in that article, there are various types of neural nets, which are used for different use cases and data types. For example, recurrent neural networks are commonly used for natural language processing and speech recognition whereas convolutional neural networks (ConvNets or CNNs) are more often utilized for classification and computer vision tasks. Prior to CNNs, manual, time-consuming feature extraction methods were used to identify objects in images. However, convolutional neural networks now provide a more scalable approach to image classification and object recognition tasks, leveraging principles from linear algebra, specifically matrix multiplication, to identify patterns within an image. That said, they can be computationally demanding, requiring graphical processing units (GPUs) to train models. 
"
Deep Learning,How do convolutional neural networks work?,"Convolutional neural networks are distinguished from other neural networks by their superior performance with image, speech, or audio signal inputs. They have three main types of layers, which are:
Convolutional layer
Pooling layer
Fully-connected (FC) layer
The convolutional layer is the first layer of a convolutional network. While convolutional layers can be followed by additional convolutional layers or pooling layers, the fully-connected layer is the final layer. With each layer, the CNN increases in its complexity, identifying greater portions of the image. Earlier layers focus on simple features, such as colors and edges. As the image data progresses through the layers of the CNN, it starts to recognize larger elements or shapes of the object until it finally identifies the intended object.
"
Deep Learning,Convolutional Layer,"The convolutional layer is the core building block of a CNN, and it is where the majority of computation occurs. It requires a few components, which are input data, a filter, and a feature map. Let’s assume that the input will be a color image, which is made up of a matrix of pixels in 3D. This means that the input will have three dimensions—a height, width, and depth—which correspond to RGB in an image. We also have a feature detector, also known as a kernel or a filter, which will move across the receptive fields of the image, checking if the feature is present. This process is known as a convolution.
The feature detector is a two-dimensional (2-D) array of weights, which represents part of the image. While they can vary in size, the filter size is typically a 3x3 matrix; this also determines the size of the receptive field. The filter is then applied to an area of the image, and a dot product is calculated between the input pixels and the filter. This dot product is then fed into an output array. Afterwards, the filter shifts by a stride, repeating the process until the kernel has swept across the entire image. The final output from the series of dot products from the input and the filter is known as a feature map, activation map, or a convolved feature.
As you can see in the image above, each output value in the feature map does not have to connect to each pixel value in the input image. It only needs to connect to the receptive field, where the filter is being applied. Since the output array does not need to map directly to each input value, convolutional (and pooling) layers are commonly referred to as “partially connected” layers. However, this characteristic can also be described as local connectivity.
Note that the weights in the feature detector remain fixed as it moves across the image, which is also known as parameter sharing. Some parameters, like the weight values, adjust during training through the process of backpropagation and gradient descent. However, there are three hyperparameters which affect the volume size of the output that need to be set before the training of the neural network begins. These include:
1. The number of filters affects the depth of the output. For example, three distinct filters would yield three different feature maps, creating a depth of three. 
2. Stride is the distance, or number of pixels, that the kernel moves over the input matrix. While stride values of two or greater is rare, a larger stride yields a smaller output.
3. Zero-padding is usually used when the filters do not fit the input image. This sets all elements that fall outside of the input matrix to zero, producing a larger or equally sized output. There are three types of padding:
Valid padding: This is also known as no padding. In this case, the last convolution is dropped if dimensions do not align.
Same padding: This padding ensures that the output layer has the same size as the input layer
Full padding: This type of padding increases the size of the output by adding zeros to the border of the input.
After each convolution operation, a CNN applies a Rectified Linear Unit (ReLU) transformation to the feature map, introducing nonlinearity to the model.
As we mentioned earlier, another convolution layer can follow the initial convolution layer. When this happens, the structure of the CNN can become hierarchical as the later layers can see the pixels within the receptive fields of prior layers.  As an example, let’s assume that we’re trying to determine if an image contains a bicycle. You can think of the bicycle as a sum of parts. It is comprised of a frame, handlebars, wheels, pedals, et cetera. Each individual part of the bicycle makes up a lower-level pattern in the neural net, and the combination of its parts represents a higher-level pattern, creating a feature hierarchy within the CNN.
Ultimately, the convolutional layer converts the image into numerical values, allowing the neural network to interpret and extract relevant patterns.
"
Deep Learning,Pooling Layer,"Pooling layers, also known as downsampling, conducts dimensionality reduction, reducing the number of parameters in the input. Similar to the convolutional layer, the pooling operation sweeps a filter across the entire input, but the difference is that this filter does not have any weights. Instead, the kernel applies an aggregation function to the values within the receptive field, populating the output array. There are two main types of pooling:
Max pooling: As the filter moves across the input, it selects the pixel with the maximum value to send to the output array. As an aside, this approach tends to be used more often compared to average pooling.
Average pooling: As the filter moves across the input, it calculates the average value within the receptive field to send to the output array.
While a lot of information is lost in the pooling layer, it also has a number of benefits to the CNN. They help to reduce complexity, improve efficiency, and limit risk of overfitting. 
"
Deep Learning,Fully-Connected Layer,"The name of the full-connected layer aptly describes itself. As mentioned earlier, the pixel values of the input image are not directly connected to the output layer in partially connected layers. However, in the fully-connected layer, each node in the output layer connects directly to a node in the previous layer.
This layer performs the task of classification based on the features extracted through the previous layers and their different filters. While convolutional and pooling layers tend to use ReLu functions, FC layers usually leverage a softmax activation function to classify inputs appropriately, producing a probability from 0 to 1.
"
Deep Learning,Recurrent Neural Networks,"A recurrent neural network (RNN) is a type of artificial neural network which uses sequential data or time series data. These deep learning algorithms are commonly used for ordinal or temporal problems, such as language translation, natural language processing (nlp), speech recognition, and image captioning; they are incorporated into popular applications such as Siri, voice search, and Google Translate. Like feedforward and convolutional neural networks (CNNs), recurrent neural networks utilize training data to learn. They are distinguished by their “memory” as they take information from prior inputs to influence the current input and output. While traditional deep neural networks assume that inputs and outputs are independent of each other, the output of recurrent neural networks depend on the prior elements within the sequence. While future events would also be helpful in determining the output of a given sequence, unidirectional recurrent neural networks cannot account for these events in their predictions. 
Another distinguishing characteristic of recurrent networks is that they share parameters across each layer of the network. While feedforward networks have different weights across each node, recurrent neural networks share the same weight parameter within each layer of the network. That said, these weights are still adjusted in the through the processes of backpropagation and gradient descent to facilitate reinforcement learning.
Recurrent neural networks leverage backpropagation through time (BPTT) algorithm to determine the gradients, which is slightly different from traditional backpropagation as it is specific to sequence data. The principles of BPTT are the same as traditional backpropagation, where the model trains itself by calculating errors from its output layer to its input layer. These calculations allow us to adjust and fit the parameters of the model appropriately. BPTT differs from the traditional approach in that BPTT sums errors at each time step whereas feedforward networks do not need to sum errors as they do not share parameters across each layer.
Through this process, RNNs tend to run into two problems, known as exploding gradients and vanishing gradients. These issues are defined by the size of the gradient, which is the slope of the loss function along the error curve. When the gradient is too small, it continues to become smaller, updating the weight parameters until they become insignificant—i.e. 0. When that occurs, the algorithm is no longer learning. Exploding gradients occur when the gradient is too large, creating an unstable model. In this case, the model weights will grow too large, and they will eventually be represented as NaN. One solution to these issues is to reduce the number of hidden layers within the neural network, eliminating some of the complexity in the RNN model.
"
Deep Learning,Long short-term memory (LSTM),"This is a popular RNN architecture, which was introduced by Sepp Hochreiter and Juergen Schmidhuber as a solution to vanishing gradient problem. In their paper (PDF, 388 KB) (link resides outside IBM), they work to address the problem of long-term dependencies. That is, if the previous state that is influencing the current prediction is not in the recent past, the RNN model may not be able to accurately predict the current state. As an example, let’s say we wanted to predict the italicized words in following, “Alice is allergic to nuts. She can’t eat peanut butter.” The context of a nut allergy can help us anticipate that the food that cannot be eaten contains nuts. However, if that context was a few sentences prior, then it would make it difficult, or even impossible, for the RNN to connect the information. To remedy this, LSTMs have “cells” in the hidden layers of the neural network, which have three gates–an input gate, an output gate, and a forget gate. These gates control the flow of information which is needed to predict the output in the network.  For example, if gender pronouns, such as “she”, was repeated multiple times in prior sentences, you may exclude that from the cell state.
The different steps in LSTM include the following.
Step 1:The network helps decide what needs to be remembered and forgotten
Step 2:The selection is made for cell state values that can be updated
Step 3: The network decides as to what can be made as part of the current output
"
Deep Learning,LSTM architecture,"In the LSTM figure, we can see that we have 8 different weight parameters (4 associated with the hidden state (cell state) and 4 associated with the input vector). We also have 4 different bias parameters. To better understand this we can use the following equations and better understand the operations in LSTM cell.
Here, with the help of the above equations, we can clearly see a total of 4 biases and 8 weights. Let's take an example.
Seq_len of the input sentence (S)= 12
embedding dimension (E)= 30
No of LSTM cells (hidden units) (H)= 10
Batch_size (B) = 1
The input (x) will be batch size * embedding dimension = B*D
The previous hidden state will be batch size * hidden units = B*H
Equation 1: forget gate = [(1*10).(10*10)+(1*30).(30*10) + (1*10)]
= (1*10) = (B*H)
Equation 2: update gate = [(1*10).(10*10)+(1*30).(30*10) + (1*10)]
= (1*10) = (B*H)
Equation 3: candidate memory=[(1*10).(10*10)+(1*30).(30*10)+(1*10)]
= (1*10) = (B*H)
Equation 4: output gate =[(1*10).(10*10)+(1*30).(30*10) + (1*10)]
= (1*10) = (B*H)
Since all weights follow the same structure these can be combined together can then multiplied with the respective output. weights associated with hidden state are called kernel weights and weights associated with input are called recurrent kernel weights.
Note:
1. Since LSTM processes data in sequential nature. It will receive 1 word at a time and the same LSTM cell will receive the next subsequent words. No. of LSTM cell doesn’t mean that many times LSTM is repeated. It means it can be unfolded up to the sequence length. In the actual LSTM cell, the same cell will receive all the words one by one.
2. Sequence length does not have any effect on the weights and bias dimension. It can be clearly seen in the above calculations.
3. Weight is multiplied by taking the transpose of weight, but here I have rearranged weight and input for simplification.
To see all the weights and bias dimensions, I have put them in a table and named them accordingly as per equations.
Here, with the help of the above equations, we can clearly see a total of 4 biases and 8 weights. Let's take an example.
Seq_len of the input sentence (S)= 12
embedding dimension (E)= 30
No of LSTM cells (hidden units) (H)= 10
Batch_size (B) = 1
The input (x) will be batch size * embedding dimension = B*D
The previous hidden state will be batch size * hidden units = B*H
Equation 1: forget gate = [(1*10).(10*10)+(1*30).(30*10) + (1*10)]
= (1*10) = (B*H)
Equation 2: update gate = [(1*10).(10*10)+(1*30).(30*10) + (1*10)]
= (1*10) = (B*H)
Equation 3: candidate memory=[(1*10).(10*10)+(1*30).(30*10)+(1*10)]
= (1*10) = (B*H)
Equation 4: output gate =[(1*10).(10*10)+(1*30).(30*10) + (1*10)]
= (1*10) = (B*H)
Since all weights follow the same structure these can be combined together can then multiplied with the respective output. weights associated with hidden state are called kernel weights and weights associated with input are called recurrent kernel weights.
Note:
1. Since LSTM processes data in sequential nature. It will receive 1 word at a time and the same LSTM cell will receive the next subsequent words. No. of LSTM cell doesn’t mean that many times LSTM is repeated. It means it can be unfolded up to the sequence length. In the actual LSTM cell, the same cell will receive all the words one by one.
2. Sequence length does not have any effect on the weights and bias dimension. It can be clearly seen in the above calculations.
3. Weight is multiplied by taking the transpose of weight, but here I have rearranged weight and input for simplification.
To see all the weights and bias dimensions, I have put them in a table and named them accordingly as per equations.
"
Deep Learning,Regularization Methods for Neural Networks,"The simplest and perhaps most common regularization method is to add a penalty to the loss function in proportion to the size of the weights in the model.
Weight Regularization (weight decay): Penalize the model during training based on the magnitude of the weights.
This will encourage the model to map the inputs to the outputs of the training dataset in such a way that the weights of the model are kept small. This approach is called weight regularization or weight decay and has proven very effective for decades for both simpler linear models and neural networks.
A simple alternative to gathering more data is to reduce the size of the model or improve regularization, by adjusting hyperparameters such as weight decay coefficients…
Below is a list of five of the most common additional regularization methods.
Activity Regularization: Penalize the model during training base on the magnitude of the activations.
Weight Constraint: Constrain the magnitude of weights to be within a range or below a limit.
Dropout: Probabilistically remove inputs during training.
Noise: Add statistical noise to inputs during training.
Early Stopping: Monitor model performance on a validation set and stop training when performance degrades.
Most of these methods have been demonstrated (or proven) to approximate the effect of adding a penalty to the loss function.
Each method approaches the problem differently, offering benefits in terms of a mixture of generalization performance, configurability, and/or computational complexity.
"
Deep Learning,Batch Normalization,"To understand what happens without normalization, let’s look at an example with just two features that are on drastically different scales. Since the network output is a linear combination of each feature vector, this means that the network learns weights for each feature that are also on different scales. Otherwise, the large feature will simply drown out the small feature.
Then during gradient descent, in order to “move the needle” for the Loss, the network would have to make a large update to one weight compared to the other weight. This can cause the gradient descent trajectory to oscillate back and forth along one dimension, thus taking more steps to reach the minimum.
Just like the parameters (eg. weights, bias) of any network layer, a Batch Norm layer also has parameters of its own:
Two learnable parameters called beta and gamma.
Two non-learnable parameters (Mean Moving Average and Variance Moving Average) are saved as part of the ‘state’ of the Batch Norm layer.
These parameters are per Batch Norm layer. So if we have, say, three hidden layers and three Batch Norm layers in the network, we would have three learnable beta and gamma parameters for the three layers. Similarly for the Moving Average parameters.
During training, we feed the network one mini-batch of data at a time. During the forward pass, each layer of the network processes that mini-batch of data. The Batch Norm layer processes its data as follows:
1. Activations
The activations from the previous layer are passed as input to the Batch Norm. There is one activation vector for each feature in the data.
2. Calculate Mean and Variance
For each activation vector separately, calculate the mean and variance of all the values in the mini-batch.
3. Normalize
Calculate the normalized values for each activation feature vector using the corresponding mean and variance. These normalized values now have zero mean and unit variance.
4. Scale and Shift
This step is the huge innovation introduced by Batch Norm that gives it its power. Unlike the input layer, which requires all normalized values to have zero mean and unit variance, Batch Norm allows its values to be shifted (to a different mean) and scaled (to a different variance). It does this by multiplying the normalized values by a factor, gamma, and adding to it a factor, beta. Note that this is an element-wise multiply, not a matrix multiply.
What makes this innovation ingenious is that these factors are not hyperparameters (ie. constants provided by the model designer) but are trainable parameters that are learned by the network. In other words, each Batch Norm layer is able to optimally find the best factors for itself, and can thus shift and scale the normalized values to get the best predictions.
5. Moving Average
In addition, Batch Norm also keeps a running count of the Exponential Moving Average (EMA) of the mean and variance. During training, it simply calculates this EMA but does not do anything with it. At the end of training, it simply saves this value as part of the layer’s state, for use during the Inference phase.
We will return to this point a little later when we talk about Inference. The Moving Average calculation uses a scalar ‘momentum’ denoted by alpha below. This is a hyperparameter that is used only for Batch Norm moving averages and should not be confused with the momentum that is used in the Optimizer.
Vector Shapes
Below, we can see the shapes of these vectors. The values that are involved in computing the vectors for a particular feature are also highlighted in red. However, remember that all feature vectors are computed in a single matrix operation.
Shapes of Batch Norm vectors (Image by Author)
After the forward pass, we do the backward pass as normal. Gradients are calculated and updates are done for all layer weights, as well as for all beta and gamma parameters in the Batch Norm layers.
"
Deep Learning,Batch Normalization during Inference,"As we discussed above, during Training, Batch Norm starts by calculating the mean and variance for a mini-batch. However, during Inference, we have a single sample, not a mini-batch. How do we obtain the mean and variance in that case?
Here is where the two Moving Average parameters come in — the ones that we calculated during training and saved with the model. We use those saved mean and variance values for the Batch Norm during Inference.
Ideally, during training, we could have calculated and saved the mean and variance for the full data. But that would be very expensive as we would have to keep values for the full dataset in memory during training. Instead, the Moving Average acts as a good proxy for the mean and variance of the data. It is much more efficient because the calculation is incremental — we have to remember only the most recent Moving Average.
"
Deep Learning,Order of placement of Batch Norm layer,"There are two opinions for where the Batch Norm layer should be placed in the architecture — before and after activation. The original paper placed it before, although I think you will find both options frequently mentioned in the literature. Some say ‘after’ gives better results.
"
Deep Learning,What is Layer Normalization?,"Layer Normalization was proposed by researchers Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. In layer normalization, all neurons in a particular layer effectively have the same distribution across all features for a given input.
For example, if each input has d features, it’s a d-dimensional vector. If there are B elements in a batch, the normalization is done along the length of the d-dimensional vector and not across the batch of size B.
Normalizing across all features but for each of the inputs to a specific layer removes the dependence on batches. This makes layer normalization well suited for sequence models such as transformers and recurrent neural networks (RNNs) that were popular in the pre-transformer era.
Here’s an example showing the computation of the mean and variance for layer normalization. We consider the example of a mini-batch containing three input samples, each with four features.
From these steps, we see that they’re similar to the steps we had in batch normalization. However, instead of the batch statistics, we use the mean and variance corresponding to specific input to the neurons in a particular layer, say k. This is equivalent to normalizing the output vector from the layer k-1.
"
Deep Learning,"
Batch Normalization vs Layer Normalization","Batch normalization normalizes each feature independently across the mini-batch. Layer normalization normalizes each of the inputs in the batch independently across all features.
As batch normalization is dependent on batch size, it’s not effective for small batch sizes. Layer normalization is independent of the batch size, so it can be applied to batches with smaller sizes as well.
Batch normalization requires different processing at training and inference times. As layer normalization is done along the length of input to a specific layer, the same set of operations can be used at both training and inference times.
Batch Normalization in Convolutional Neural Networks
Batch Norm works in a very similar way in Convolutional Neural Networks. Although we could do it in the same way as before, we have to follow the convolutional property.
In convolutions, we have shared filters that go along the feature maps of the input (in images, the feature map is generally the height and width). These filters are the same on every feature map. It is then reasonable to normalize the output, in the same way, sharing it over the feature maps.
In other words, this means that the parameters used to normalize are calculated along with each entire feature map. In a regular Batch Norm, each feature would have a different mean and standard deviation. Here, each feature map will have a single mean and standard deviation, used on all the features it contains.
"
Deep Learning,Semantic Segmentation,"Semantic segmentation is a natural step in the progression from coarse to fine inference: The origin could be located at classification, which consists of making a prediction for a whole input. The next step is localization / detection, which provide not only the classes but also additional information regarding the spatial location of those classes. Finally, semantic segmentation achieves fine-grained inference by making dense predictions inferring labels for every pixel, so that each pixel is labeled with the class of its enclosing object ore region.
"
Deep Learning,Metrics for Semantic Segmentation,"Let's discuss the metrics which are generally used to understand and evaluate the results of a model.
Pixel Accuracy
Pixel accuracy is the most basic metric which can be used to validate the results. Accuracy is obtained by taking the ratio of correctly classified pixels w.r.t total pixels
Accuracy = (TP+TN) / (TP+TN+FP+FN)
The main disadvantage of using such a technique is the result might look good if one class overpowers the other. Say for example the background class covers 90% of the input image we can get an accuracy of 90% by just classifying every pixel as background
Intersection Over Union
IOU is defined as the ratio of intersection of ground truth and predicted segmentation outputs over their union. If we are calculating for multiple classes, IOU of each class is calculated and their mean is taken. It is a better metric compared to pixel accuracy as if every pixel is given as background in a 2 class input the IOU value is (90/100+0/100)/2 i.e 45% IOU which gives a better representation as compared to 90% accuracy.
When using this approximation, IoU becomes differentiable and can be used as a loss function. The comparison between IoU loss and Binary Cross Entropy loss is made by testing two deep neural network models on multiple datasets and data splits.
Frequency weighted IOU
This is an extension over mean IOU which we discussed and is used to combat class imbalance. If one class dominates most part of the images in a dataset like for example background, it needs to be weighed down compared to other classes. Thus instead of taking the mean of all the class results, a weighted mean is taken based on the frequency of the class region in the dataset.
F1 Score
The metric popularly used in classification F1 Score can be used for segmentation task as well to deal with class imbalance.
Average Precision
The average precision (AP) is a way to summarize the precision-recall curve into a single value representing the average of all precisions. The AP is calculated according to the next equation. Using a loop that goes through all precisions/recalls, the difference between the current and next recalls is calculated and then multiplied by the current precision. In other words, the AP is the weighted sum of precisions at each threshold where the weight is the increase in recall.
"
Deep Learning,Loss functions for Semantic Segmentation,"Cross Entropy Loss
Simple average of cross-entropy classification loss for every pixel in the image can be used as an overall function. But this again suffers due to class imbalance which FCN proposes to rectify using class weights
UNet tries to improve on this by giving more weight-age to the pixels near the border which are part of the boundary as compared to inner pixels as this makes the network focus more on identifying borders and not give a coarse output.
Focal Loss
Focal loss was designed to make the network focus on hard examples by giving more weight-age and also to deal with extreme class imbalance observed in single-stage object detectors. The same can be applied in semantic segmentation tasks as well
Dice Loss
Dice function is nothing but F1 score. This loss function directly tries to optimize F1 score. Similarly direct IOU score can be used to run optimization as well
Tversky Loss
It is a variant of Dice loss which gives different weight-age to FN and FP
Boundary loss
One variant of the boundary loss is applied to tasks with highly unbalanced segmentations. This loss’s form is that of a distance metric on space contours and not regions. In this manner, it tackles the problem posed by regional losses for highly imbalanced segmentation tasks.
Hausdorff distance
It is a technique used to measure similarity between boundaries of ground truth and predicted. It is calculated by finding out the max distance from any point in one boundary to the closest point in the other. Reducing directly the boundary loss function is a recent trend and has been shown to give better results especially in use-cases like medical image segmentation where identifying the exact boundary plays a key role.
The advantage of using a boundary loss as compared to a region based loss like IOU or Dice Loss is it is unaffected by class imbalance since the entire region is not considered for optimization, only the boundary is considered.
"
Deep Learning,What Semantic Segmentation models do you know?,"U-Net
U-Net is a convolutional neural network originally developed for segmenting biomedical images. When visualized its architecture looks like the letter U and hence the name U-Net. Its architecture is made up of two parts, the left part – the contracting path and the right part – the expansive path. The purpose of the contracting path is to capture context while the role of the expansive path is to aid in precise localization.
The contracting path is made up of two three-by-three convolutions. The convolutions are followed by a rectified linear unit and a two-by-two max-pooling computation for downsampling.
FastFCN —Fast Fully Convolutional Network
In this architecture, a Joint Pyramid Upsampling(JPU) module is used to replace dilated convolutions since they consume a lot of memory and time. It uses a fully-connected network at its core while applying JPU for upsampling. JPU upsamples the low-resolution feature maps to high-resolution feature maps.
DeepLab
In this architecture, convolutions with upsampled filters are used for tasks that involve dense prediction. Segmentation of objects at multiple scales is done via atrous spatial pyramid pooling. Finally, DCNNs are used to improve the localization of object boundaries. Atrous convolution is achieved by upsampling the filters through the insertion of zeros or sparse sampling of 
Mask R-CNN
In this architecture, objects are classified and localized using a bounding box and semantic segmentation that classifies each pixel into a set of categories. Every region of interest gets a segmentation mask. A class label and a bounding box are produced as the final output. The architecture is an extension of the Faster R-CNN. The Faster R-CNN is made up of a deep convolutional network that proposes the regions and a detector that utilizes the regions.
"
Deep Learning,What is object detection?,"Object detection is the field of computer vision that deals with the localization and classification of objects contained in an image or video.
To put it simply: Object detection comes down to drawing bounding boxes around detected objects which allow us to locate them in a given scene (or how they move through it).
"
Deep Learning,Object detection vs image segmentation,"Image segmentation is the process of defining which pixels of an object class are found in an image. 
Semantic image segmentation will mark all pixels belonging to that tag, but won’t define the boundaries of each object.
Object detection instead will not segment the object, but will clearly define the location of each individual object instance with a box.
Combining semantic segmentation with object detection leads to instance segmentation, which first detects the object instances, and then segments each within the detected boxes (known in this case as regions of interest).
"
Deep Learning,Types and modes of object detection,"Before deep learning took off in 2013, almost all object detection was done through classical machine learning techniques. Common ones included viola-jones object detection technique, scale-invariant feature transforms (SIFT), and histogram of oriented gradients.  
These would detect a number of common features across the image, and classify their clusters using logistic regression, color histograms, or random forests. Today’s deep learning-based techniques vastly outperform these.
Deep learning-based approaches use neural network architectures like RetinaNet, YOLO (You Only Look Once), CenterNet, SSD (Single Shot Multibox detector), Region proposals (R-CNN, Fast-RCNN, Faster RCNN, Cascade R-CNN) for feature detection of the object, and then identification into labels.
Object detection generally is categorized into 2 stages:
Single-stage object detectors.
A single-stage detector removes the RoI extraction process and directly classifies and regresses the candidate anchor boxes. Examples are: YOLO family (YOLOv2, YOLOv3, YOLOv4, and YOLOv5) CornerNet, CenterNet, and others.
Two-stage object detectors.
Two-stage detectors divide the object detection task into two stages: extract RoIs (Region of interest), then classify and regress the RoIs. Examples of object detection architectures that are 2 stage oriented include R-CNN, Fast-RCNN, Faster-RCNN, Mask-RCNN and others.
"
Deep Learning,R-CNN Model Family ,"Their proposed R-CNN model is comprised of three modules; they are:
Module 1: Region Proposal. Generate and extract category independent region proposals, e.g. candidate bounding boxes.
Module 2: Feature Extractor. Extract feature from each candidate region, e.g. using a deep convolutional neural network.
Module 3: Classifier. Classify features as one of the known class, e.g. linear SVM classifier model.
The R-CNN Model family includes the following:
R-CNN—This utilizes a selective search method to locate RoIs in the input images and uses a DCN (Deep Convolutional Neural Network)-based region wise classifier to classify the RoIs independently. 
SPPNet and Fast R-CNN—This is an improved version of R-CNN that deals with the extraction of the RoIs from the feature maps. This was found to be much faster than the conventional R-CNN architecture.
Faster R-CNN—This is an improved version of Fast R-CNN that was trained end to end by introducing RPN (region proposal network). An RPN is a network utilized in generating RoIs by regressing the anchor boxes. Hence, the anchor boxes are then used in the object detection task. 
Mask R-CNN adds a mask prediction branch on the Faster R-CNN, which can detect objects and predict their masks at the same time. 
R-FCN  replaces the fully connected layers with the position-sensitive score maps for better detecting objects. 
Cascade R-CNN addresses the problem of overfitting at training and quality mismatch at inference by training a sequence of detectors with increasing IoU thresholds.
"
Deep Learning,YOLO Model Family,"The model works by first splitting the input image into a grid of cells, where each cell is responsible for predicting a bounding box if the center of a bounding box falls within the cell. Each grid cell predicts a bounding box involving the x, y coordinate and the width and height and the confidence. A class prediction is also based on each cell.
For example, an image may be divided into a 7×7 grid and each cell in the grid may predict 2 bounding boxes, resulting in 94 proposed bounding box predictions. The class probabilities map and the bounding boxes with confidences are then combined into a final set of bounding boxes and class labels. The image taken from the paper below summarizes the two outputs of the model.
The YOLO family model includes the following:
YOLO uses fewer anchor boxes (divide the input image into an S × S grid) to do regression and classification. This was built using darknet neural networks.
YOLOv2 improves the performance by using more anchor boxes and a new bounding box regression method. 
YOLOv3 is an enhanced version of the v2 variant with a deeper feature detector network and minor representational changes. YOLOv3 has relatively speedy inference times with it taking roughly 30ms per inference.
YOLOv4 (YOLOv3 upgrade) works by breaking the object detection task into two pieces, regression to identify object positioning via bounding boxes and classification to determine the object's class. YOLO V4 and its successors are technically the product of a different set of researchers than versions 1-3.
YOLOv5 is an improved version of YOLOv4 with a mosaic augmentation technique for increasing the general performance of YOLOv4.
"
Deep Learning,CenterNet,"CenterNet is a deep detection architecture that removes the need for anchors and the computationally heavy NMS. It is based on the insight that box predictions can be sorted for relevance based on the location of their centers, rather than their overlap with the object. This insight is now being used in other deep learning tasks.
The CenterNet family model includes the following:
SSD places anchor boxes densely over an input image and uses features from different convolutional layers to regress and classify the anchor boxes. 
DSSD introduces a deconvolution module into SSD to combine low level and high-level features. While R-SSD uses pooling and deconvolution operations in different feature layers to combine low-level and high-level features.
RON proposes a reverse connection and an objectness prior to extracting multiscale features effectively.
RefineDet refines the locations and sizes of the anchor boxes for two times, which inherits the merits of both one-stage and two-stage approaches. 
CornerNet is another keypoint-based approach, which directly detects an object using a pair of corners. Although CornerNet achieves high performance, it still has more room to improve.
CenterNet explores the visual patterns within each bounding box. For detecting an object, this uses a triplet, rather than a pair, of keypoints. CenterNet evaluates objects as single points by predicting the x and y coordinate of the object’s center and it’s area of coverage (width and height). It is a unique technique that has proven to out-perform variants like the SSD and R-CNN family.  
"
Deep Learning,Flatten Layer vs GlobalAveragePooling,"Flatten Layer will take a tensor of any shape and transform it into a one-dimensional tensor but keeping all values in the tensor. For example a tensor (samples, 10, 10, 32) will be flattened to (samples, 10 * 10 * 32).
An architecture like this has the risk of overfitting to the training dataset. In practice, dropout layers are used to avoid overfitting.
Global Average Pooling does something different. It applies average pooling on the spatial dimensions until each spatial dimension is one, and leaves other dimensions unchanged. For example, a tensor (samples, 10, 10, 32) would be output as (samples, 1, 1, 32).
"
Deep Learning,Atrous(Dilated) Convolution,"To understand how atrous convolution differs from the standard convolution, we first need to know what receptive field is. Receptive Field is defined as the size of the region of the input feature map that produces each output element. In the case of Fig.1, the receptive field is 3x3 as each element in the output feature map sees(uses) 3x3 input elements.
Deep CNNs use a combination of Convolutions and max-pooling. This has the disadvantage that, at each step, the spatial resolution of the feature map is halved. Implanting the resultant feature map onto the original image results in sparse feature extraction. This effect can be seen in Fig. 2. The conv. filter downsamples the input image by a factor of two. Upsampling and imposing the feature map on the image shows that the responses correspond to only 1/4th of the image locations(Sparse feature extraction).
Atrous(Dilated) convolution fixes this problem and allows for dense feature extraction. This is achieved a new parameter called rate(r). Put simply, atrous convolution is akin to the standard convolution except that the weights of an atrous convolution kernel are spaced r locations apart, i.e., the kernel of dilated convolution layers are sparse.
By controlling the rate parameter, we can arbitrarily control the receptive fields of the conv. layer. This allows the conv. filter to look at larger areas of the input(receptive field) without a decrease in the spatial resolution or increase in the kernel size. 
Compared to standard convolution used in Fig. 2, dense features are extracted by using a dilated kernel with rate r=2. Dilated convolutions can be trivially implemented by just setting the dilation parameter to the required dilation rate.
"
Deep Learning,Transformers,"A transformer is a deep learning architecture developed by researchers at Google and based on the multi-head attention mechanism, proposed in a 2017 paper ""Attention Is All You Need"".
The transformer architecture is composed of an encoder and a decoder, each of which is made up of multiple layers of self-attention and feedforward neural networks. The self-attention mechanism is the heart of the transformer, allowing the model to weigh the importance of different words in a sentence based on their affinity with each other. This is similar to how a human might read a sentence, focusing on the most relevant parts of the text rather than reading it linearly from beginning to end.
In addition to self-attention, the transformer also introduces positional bias, which allows the model to keep track of the relative positions of words in a sentence. This is important because the order of words in a sentence can significantly impact its meaning.
"
Deep Learning,Self-Attention,"Attention allowed us to focus on parts of our input sequence while we predicted our output sequence. If our model predicted the word “rouge” [French translation for the color red], we are very likely to find a high weight-age for the word “red” in our input sequence. So attention, in a way, allowed us to map some connection/correlation between the input word “rouge” and the output word “red”.
Self attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.
In simpler terms, self attention helps us create similar connections but within the same sentence. Look at the following example:
“I poured water from the bottle into the cup until it was full.”
it => cup“I poured water from the bottle into the cup until it was empty.”
it=> bottle
By changing one word “full” — > “empty” the reference object for “it” changed. If we are translating such a sentence, we will want to know the word “it” refers to.
The three kinds of Attention possible in a model:
Encoder-Decoder Attention: Attention between the input sequence and the output sequence.
Self attention in the input sequence: Attends to all the words in the input sequence.
Self attention in the output sequence: One thing we should be wary of here is that the scope of self attention is limited to the words that occur before a given word. This prevents any information leaks during the training of the model. This is done by masking the words that occur after it for each step. So for step 1, only the first word of the output sequence is NOT masked, for step 2, the first two words are NOT masked and so on.
Keys, Values, and Queries:
The three random words I just threw at you in this heading are vectors created as abstractions are useful for calculating self attention, more details on each below. These are calculated by multiplying your input vector(X) with weight matrices that are learnt while training.
Query Vector: q= X * Wq. Think of this as the current word.
Key Vector: k= X * Wk. Think of this as an indexing mechanism for Value vector. Similar to how we have key-value pairs in hash maps, where keys are used to uniquely index the values.
Value Vector: v= X * Wv. Think of this as the information in the input word.
What we want to do is take query q and find the most similar key k, by doing a dot product for q and k. The closest query-key product will have the highest value, followed by a softmax that will drive the q.k with smaller values close to 0 and q.k with larger values towards 1. This softmax distribution is multiplied with v. The value vectors multiplied with ~1 will get more attention while the ones ~0 will get less. The sizes of these q, k and v vectors are referred to as “hidden size” by various implementations.
The values represent the index for q, k and i.
All these matrices Wq, Wk and Wv are learnt while being jointly trained during the model training.
Calculating Self attention from q, k and v:
If we are calculating self attention for #i input word,
Step 1: Multiply qᵢ by the kⱼ key vector of word.
Step 2: Then divide this product by the square root of the dimension of key vector.
This step is done for better gradient flow which is specially important in cases when the value of the dot product in previous step is too big. As using them directly might push the softmax into regions with very little gradient flow.
Step 3: Once we have scores for all js, we pass these through a softmax. We get normalized value for each j.
Step 4: Multiply softmax scores for each j with vᵢ vector.
The idea/purpose here is, very similar attention, to keep preserve only the values v of the input word(s) we want to focus on by multiplying them with high probability scores from softmax ~1, and remove the rest by driving them towards 0, i.e. making them very small by multiplying them with the low probability scores ~0 from softmax.
Calculating output of self attention for the ith input word. If you are looking for an analogy between self attention and attention, think of z serving the purpose of context vectors and not global alignment weights.
"
Deep Learning,Encoder,"The encoder maps an input sequence of symbol representations (x₁, …, xₙ) to a sequence of representations z = (z₁, …, zₙ). Think of them as the outputs from self attention with some post-processing.
Each encoder has two sub-layers.
A multi-head self attention mechanism on the input vectors (Think parallelized and efficient sibling of self attention).
A simple, position-wise fully connected feed-forward network (Think post-processing).
"
Deep Learning,Decoder,"Given z, the decoder then generates an output sequence (y, …, yₘ) of symbols one element at a time.
Each decoder has three sub-layers.
A masked multi-head self attention mechanism on the output vectors of the previous iteration.
A multi-head attention mechanism on the output from encoder and masked multi-headed attention in decoder.
A simple, position-wise fully connected feed-forward network (think post-processing).
A few additional points:
In the original paper, 6 layers were present in the encoder stack (2 sub-layer version) and 6 in the decoder stack (3 sub-layer version).
All sub-layers in the model, as well as the embedding layers, produce outputs of the same dimension. This is done to facilitate the residual connections.
"
Deep Learning,Multi-Head Attention,"The output of each sub-layer needs to be of the same dimension which is 512 in our paper.
=> zᵢ needs to be of 512 dimensions.
=> vᵢ needs to be of 512 dimensions as zᵢ are just sort of weighted sums of vᵢs.
Additionally, we want to allow the model to focus on different positions is by calculating self attention multiple times with different sets of q, k and v vectors, then take an average of all those outputs to get our final z.
So instead of dealing with these humongous vectors and averaging multiple outputs, we reduce the size of our k,q and v vectors to some smaller dimension — reduces size of Wq, Wk, and Wv matrices as well. We keep the multiple sets (h) of k, q and v and refer to each set as an “attention head”, hence the name multi-headed attention. And lastly, instead of averaging to get final z, we concatenate them.
The size of the concatenated vector will be too large to be fed to the next sub-layer, so we scale it down by multiplying it with another learnt matrix Wo.
Multiple attention heads allowed the model to jointly attend to information from different representation sub-spaces at different positions which was inhibited by averaging in a single attention head.
"
Deep Learning,Input and Output Pre-processing in Transformers,"The input words are represented using some form of embedding. This is done for both encoder and decoder.
Word embedding on their own lack any positional information which is achieved in RNNs by virtue of their sequential nature. Meanwhile in self-attention, due to softmax, any such positional information is lost.
To preserve the positional information, the transformer injects a vector to individual input embeddings (could be using word embeddings for corresponding to the input words). These vectors follow a specific periodic function (Example: combination of various sines/cosines having different frequency, in short not in sync with each other) that the model learns and is able to determine the position of individual word wrt each other based on the values .
This injected vector is called “positional encoding” and are added to the input embeddings at the bottoms of both encoder and decoder stacks.
"
Deep Learning,Decoder Stack,"The output of the decoder stack at each step is fed back to the decoder in the next time step — pretty similar to how outputs from previous steps in RNNs were used as next hidden states. And just as we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to preserve the position of each word. This positional encoding + word embedding combo is then fed into a masked multi-headed self attention.
This self-attention sub-layer in the decoder stack is modified to prevent positions from attending to subsequent positions — you can’t look at future words. This masking ensures that the predictions for position i can depend only on the known outputs at positions less than i.
The outputs from the encoder stack are then used as multiple sets of key vectors k and value vectors v, for the “encoder decoder attention” — shown in green in the diagram — layer. It helps the decoder focus on the contextually relevant parts in the input sequence for that step. (The part similar to global attention vectors.) The q vector comes from the “output self attention” layer.
Once we get the output from the decoder, we do a softmax again to select the final probabilities of words.
"
Deep Learning,Transformer Encoder-Decoder Architecture,"The transformer encoder-decoder architecture is used for tasks like language translation, where the model must take in a sentence in one language and output a sentence in another language. The encoder takes in the input sentence and produces a fixed-size vector representation of it, which is then fed into the decoder to generate the output sentence. The decoder uses both self-attention and cross-attention, where the attention mechanism is applied to the output of the encoder and the input of the decoder.
One of the most popular transformer encoder-decoder models is the T5 (Text-to-Text Transfer Transformer), which was introduced by Google in 2019. The T5 can be fine-tuned for a wide range of NLP tasks, including language translation, question answering, summarization, and more.
Real-world examples of the transformer encoder-decoder architecture include Google Translate, which uses the T5 model to translate text between languages, and Facebook’s M2M-100, a massive multilingual machine translation model that can translate between 100 different languages.
"
Deep Learning,Transformer Encoder Architecture,"The transformer encoder architecture is used for tasks like text classification, where the model must classify a piece of text into one of several predefined categories, such as sentiment analysis, topic classification, or spam detection. The encoder takes in a sequence of tokens and produces a fixed-size vector representation of the entire sequence, which can then be used for classification.
One of the most popular transformer encoder models is BERT (Bidirectional Encoder Representations from Transformers), which was introduced by Google in 2018. BERT is pre-trained on large amounts of text data and can be fine-tuned for a wide range of NLP tasks.
Unlike the encoder-decoder architecture, the transformer encoder is only concerned with the input sequence and does not generate any output sequence. It applies self-attention mechanism to the input tokens, allowing it to focus on the most relevant parts of the input for the given task.
Real-world examples of the transformer encoder architecture include sentiment analysis, where the model must classify a given review as positive or negative, and email spam detection, where the model must classify a given email as spam or not spam.
"
Deep Learning,Transformer Decoder Architecture,"The transformer decoder architecture is used for tasks like language generation, where the model must generate a sequence of words based on an input prompt or context. The decoder takes in a fixed-size vector representation of the context and uses it to generate a sequence of words one at a time, with each word being conditioned on the previously generated words.
One of the most popular transformer decoder models is the GPT-3 (Generative Pre-trained Transformer 3), which was introduced by OpenAI in 2020. The GPT-3 is a massive language model that can generate human-like text in a wide range of styles and genres.
The transformer decoder architecture introduces a technique called triangle masking for attention, which ensures that the attention mechanism only looks at tokens to the left of the current token being generated. This prevents the model from “cheating” by looking at tokens that it hasn’t generated yet.
Real-world examples of the transformer decoder architecture include text generation, where the model must generate a story or article based on a given prompt or topic, and chatbots, where the model must generate responses to user inputs in a natural and engaging way.
"
Deep Learning,Drawbacks of Transformers,"The drawbacks of the transformer architecture are:
High computational cost due to the attention mechanism, which increases quadratically with sequence length.
Difficulty in interpretation and debugging due to the attention mechanism operating over the entire input sequence.
Prone to overfitting when fine-tuned on small amounts of task-specific data.
Despite these downsides, the transformer architecture remains a powerful and widely-used tool in NLP, and research is ongoing to mitigate its computational requirements and improve its interpretability and robustness.
"
Deep Learning,GPT vs BERT: What’s The Difference?,"BERT is a Transformer encoder, which means that, for each position in the input, the output at the same position is the same token (or the [MASK] token for masked tokens), that is the inputs and output positions of each token are the same. Models with only an encoder stack like BERT generate all its outputs at once.
BERT has two training objectives, and the most important of them is the Masked Language Modeling (MLM) objective. is With the MLM objective, at step the following happens:
select some tokens
(each token is selected with the probability of 15%)
replace these selected tokens
(with the special token [MASK] - with p=80%, with a random token - with p=10%, with the original token (remain unchanged) - with p=10%)
predict original tokens (compute loss).
The illustration below shows an example of a training step for one sentence. You can go over the slides to see the whole process.
GPT is an autoregressive transformer decoder, which means that each token is predicted and conditioned on the previous token. We don't need an encoder, because the previous tokens are received by the decoder itself. This makes these models really good at tasks like language generation, but not good at classification. These models can be trained with unlabeled large text corpora from books or web articles.
In conclusion, while both GPT and BERT are examples of transformer architectures that have been influencing the field of natural language processing in recent years, they have different strengths and weaknesses that make them suitable for different types of tasks. GPT excels at generating long sequences of text with high accuracy whereas BERT focuses more on the understanding context within given texts in order to perform more sophisticated tasks such as question answering or sentiment analysis. Data scientists, developers, and machine learning engineers should decide which architecture best fits their needs before embarking on any NLP project using either model. Ultimately, both GPT and BERT are powerful tools that offer unique advantages depending on the task at hand.
"
Math,Normal matrix,"In mathematics, a complex square matrix A is normal if it commutes with its conjugate transpose A*:
A*A = AA*
For real matrices, the conjugate transpose is just the transpose, A* = AT
"
Math,Eigen Decomposition,"In linear algebra, eigendecomposition is the factorization of a matrix into a canonical form, whereby the matrix is represented in terms of its eigenvalues and eigenvectors. Only diagonalizable matrices can be factorized in this way. When the matrix being factorized is a normal or real symmetric matrix, the decomposition is called ""spectral decomposition"", derived from the spectral theorem.
Matrix A is diagonalizable if it's similar to a diagonal matrix (a matrix A is similar to B if there exists an invertible M s.t. B=M^−1AM)
A is diagonalizable if there exists invertible matrix PP s.t. P^−1AP is diagonal .
Let A be a square n × n matrix with n linearly independent eigenvectors qi (where i = 1, ..., n). Then A can be factorized as 
A = Q ΛQ^-1
where Q is the square n × n matrix whose ith column is the eigenvector qi of A, and Λ is the diagonal matrix whose diagonal elements are the corresponding eigenvalues, Λii = λi. Note that only diagonalizable matrices can be factorized in this way. 
One nice thing about eigendecompositions is that we can write many operations we usually encounter cleanly in terms of the eigendecomposition:
A^n = Q Λ^nQ^-1
"
Math,Singular value decomposition,"The Singular Value Decomposition (SVD) of a matrix is a factorization of that matrix into three matrices. 
The SVD of  mxn matrix A is given by the formula  A=UΣVT
where:
U:  mxm matrix of the orthonormal eigenvectors of AAT                     
VT: transpose of a nxn matrix containing the orthonormal eigenvectors of ATA.
Σ : diagonal matrix with r elements equal to the root of the positive eigenvalues of AAᵀ or Aᵀ A.
"
Math,Applications of SVD,"Calculation of Pseudo-inverse:  Pseudo inverse or Moore-Penrose inverse is the generalization of the matrix inverse that may not be invertible (such as low-rank matrices). If the matrix is invertible then its inverse will be equal to Pseudo inverse but pseudo inverse exists for the matrix that is not invertible. It is denoted by A+.
Solving a set of Homogeneous Linear Equation (Mx =b): if b=0,  calculate SVD and take any column of VT associated with a singular value (in W) equal to 0.
Curve Fitting Problem: Singular value decomposition can be used to minimize the least square error. It uses the pseudo inverse to approximate it.
Besides the above application, singular value decomposition and pseudo-inverse can also be used in Digital signal processing and image processing
"
Math,Gradient,"In the case of a univariate function, it is simply the first derivative at a selected point. In the case of a multivariate function, it is a vector of derivatives in each main direction (along variable axes). Because we are interested only in a slope along one axis and we don’t care about others these derivatives are called partial derivatives.
A gradient for an n-dimensional function f(x) at a given point p is defined as follows:
The upside-down triangle is a so-called nabla symbol  and you read it “del”.
"
Math,Matrix differentiation,"In mathematics, a real-valued function is called convex if the line segment between any two distinct points on the graph of the function lies above the graph between the two points. Equivalently, a function is convex if its epigraph (the set of points on or above the graph of the function) is a convex set. 
Concave function is one for which the value at any convex combination of elements in the domain is greater than or equal to the convex combination of the values at the endpoints. 
Let function f : RD → R be a function whose domain is a convex set. The function f is a convex function if for all x, y in the domain convex function of f, and for any scalar θ with 0 ⩽ θ ⩽ 1, we have 
f(θx + (1 − θ)y) ⩽ θf(x) + (1 − θ)f(y).
A concave function is the negative of a convex function
A function f(x) is convex if and only if for any two points x, y it holds that 
f(y) ⩾ f(x) + ∇xf(x) ⊤(y − x).
If we further know that a function f(x) is twice differentiable, that is, the Hessian exists for all values in the domain of x, then the function f(x) is convex if and only if ∇2 x f(x) is positive semidefinite
"
Math,Supporting hyperplane,"In geometry, a supporting hyperplane of a set S in Euclidean space Rn is a hyperplane that has both of the following two properties: 
Sis entirely contained in one of the two closed half-spaces bounded by the hyperplane,
S has at least one boundary-point on the hyperplane.
"
Math,Legendre–Fenchel Transform and Convex Conjugate,"The Legendre-Fenchel transform is a transformation from a convex differentiable function f(x) to a function that depends on the tangents s(x) = ∇f(x). It is worth stressing that this is a transformation of the function f(·) and not the variable x or the function evaluated at x. The Legendre-Fenchel transform is also known as the convex conjugate and is closely related to duality.
The convex conjugate of a function f : RD → R is a function f ∗ defined by 
f ∗ (s) = supremum x∈RD (⟨s, x⟩ − f(x)) .
We can reconstruct any function f(x), with some restriction, by just knowing its tangent line at each point on its graph.
Describing the tangent line of a function, on the other hand, requires two pieces of information; the slope of the line at each point, given by the value of df/dx, and the y-interception of the line at each point, b.
Therefore, we can encode all the information of a function f(x) into just these two values and this is indeed exactly what the Legendre transformation does; generates a new function from df/dx and b.
The important thing about this is that the Legendre transformation of a function then contains exactly the same information as the original function, just “presented” in a different way. This is why it’s useful in the first place.
"
Math,Dirac delta function,"The Dirac delta function δ(x) can be loosely thought of as a function on the real line which is zero everywhere except at the origin, where it is infinite, аnd its integral is 1.
∫−∞∞δ(x)dx=1.
"
Metrics,List Classification Metrics,"Accuracy, Cross-Entropy, Precision, Recall, F1-score, AUC ROC
"
Metrics,Classification Accuracy,"Classification Accuracy is what we usually mean, when we use the term accuracy. It is the ratio of number of correct predictions to the total number of input samples.
It works well only if there are equal number of samples belonging to each class.
"
Metrics,Logarithmic Loss (cross-entropy loss),"Logarithmic Loss or Log Loss, works by penalising the false classifications. It works well for multi-class classification. When working with Log Loss, the classifier must assign probability to each class for all the samples. Suppose, there are N samples belonging to M classes, then the Log Loss is calculated as below :
Log Loss has no upper bound and it exists on the range [0, ∞). Log Loss nearer to 0 indicates higher accuracy, whereas if the Log Loss is away from 0 then it indicates lower accuracy.
In general, minimising Log Loss gives greater accuracy for the classifier.
"
Metrics,Confusion Matrix,"Confusion Matrix as the name suggests gives us a matrix as output and describes the complete performance of the model.
There are 4 important terms :
True Positives : The cases in which we predicted YES and the actual output was also YES.
True Negatives : The cases in which we predicted NO and the actual output was NO.
False Positives : The cases in which we predicted YES and the actual output was NO.
False Negatives : The cases in which we predicted NO and the actual output was YES.
Accuracy for the matrix can be calculated by taking average of the values lying across the “main diagonal” 
Confusion Matrix forms the basis for the other types of metrics.
"
Metrics,Area Under Curve,"Area Under Curve (AUC) is one of the most widely used metrics for evaluation. It is used for binary classification problem. AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example. Before defining AUC, let us understand two basic terms :
True Positive Rate (Sensitivity) : True Positive Rate is defined as TP/ (FN+TP). True Positive Rate corresponds to the proportion of positive data points that are correctly considered as positive, with respect to all positive data points.
True Negative Rate (Specificity) : True Negative Rate is defined as TN / (FP+TN). False Positive Rate corresponds to the proportion of negative data points that are correctly considered as negative, with respect to all negative data points.
False Positive Rate : False Positive Rate is defined as FP / (FP+TN). False Positive Rate corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points.
False Positive Rate and True Positive Rate both have values in the range [0, 1]. FPR and TPR both are computed at varying threshold values such as (0.00, 0.02, 0.04, …., 1.00) and a graph is drawn. AUC is the area under the curve of plot False Positive Rate vs True Positive Rate at different points in [0, 1]. 
As evident, AUC has a range of [0, 1]. The greater the value, the better is the performance of our model.
"
Metrics,"Precision, Recall, F1 Score","Precision = TP / (TP + FP): It is the number of correct positive results divided by the number of positive results predicted by the classifier.
Recall = TP/(TP + FN) : It is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive).
F1 Score is used to measure a test’s accuracy
F1 Score is the Harmonic Mean between precision and recall. The range for F1 Score is [0, 1]. It tells you how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances).
High precision but lower recall, gives you an extremely accurate, but it then misses a large number of instances that are difficult to classify. The greater the F1 Score, the better is the performance of our model. Mathematically, it can be expressed as :
F1 Score tries to find the balance between precision and recall.
"
Metrics,"Explain Macro Average, Weighted Average and Micro Average","Macro averaging is perhaps the most straightforward among the numerous averaging methods.
The macro-averaged F1 score (or macro F1 score) is computed using the arithmetic mean (aka unweighted mean) of all the per-class F1 scores.
This method treats all classes equally regardless of their support values.
Weighted Average
The weighted-averaged F1 score is calculated by taking the mean of all per-class F1 scores while considering each class’s support.
Support refers to the number of actual occurrences of the class in the dataset. For example, the support value of 1 in Boat means that there is only one observation with an actual label of Boat.
The ‘weight’ essentially refers to the proportion of each class’s support relative to the sum of all support values.
With weighted averaging, the output average would have accounted for the contribution of each class as weighted by the number of examples of that given class.
Micro Average
Micro averaging computes a global average F1 score by counting the sums of the True Positives (TP), False Negatives (FN), and False Positives (FP).
We first sum the respective TP, FP, and FN values across all classes and then plug them into the F1 equation to get our micro F1 score.
In the classification report, you might be wondering why our micro F1 score of 0.60 is displayed as ‘accuracy’ and why there is NO row stating ‘micro avg’.
This is because micro-averaging essentially computes the proportion of correctly classified observations out of all observations. If we think about this, this definition is what we use to calculate overall accuracy.
micro-F1 = accuracy = micro-precision = micro-recall
"
Metrics,Which average should I choose?,"In general, if you are working with an imbalanced dataset where all classes are equally important, using the macro average would be a good choice as it treats all classes equally.
It means that for our example involving the classification of airplanes, boats, and cars, we would use the macro-F1 score.
If you have an imbalanced dataset but want to assign greater contribution to classes with more examples in the dataset, then the weighted average is preferred.
This is because, in weighted averaging, the contribution of each class to the F1 average is weighted by its size.
Suppose you have a balanced dataset and want an easily understandable metric for overall performance regardless of the class. In that case, you can go with accuracy, which is essentially our micro F1 score.
"
Metrics,Why F1-Score is a Harmonic Mean(HM) of Precision and Recall?,"If Precision = 0, Recall = 1, their average is 0.5 and F1 is 0.
"
Metrics,What is Average Precision?,"Average precision is the area under the PR curve.
AP summarizes the PR Curve to one scalar value. Average precision is high when both precision and recall are high, and low when either of them is low across a range of confidence threshold values. The range for AP is between 0 to 1.
"
Metrics,Explain ROC curve,"A ROC curve is a plot of the true positive rate (Sensitivity) in function of the false positive rate (100-Specificity) for different cut-off points of a parameter. Each point on the ROC curve represents a sensitivity/specificity pair corresponding to a particular decision threshold. The Area Under the ROC curve (AUC) is a measure of how well a parameter can distinguish between two diagnostic groups (diseased/normal).
"
Metrics,In which cases AU PR is better than AU ROC? ,"AU ROC looks at a true positive rate TPR and false positive rate FPR while AU PR looks at positive predictive value PPV and true positive rate TPR.
Typically, if true negatives are not meaningful to the problem or you care more about the positive class, AU PR is typically going to be more useful; otherwise, If you care equally about the positive and negative class or your dataset is quite balanced, then going with AU ROC is a good idea.
ROC curves should be used when there are roughly equal numbers of observations for each class.
Precision-Recall curves should be used when there is a moderate to large class imbalance.
"
Metrics,Explain Index of Union (IU),"Perkins and Schisterman [4] stated that the “optimal” cut-point should be chosen as the point which classifies most of the individuals correctly and thus least of them incorrectly. From this point of view, in this study, the Index of Union method is proposed. This method provides an “optimal” cut-point which has maximum sensitivity and specificity values at the same time. In order to find the highest sensitivity and specificity values at the same time, the AUC value is taken as the starting value of them. For example, let AUC value be 0.8. The next step is to look for a cut-point from the coordinates of ROC whose sensitivity and specificity values are simultaneously so close or equal to 0.8. This cut-point is then defined as the “optimal” cut-point. The above criteria correspond to the following equation:The cut-point , which minimizes the  function and the  difference, will be the “optimal” cut-point value.
In other words, the cut-point cˆIU defined by the IU method should satisfy two conditions: (1) sensitivity and specificity obtained at this cut-point should be simultaneously close to the AUC value; (2) the difference between sensitivity and specificity obtained at this cut-point should be minimum. The second condition is not compulsory, but it is an essential condition when multiple cut-points satisfy the equation.
"
Metrics,List Regression Losses,"Mean Square Error/Quadratic Loss/L2 Loss
As the name suggests, Mean square error is measured as the average of squared difference between predictions and actual observations. It’s only concerned with the average magnitude of error irrespective of their direction. However, due to squaring, predictions which are far away from actual values are penalized heavily in comparison to less deviated predictions. Plus MSE has nice mathematical properties which makes it easier to calculate gradients.
Mean Absolute Error/L1 Loss
Mean absolute error, on the other hand, is measured as the average of sum of absolute differences between predictions and actual observations. Like MSE, this as well measures the magnitude of error without considering their direction. Unlike MSE, MAE needs more complicated tools such as linear programming to compute the gradients. Plus MAE is more robust to outliers since it does not make use of square.
Mean Bias Error
This is much less common in machine learning domain as compared to it’s counterpart. This is same as MSE with the only difference that we don’t take absolute values. Clearly there’s a need for caution as positive and negative errors could cancel each other out. Although less accurate in practice, it could determine if the model has positive bias or negative bias.
"
Metrics,List Classification Losses,"Hinge Loss/Multi class SVM Loss
In simple terms, the score of correct category should be greater than sum of scores of all incorrect categories by some safety margin (usually one). And hence hinge loss is used for maximum-margin classification, most notably for support vector machines. Although not differentiable, it’s a convex function which makes it easy to work with usual convex optimizers used in machine learning domain.
Cross Entropy Loss/Negative Log Likelihood
This is the most common setting for classification problems. Cross-entropy loss increases as the predicted probability diverges from the actual label.
"
Metrics,Can I use my metric as the loss function?,"Not always!
You can’t, for several reasons
Some algorithms require the loss function to be differentiable and some metrics, such as accuracy or any other step function, are not.
These algorithms usually use some form of gradient descent to update the parameters. This is the case for neural network weight stepping, where the partial derivative of the loss with respect to each weight is calculated.
Let’s illustrate this by using accuracy on a classification problem where the model assigns a probability to each mutually exclusive class for a given input.
In this case, a small change to a parameter’s weight may not change the outcome of our predictions but only our confidence in them, meaning the accuracy remains the same. The partial derivative of the loss with respect to this parameter would be 0 (infinity at the threshold) most of the time, preventing our model from learning (a step of 0 would keep the weight and model as is).
In other words, we want the small changes made to the parameter weights to be reflected in the loss function.
Some algorithms don’t require their function to be differentiable but would not work with some functions by their nature. You can read this post as an example of why classification error can’t be used for decision tree.
It may not be ideal
Some objective functions are easier to optimize than others. We might want to use a proxy easy loss function instead of a hard one.
We often choose to optimize smooth and convex loss functions because:
They are differentiable anywhere.
A minimum is always a global minimum.
Using gradient descent on such function will lead you surely towards the global minima and not get stuck in a local mimimum or saddle point.
There are plenty of ressources about convex functions on the internet. I’ll share one with you. I personally didn’t get all of it but maybe you will.
Some algorithms seem to empirically work well with non-convex functions. This is the case of Deep Learning for example, where we often use gradient descent on a non-convex loss function.
Another thing you need to be careful of is that different loss functions bring different assumptions to the model. For example, the logistic regression loss assumes a Bernoulli distribution.
"
Metrics,"What are some differences you would expect in a model that minimizes squared error, versus a model that minimizes absolute error? In which cases would each error metric be appropriate?","MSE is more strict to having outliers. MAE is more robust in that sense, but is harder to fit the model for because it cannot be numerically optimized. So when there are less variability in the model and the model is computationally easy to fit, we should use MAE, and if that’s not the case, we should use MSE.
MSE: easier to compute the gradient, MAE: linear programming needed to compute the gradient
MAE more robust to outliers. If the consequences of large errors are great, use MSE
MSE corresponds to maximizing likelihood of Gaussian random variables
"
Metrics,"Define the terms KPI, lift, model fitting, robustness and DOE. ","KPI: KPI stands for Key Performance Indicator that measures how well the business achieves its objectives.
Lift: This is a performance measure of the target model measured against a random choice model. Lift indicates how good the model is at prediction versus if there was no model.
Model fitting: This indicates how well the model under consideration fits given observations.
Robustness: This represents the system’s capability to handle differences and variances effectively.
DOE: stands for the design of experiments, which represents the task design aiming to describe and explain information variation under hypothesized conditions to reflect variables.
Design of experiments (DOE) is a systematic, efficient method that enables scientists and engineers to study the relationship between multiple input variables (aka factors) and key output variables (aka responses). It is a structured approach for collecting data and making discoveries.
"
Metrics,Empirical Risk Minimization,"For a given training set {(x1, y1), . . . ,(xN , yN )}, we introduce the notation of an example matrix X := [x1, . . . , xN ] ⊤ ∈ RN×D and a label vector y := [y1, . . . , yN ] ⊤ ∈ RN . Using this matrix notation the average loss is given by
R (f, X, y) = 1 / N * sum n=1..N ℓ(yn, yˆn),
where yˆn = f(xn, θ). Equation is called the empirical risk and depends on three arguments, the predictor f and the data X, y.
"
Metrics,Least-Squares Loss,"min θ∈R^D 1/N * ∥y − Xθ∥ 2
"
Metrics,Maximum Likelihood Estimation,"The idea behind maximum likelihood estimation (MLE) is to define a function of the parameters that enables us to find a model that fits the data well. The estimation problem is focused on the likelihood function, or more precisely its negative logarithm. For data represented by a random variable x and for a family of probability densities p(x | θ) parametrized by θ, the negative log-likelihood is given by
Lx(θ) = − log p(x | θ)
Let us interpret what the probability density p(x | θ) is modeling for a fixed value of θ. It is a distribution that models the uncertainty of the data for a given parameter setting. For a given dataset x, the likelihood allows us to express preferences about different settings of the parameters θ, and we can choose the setting that more “likely” has generated the data.
We assume that the set of examples (x1, y1), . . . ,(xN , yN ) are independent and identically distributed (i.i.d.). Hence, in machine learning we often consider the negative log-likelihood 
L(θ) = − sum n=1..N log p(yn | xn, θ).
We often assume that we can explain our observation uncertainty by independent Gaussian noise with zero mean. We further assume that the linear model <x ⊤ n, θ> is used for prediction. This means we specify a Gaussian likelihood for each example label pair (xn, yn), 
p(yn | xn, θ) = N
In this case, minimizing the negative log-likelihood corresponds to solving the least-squares problem.
For other likelihood functions, i.e., if we model our noise with non-Gaussian distributions, maximum likelihood estimation may not have a closed-form analytic solution. In this case, we resort to numerical optimization methods.
MLE can be seen as a special case of the maximum a posteriori estimation (MAP) that assumes a uniform prior distribution of the parameters, or as a variant of the MAP that ignores the prior and which therefore is unregularized.
For gaussian mixtures, non parametric models, it doesn’t exist.
MLE is a random variable as it is calculated on a random sample. MLE is a consistent estimator and under certain conditions, it asymptotically converges to a normal distribution with true parameter as mean and variance equal to the inverse of the Fisher information matrix.
"
Metrics,Maximum A Posteriori Estimation,"If we have prior knowledge about the distribution of the parameters θ, we can multiply an additional term to the likelihood. This additional term is a prior probability distribution on parameters p(θ). Bayes’ theorem gives us a principled tool to update our probability distributions of random variables. It allows us to compute a posterior distribution p(θ | x) (the more specific knowledge) on the parameters θ from general prior statements (prior distribution) p(θ) and the function p(x | θ) that links the parameters θ and the observed data x (called the likelihood): 
p(θ | x) = p(x | θ)p(θ) p(x)
Since the distribution p(x) does not depend on θ, we can ignore the value of the denominator for the optimization.
Instead of estimating the minimum of the negative log-likelihood, we now estimate the minimum of the negative log-posterior, which is referred to as maximum a posteriori estimation (MAP estimation).
In addition to the assumption of Gaussian likelihood in the previous example, we assume that the parameter vector is distributed as a multivariate Gaussian with zero mean. Note that the conjugate prior of a Gaussian is also a Gaussian, and therefore we expect the posterior distribution to also be a Gaussian.
"
Metrics,Explain Method of Moments (MOM),"According to the Law of Large Numbers (LLN), the average converges to the expectation as the sample size tends to infinity. Using this law, the population moments which are a function of parameters are set equal to sample moments to solve for the parameters. For a normal distribution, both MLE and MOM produce sample mean as an estimate to the population mean.
"
Metrics,Explain Kernel Density Estimation (KDE),"KDE is a non-parametric method to estimate pdf of data generating distribution. KDE allocates high density to certain x if sample data has many datapoints around it. A datapoint’s contribution to certain x depends on its distance to x and bandwidth. As the sample size increases, KDE approximation under certain conditions approaches true pdf.
In practice, we use the t-distribution most often when performing hypothesis tests or constructing confidence intervals.
"
Metrics,Formula for R-Squared,"R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. 
goodness of fit measure. variance explained by the regression / total variance
the more predictors you add the higher R^2 becomes.
"
Metrics,What Is a Variance Inflation Factor (VIF)? ,"A variance inflation factor (VIF) is a measure of the amount of multicollinearity in regression analysis. Multicollinearity exists when there is a correlation between multiple independent variables in a multiple regression model. This can adversely affect the regression results. Thus, the variance inflation factor can estimate how much the variance of a regression coefficient is inflated due to multicollinearity. 
  Detecting multicollinearity is important because while multicollinearity does not reduce the explanatory power of the model, it does reduce the statistical significance of the independent variables. 
  A large VIF on an independent variable indicates a highly collinear relationship to the other variables that should be considered or adjusted for in the structure of the model and selection of independent variables.
The formula for VIF is:
VIF=1/(1−Ri2)
where:Ri2=Unadjusted coefficient of determination forregressing the ith independent variable on theremaining ones
A rule of thumb for interpreting the variance inflation factor:
1 = not correlated.
Between 1 and 5 = moderately correlated.
Greater than 5 = highly correlated.
"
Metrics,Explain Huber Loss,"Huber loss, also known as smooth L1 loss, is a loss function commonly used in regression problems, particularly in machine learning tasks involving regression tasks. It is a modified version of the Mean Absolute Error (MAE) and Mean Squared Error (MSE) loss functions, which combines the best properties of both.
Below are some advantages of Huber Loss –
Robustness to outliers
Differentiability
The balance between L1 and L2 loss
Smoother optimization landscape
Efficient optimization
User-defined threshold
Wide applicability
While there are also some disadvantages of using this loss function –
Hyperparameter tuning
Task-specific performance
Less emphasis on smaller errors
"
Metrics,What Loss Function and Activation to Use?,"Regression Problem
A problem where you predict a real-value quantity.
Output Layer Configuration: One node with a linear activation unit.
Loss Function: Mean Squared Error (MSE).
Binary Classification Problem
A problem where you classify an example as belonging to one of two classes.
The problem is framed as predicting the likelihood of an example belonging to class one, e.g. the class that you assign the integer value 1, whereas the other class is assigned the 0value 0.
Output Layer Configuration: One node with a sigmoid activation unit.
Loss Function: Cross-Entropy, also referred to as Logarithmic loss.
Multi-Class Classification Problem
A problem where you classify an example as belonging to one of more than two classes.
The problem is framed as predicting the likelihood of an example belonging to each class.
Output Layer Configuration: One node for each class using the softmax activation function.
Loss Function: Cross-Entropy, also referred to as Logarithmic loss.
Categorical Cross-Entropy loss or Softmax Loss is a Softmax activation plus a Cross-Entropy loss. If we use this loss, we will train a CNN to output a probability over the C classes for each image. It is used for multi-class classification.
Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss. Unlike Softmax loss it is independent for each vector component (class), meaning that the loss computed for every CNN output vector component is not affected by other component values. That’s why it is used for multi-label classification, where the insight of an element belonging to a certain class should not influence the decision for another class.
"
Metrics,Gini impurity,"The gini impurity is calculated using the following formula:
GiniIndex=1–∑jpj^2
Where pj is the probability of class j.
The gini impurity measures the frequency at which any element of the dataset will be mislabelled when it is randomly labeled.
The minimum value of the Gini Index is 0. This happens when the node is pure, this means that all the contained elements in the node are of one unique class. Therefore, this node will not be split again. Thus, the optimum split is chosen by the features with less Gini Index. Moreover, it gets the maximum value when the probability of the two classes are the same.
Ginimin=1–(12)=0
Ginimax=1–(0.52+0.52)=0.5
"
Metrics,Entropy,"The entropy is calculated using the following formula:
Entropy=–∑jpj⋅log2pj
Where, as before, pj
is the probability of class j.
Entropy is a measure of information that indicates the disorder of the features with the target. Similar to the Gini Index, the optimum split is chosen by the feature with less entropy. It gets its maximum value when the probability of the two classes is the same and a node is pure when the entropy has its minimum value, which is 0:
Entropymin=−1⋅log2(1)=0
Entropymax=–0.5⋅log2(0.5)–0.5⋅log2(0.5)=1
"
Metrics,Gini vs Entropy,"The Gini Index and the Entropy have two main differences:
Gini is the probability of misclassifying a randomly chosen element in a set while entropy measures the amount of uncertainty or randomness in a set.
The range of the Gini index is [0, 1], where 0 indicates perfect purity and 1 indicates maximum impurity. The range of entropy is [0, log(c)], where c is the number of classes.
Gini index is a linear measure.	Entropy is a logarithmic measure.
Gini can be interpreted as the expected error rate in a classifier. Entropy can be interpreted as the average amount of information needed to specify the class of an instance.
Gini is sensitive to the distribution of classes in a set. Entropy is sensitive to the number of classes.
The computational complexity of the Gini index is O(c). Computational complexity of entropy is O(c * log(c)).
Entropy is more robust than Gini index and comparatively less sensitive.
Formula for the Gini index is Gini(P) = 1 – ∑(Px)^2 , where Pi is the proportion of the instances of class x in a set. Formula for entropy is Entropy(P) = -∑(Px)log(Px), where pi is the proportion of the instances of class x in a set.
Gini  has a bias toward selecting splits that result in a more balanced distribution of classes. Entropyhas a bias toward selecting splits that result in a higher reduction of uncertainty.
Gini index is typically used in CART (Classification and Regression Trees) algorithms. Entropy is typically used in ID3 and C4.5 algorithms
"
NLP,What is RAG?,"LLMs, although capable of generating text that is both meaningful and grammatically correct, these LLMs suffer from a problem called hallucination. Hallucination in LLMs is the concept where the LLMs confidently generate wrong answers, that is they make up wrong answers in a way that makes us believe that it is true. This has been a major problem since the introduction of the LLMs. These hallucinations lead to incorrect and factually wrong answers. Hence Retrieval Augmented Generation was introduced.
In RAG, we take a list of documents/chunks of documents and encode these textual documents into a numerical representation called vector embeddings, where a single vector embedding represents a single chunk of document and stores them in a database called vector store. The models required for encoding these chunks into embeddings are called encoding models or bi-encoders. These encoders are trained on a large corpus of data, thus making them powerful enough to encode the chunks of documents in a single vector embedding representation.
"
NLP,Explain Semantic Chunking,"In order to abide by the context window of the LLM , we usually break text into smaller parts / pieces which is called chunking.
Different chunking methods:
Fixed size chunking
Recursive Chunking
Document Specific Chunking
Semantic Chunking
Agentic Chunking
Semantic chunking involves taking the embeddings of every sentence in the document, comparing the similarity of all sentences with each other, and then grouping sentences with the most similar embeddings together. By focusing on the text’s meaning and context, Semantic Chunking significantly enhances the quality of retrieval. It’s a top-notch choice when maintaining the semantic integrity of the text is vital.
The hypothesis here is we can use embeddings of individual sentences to make more meaningful chunks. Basic idea is as follows :-
Split the documents into sentences based on separators(.,?,!)
Index each sentence based on position.
Group: Choose how many sentences to be on either side. Add a buffer of sentences on either side of our selected sentence.
Calculate distance between group of sentences.
Merge groups based on similarity i.e. keep similar sentences together.
Split the sentences that are not similar.
"
NLP,What Is a Knowledge Graph?,"A knowledge graph is an organized representation of real-world entities and their relationships. It is typically stored in a graph database, which natively stores the relationships between data entities. Entities in a knowledge graph can represent objects, events, situations, or concepts. The relationships between these entities capture the context and meaning of how they are connected.
Now that you understand how knowledge graphs organize and access data with context, let’s look at the building blocks of a knowledge graph data model. The definition of knowledge graphs varies depending on whom you ask, but we can distill the essence into three key components: nodes, relationships, and organizing principles. 
Nodes denote and store details about entities, such as people, places, objects, or institutions. Each node has a (or sometimes several) label to identify the node type and may optionally have one or more properties (attributes). Nodes are also sometimes called vertices.
Relationships link two nodes together: they show how the entities are related. Like nodes, each relationship has a label identifying the relationship type and may optionally have one or more properties. Relationships are also sometimes called edges. 
Organizing Principles are a framework, or schema, that organizes nodes and relationships according to fundamental concepts essential to the use cases at hand. Unlike many data designs, knowledge graphs easily incorporate multiple organizing principles.
In generative AI applications, knowledge graphs capture and organize key domain-specific or proprietary company information. Knowledge graphs are not limited to structured data; they can handle less organized data as well. 
In Fraud Detection and Analytics, the knowledge graph represents a network of transactions, their participants, and relevant information about them.
"
NLP,What is NLU?," NLU stands for Natural Language Understanding. It is a subdomain of NLP that concerns making a machine learn the skills of reading comprehension. A few applications of NLU include Machine translation (MT), Newsgathering, and Text categorization. It often goes by the name Natural Language Interpretation (NLI) as well.
"
NLP,What do you know about Latent Semantic Indexing (LSI)?,"LSI is a technique that analyzes a set of documents to find the statistical coexistence of words that appear together. It gives an insight into the topics of those documents.
LSI is also known as Latent Semantic Analysis.
"
NLP,What is perplexity in NLP?,"In general, perplexity is a measurement of how well a probability model predicts a sample. In the context of Natural Language Processing, perplexity is one way to evaluate language models.
A language model is a probability distribution over sentences: it’s both able to generate plausible human-written sentences (if it’s a good language model) and to evaluate the goodness of already written sentences. Presented with a well-written document, a good language model should be able to give it a higher probability than a badly written document, i.e. it should not be “perplexed” when presented with a well-written document.
Thus, the perplexity metric in NLP is a way to capture the degree of ‘uncertainty’ a model has in predicting (i.e. assigning probabilities to) text.
The probability of a generic sentence W, made of the words w1, w2, up to wn, can be expressed as the following:
P(W) = P(w1, w2, …, wn)
This can be done by normalizing the sentence probability by the number of words in the sentence. Since the probability of a sentence is obtained by multiplying many factors, we can average them using the geometric mean.
Let’s call Pnorm(W) the normalized probability of the sentence W. Let n be the number of words in W. Then, applying the geometric mean:
Pnorm(W) = P(W) ^ (1 / n)
Perplexity can be computed also starting from the concept of Shannon entropy. Let’s call H(W) the entropy of the language model when predicting a sentence W. Then, it turns out that:
PP(W) = 2 ^ (H(W))
"
NLP,Briefly describe the N-gram model in NLP," N-gram model is a model in NLP that predicts the probability of a word in a given sentence using the conditional probability of n-1 previous words in the sentence. The basic intuition behind this algorithm is that instead of using all the previous words to predict the next word, we use only a few previous words.
"
NLP,What is the Markov assumption for the bigram model?," The Markov assumption assumes for the bigram model that the probability of a word in a sentence depends only on the previous word in that sentence and not on all the previous words.
"
NLP,"
What do you understand by word embedding?","In NLP, word embedding is the process of representing textual data through a real-numbered vector. This method allows words having similar meanings to have a similar representation.
"
NLP,What is an embedding matrix?,"A word embedding matrix is a matrix that contains embedding vectors of all the words in a given text.
"
NLP,List a few popular methods used for word embedding.,"Following are a few methods of word embedding.
One-hot
BOW
TF-IDF
Word2Vec:
CBOW
Skip-gram
Glove
"
NLP,"For correcting spelling errors in a corpus, which one is a better choice: a giant dictionary or a smaller dictionary, and why?","Initially, a smaller dictionary is a better choice because most NLP researchers feared that a giant dictionary would contain rare words that may be similar to misspelled words. However, later it was found (Damerau and Mays (1989)) that in practice, a more extensive dictionary is better at marking rare words as errors.
"
NLP,Do you always recommend removing punctuation marks from the corpus you’re dealing with? Why/Why not?,"No, it is not always a good idea to remove punctuation marks from the corpus as they are necessary for certain NLP applications that require the marks to be counted along with words.
For example: Part-of-speech tagging, parsing, speech synthesis.
"
NLP,"
What is a hapax/hapax legomenon?","The rare words that only occur once in a sample text or corpus are called hapaxes. Each one of them is called an hapax or hapax legomenon (greek for ‘read-only once’). It is also called a singleton.
"
NLP,What is a collocation?,"A collocation is a group of two or more words that possess a relationship and provide a classic alternative of saying something. For example, ‘strong breeze’, ‘the rich and powerful’, ‘weapons of mass destruction.
"
NLP,What do you understand by regular expressions in NLP?,"Regular expressions in natural language processing are algebraic notations representing a set of strings. They are mainly used to find or replace strings in a text and can also be used to define a language in a formal way.
"
NLP,Define the term parsing concerning NLP,"Parsing refers to the task of generating a linguistic structure for a given input. For example, parsing the word ‘helping’ will result in verb-pass + gerund-ing.
Simply speaking, parsing in NLP is the process of determining the syntactic structure of a text by analyzing its constituent words based on an underlying grammar (of the language).
See this example grammar below, where each line indicates a rule of the grammar to be applied to an example sentence “Tom ate an apple”.
"
NLP,What is BLEU?,"BLEU is the abbreviation for bilingual evaluation understudy.
BLEU is used for evaluating the quality of machine translated text.
A text is considered higher quality the similar it is to a professional human translator.
BLEU's is always between 0 and 1. The value of 1 is for the most similar text to the target text.
A BLEU score of 0.5 is considered high quality, while a score of >0.6 is considered better than humans. Anything less than 0.2 is not understandable and does not give the gist of the translation well.
"
NLP,What Is Semantic Analysis?,"Simply put, semantic analysis is the process of drawing meaning from text. It allows computers to understand and interpret sentences, paragraphs, or whole documents, by analyzing their grammatical structure, and identifying relationships between individual words in a particular context.
It’s an essential sub-task of Natural Language Processing (NLP) and the driving force behind machine learning tools like chatbots, search engines, and text analysis.
"
NLP,Natural Language Toolkit (NLTK),"NLTK is the main library for building Python projects to work with human language data. It gives simple to-utilize interfaces to more than 50 corpora and lexical assets like WordNet, alongside a set-up of text preprocessing libraries for tagging, parsing, classification, stemming, tokenization and semantic reasoning wrappers for NLP libraries and an active conversation discussion. NLTK is accessible for Windows, Mac OS, and Linux. The best part is that NLTK is a free, open-source, local area-driven venture. It has some disadvantages as well. It is slow and difficult to match the demands of production usage. The learning curve is somehow steep. Some of the features provided by NLTK are;
Entity Extraction
Part-of-speech tagging
Tokenization
Parsing
Semantic reasoning
Stemming
Text classification
"
NLP,GenSim,"Gensim is a famous python library for natural language processing tasks. It provides a special feature to identify semantic similarity between two documents by the use of vector space modelling and the topic modelling toolkit. All algorithms in GenSim are memory-independent concerning corpus size it means we can process input larger than RAM. It provides a set of algorithms that are very useful in natural language tasks such as Hierarchical Dirichlet Process(HDP), Random Projections(RP), Latent Dirichlet Allocation(LDA), Latent Semantic Analysis(LSA/SVD/LSI) or word2vec deep learning. The most advanced feature of GenSim is its processing speed and fantastic memory usage optimization. The main uses of GenSim include Data Analysis, Text generation applications (chatbots) and Semantic search applications.GenSim highly depends on SciPy and NumPy for scientific computing.
"
NLP,SpaCy,"SpaCy is an open-source python Natural language processing library. It is mainly designed for production usage- to build real-world projects and it helps to handle a large number of text data. This toolkit is written in python in Cython which’s why it much faster and efficient to handle a large amount of text data. Some of the features of SpaCy are shown below:
It provides multi trained transformers like BERT
It is way faster than other libraries
Provides tokenization that is motivated linguistically In more than 49 languages
Provides functionalities such as text classification, sentence segmentation, lemmatization, part-of-speech tagging, named entity recognition and many more
It
has 55 trained pipelines in more than 17 languages.
"
NLP,Hashing Vecrtorizer,"Hashing Vectorizer converts text to a matrix of occurrences using the “hashing trick” Each word is mapped to a feature and using the hash function converts it to a hash. If the word occurs again in the body of the text it is converted to that same feature which allows us to count it in the same feature without retaining a dictionary in memory.
"
NLP,Lemmatization,"Lemmatization is a process by which inflected forms of words are grouped together to be analyzed as a single aspect. It is a way of using the intended meaning of a word to determine the “lemma”. It is largely depending on correctly finding “intended parts of speech” and the true meaning of a word in a sentence, paragraph, or larger documents. Examples of Lemmatization are that “run” is a base form for words like “running” or “ran” or that the word “better” and “good” are in the same lemma so they are considered the same.
"
NLP,Stemming,"Stemming is quite similar to Lemmatization in that it groups words together, but unlike lemmatization it takes a word and refers it back to its base or root form. In fact, on the best examples I’ve come across to describe it involves refereeing Stemming back to its base form. “Stems”, “Stemming”, “Stemmed”, “and Stemtization” are all based on the single word “stem”.
"
NLP,Part of Speech Tagging,"Each word has its own role in a sentence. For example,’Geeta is dancing’. Geeta is the person or ‘Noun’ and dancing is the action performed by her, so it is a ‘Verb’.Likewise, each word can be classified. This is referred as POS or Part of Speech Tagging.
Rule-based POS tagging: The rule-based POS tagging models apply a set of handwritten rules and use contextual information to assign POS tags to words. These rules are often known as context frame rules. One such rule might be: “If an ambiguous/unknown word ends with the suffix ‘ing’ and is preceded by a Verb, label it as a Verb”.
Transformation Based Tagging:  The transformation-based approaches use a pre-defined set of handcrafted rules as well as automatically induced rules that are generated during training.
Deep learning models: Various Deep learning models have been used for POS tagging such as Meta-BiLSTM which have shown an impressive accuracy of around 97 percent.
Stochastic (Probabilistic) tagging: A stochastic approach includes frequency, probability or statistics. The simplest stochastic approach finds out the most frequently used tag for a specific word in the annotated training data and uses this information to tag that word in the unannotated text. But sometimes this approach comes up with sequences of tags for sentences that are not acceptable according to the grammar rules of a language. One such approach is to calculate the probabilities of various tag sequences that are possible for a sentence and assign the POS tags from the sequence with the highest probability. Hidden Markov Models (HMMs) are probabilistic approaches to assign a POS Tag.
"
NLP,Dependency Parsing,"In a sentence, the words have a relationship with each other. The one word in a sentence which is independent of others, is called as Head /Root word. All the other word are dependent on the root word, they are termed as dependents.
Dependency Parsing is the method of analyzing the relationship/ dependency between different words of a sentence.
Usually, in a sentence, the verb is the head word.
You can access the dependency of a token through token.dep_ attribute.
token.dep_ prints dependency tags for each token
What is the meaning of these dependency tags ?
nsubj :Subject of the sentence
ROOT: The headword or independent word,(generally the verb)
prep: prepositional modifier, it modifies the meaning of a noun, verb, adjective, or preposition.
cc and conj: Linkages between words. For example: is, and, etc…
pobj : Denotes the object of the preposition
aux : Denotes it is an auxiliary word
dobj : direct object of the verb
det : Not specific, but not an independent word.
"
NLP,Named Entity Recognition,"Suppose you have a collection of news articles text data. What if you want to know what companies/organizations have been in the news? How will you do that?
Take another case of text dataset of information about influential people. What if you want to know the names of influencers in this dataset ?
NER is the technique of identifying named entities in the text corpus and assigning them pre-defined categories such as ‘ person names’ , ‘ locations’ ,’organizations’,etc..
It is a very useful method especially in the field of claasification problems and search egine optimizations.
NER can be implemented through both nltk and spacy`.
"
NLP,What is Extractive Text Summarization and Generative Text Summarization,"Extractive Text Summarization is the traditional method, in which the process is to identify significant phrases/sentences of the text corpus and include them in the summary.
The summary obtained from this method will contain the key-sentences of the original text corpus.
You can notice that in the extractive method, the sentences of the summary are all taken from the original text. There is no change in structure of any sentence.
Generative text summarization methods overcome this shortcoming. The concept is based on capturing the meaning of the text and generating entirely new sentences to best represent them in the summary.
"
NLP,NLP Pipeline,"NLP Pipeline is a set of steps followed to build an end to end NLP software.
Before we started we have to remember this things pipeline is not universal, Deep Learning Pipelines are slightly different, and Pipeline is non-linear.
1.  Data Acquisition
In the data acquisition step, these three possible situations happen.
A. Public Dataset – If a public dataset is available for our problem statement.
B. Web Scrapping –  Scrapping competitor data using beautiful soup or other libraries
C. API – Using different APIs. eg. Rapid API
2. Text Preprocessing
So Our data collection step is done but we can not use this data for model building. we have to do text preprocessing. 
Steps –
1. Text Cleaning – In-text cleaning we do HTML tag removing, emoji handling, Spelling checker, etc
2. Basic Preprocessing — In basic preprocessing we do tokenization(word or sent tokenization, stop word removal, removing digit, lower casing.
3. Advance Preprocessing — In this step we do POS tagging, Parsing, and Coreference resolution.
3. Featured Engineering
Feature Engineering means converting text data to numerical data. but why it is required to convert text data to numerical data, because our machine learning model doesn’t understand text data then we have to do feature engineering. This step is also called Feature extraction from text.
"
NLP,Bag of Words (BoW):,"The BoW model captures the frequencies of the word occurrences in a text corpus.
Bag of words is not concerned about the order in which words appear in the text; instead, it only cares about which words appear in the text.
Let’s understand how BoW works with an example. Consider the following phrases:
Document 1: Cats and dogs are not allowed
Document 2: Cats and dogs are antagonistic
Bag of words will first create a unique list of all the words based on the two documents. If we consider the two documents, we will have seven unique words.
‘cats’, ‘and’, ‘dogs’, ‘are’, ‘not’, ‘allowed’, ‘antagonistic’
Each unique word is a feature or dimension.
Now for each document, a feature vector will be created. Each feature vector will be seven-dimensional since we have seven unique words.
Document1 vector: [1 1 1 1 1 1 0]
Document2 vector: [1 1 1 1 0 0 1]
"
NLP,N-Grams:,"An N-Gram is a sequence of N-words in a sentence. Here, N is an integer which stands for the number of words in the sequence.
For example, if we put N=1, then it is referred to as a uni-gram. If you put N=2, then it is a bi-gram. If we substitute N=3, then it is a tri-gram.
The bag of words does not take into consideration the order of the words in which they appear in a document, and only individual words are counted.
In some cases, the order of the words might be important.
N-grams captures the context in which the words are used together. For example, it might be a good idea to consider bigrams like “New York” instead of breaking it into individual words like “New” and “York”
Consider the sentence “I like dancing in the rain”
See the Uni-Gram, Bi-Gram, and Tri-Gram cases below.
UNIGRAM: ‘I’, ‘like’, ‘dancing’, ‘in’, ‘the’, ‘rain’
BIGRAM: ‘I like’, ‘like dancing’, ‘dancing in’, ‘in the’, ‘the rain’
TRIGRAM: ‘I like dancing’, ‘like dancing in’, ‘dancing in the’, ‘in the rain’
"
NLP,"Term Frequency, Inverse Document Frequency(TF-IDF):","This is the most popular way to represent documents as feature vectors. TF-IDF stands for Term Frequency, Inverse Document Frequency.
TF-IDF measures how important a particular word is with respect to a document and the entire corpus.
Term Frequency:
Term frequency is the measure of the counts of each word in a document out of all the words in the same document. 
TF(w) = (number of times word w appears in a document) / (total number of words in the document)
For example, if we want to find the TF of the word cat which occurs 50 times in a document of 1000 words, then 
TF(cat) = 50 / 1000 = 0.05
Inverse Document Frequency:
IDF is a measure of the importance of a word, taking into consideration the frequency of the word throughout the corpus.
It measures how important a word is for the corpus.
IDF(w) = log(total number of documents / number of documents with w in it)
For example, if the word cat occurs in 100 documents out of 3000, then the IDF is calculated as
IDF(cat) = log(3000 / 100) = 1.47
Finally, to calculate TF-IDF, we multiply these two factors – TF and IDF.
TF-IDF(w) = TF(w) x IDF(w)
TF-IDF(cat) = 0.05 * 1.47 = 0.073
"
NLP,word2vec ,"It is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets. Embeddings learned through word2vec have proven to be successful on a variety of downstream natural language processing tasks.
These papers proposed two methods for learning representations of words:
Continuous bag-of-words model: predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important.
Continuous skip-gram model: predicts words within a certain range before and after the current word in the same sentence. A worked example of this is given below.
"
NLP,Skip-gram and negative sampling,"While a bag-of-words model predicts a word given the neighboring context, a skip-gram model predicts the context (or neighbors) of a word, given the word itself. The model is trained on skip-grams, which are n-grams that allow tokens to be skipped (see the diagram below for an example). The context of a word can be represented through a set of skip-gram pairs of (target_word, context_word) where context_word appears in the neighboring context of target_word.
Consider the following sentence of eight words:
The wide road shimmered in the hot sun.
The context words for each of the 8 words of this sentence are defined by a window size. The window size determines the span of words on either side of a target_word that can be considered a context word. Below is a table of skip-grams for target words based on different window sizes.
The training objective of the skip-gram model is to maximize the probability of predicting context words given the target word. For a sequence of words w1, w2, ... wT, the objective can be written as the average log probability
where c is the size of the training context. The basic skip-gram formulation defines this probability using the softmax function.
where v and v' are target and context vector representations of words and W is vocabulary size.
Computing the denominator of this formulation involves performing a full softmax over the entire vocabulary words, which are often large (105-107) terms.
The noise contrastive estimation (NCE) loss function is an efficient approximation for a full softmax. With an objective to learn word embeddings instead of modeling the word distribution, the NCE loss can be simplified to use negative sampling.
The simplified negative sampling objective for a target word is to distinguish the context word from num_ns negative samples drawn from noise distribution Pn(w) of words. More precisely, an efficient approximation of full softmax over the vocabulary is, for a skip-gram pair, to pose the loss for a target word as a classification problem between the context word and num_ns negative samples.
A negative sample is defined as a (target_word, context_word) pair such that the context_word does not appear in the window_size neighborhood of the target_word. For the example sentence, these are a few potential negative samples (when window_size is 2).
"
Statistics,Statistical Hypothesis Testing,"A statistical hypothesis test makes an assumption about the outcome, called the null hypothesis.
For example, the null hypothesis for the Pearson’s correlation test is that there is no relationship between two variables. The null hypothesis for the Student’s t test is that there is no difference between the means of two populations.
The test is often interpreted using a p-value, which is the probability of observing the result given that the null hypothesis is true, not the reverse, as is often the case with misinterpretations.
p-value (p): Probability of obtaining a result equal to or more extreme than was observed in the data.
"
Statistics,How to interpret P-value?,"In interpreting the p-value of a significance test, you must specify a significance level, often referred to as the Greek lower case letter alpha (a). A common value for the significance level is 5% written as 0.05.
The p-value is interested in the context of the chosen significance level. A result of a significance test is claimed to be “statistically significant” if the p-value is less than the significance level. This means that the null hypothesis (that there is no result) is rejected.
p <= alpha: reject H0, different distribution.
p > alpha: fail to reject H0, same distribution.
Where:
Significance level (alpha): Boundary for specifying a statistically significant finding when interpreting the p-value.
We can see that the p-value is just a probability and that in actuality the result may be different. The test could be wrong. Given the p-value, we could make an error in our interpretation.
"
Statistics,Types of Errors,"Type I Error. Reject the null hypothesis when there is in fact no significant effect (false positive). The p-value is optimistically small.  α = probability of a Type I error.
Type II Error. Not reject the null hypothesis when there is a significant effect (false negative). The p-value is pessimistically large. β = probability of a Type II error.
"
Statistics,Significance level and confidence level,"Alpha is the significance level used to compute the confidence level. The confidence level equals 100*(1 - alpha)%, or in other words, an alpha of 0.05 indicates a 95 percent confidence level. Standard_dev is the population standard deviation for the data range and is assumed to be known.
The confidence level in hypothesis testing is the probability of not rejecting the null hypothesis when the null hypothesis is True:
P(Not Rejecting H0|H0 is True) = 1 - P(Rejecting H0|H0 is True)
The default confidence level is set at 95%.
"
Statistics,What is the power of a test? ,"β = probability of a Type II error, known as a ""false negative""
1 − β = probability of a ""true positive"", i.e., correctly rejecting the null hypothesis. ""1 − β"" is also known as the power of the test.
 α = probability of a Type I error, known as a ""false positive""
1 − α = probability of a ""true negative"", i.e., correctly not rejecting the null hypothesis
The power of a test is the probability of rejecting the null hypothesis when it’s false. It’s also equal to 1 minus the beta.
"
Statistics,What are two ways to increase the power of a test?,"To increase the power of the test, you can do two things:
You can increase alpha, but it also increases the chance of a type 1 error
Increase the sample size, n. This maintains the type 1 error but reduces type 2.
"
Statistics,Power Analysis,"Statistical power is one piece in a puzzle that has four related parts; they are:
Effect Size. The quantified magnitude of a result present in the population. Effect size is calculated using a specific statistical measure, such as Pearson’s correlation coefficient for the relationship between variables or Cohen’s d for the difference between groups.
Sample Size. The number of observations in the sample.
Significance. The significance level used in the statistical test, e.g. alpha. Often set to 5% or 0.05.
Statistical Power. The probability of accepting the alternative hypothesis if it is true.
All four variables are related. For example, a larger sample size can make an effect easier to detect, and the statistical power can be increased in a test by increasing the significance level.
A power analysis involves estimating one of these four parameters given values for three other parameters. This is a powerful tool in both the design and in the analysis of experiments that we wish to interpret using statistical hypothesis tests.
For example, the statistical power can be estimated given an effect size, sample size and significance level. Alternately, the sample size can be estimated given different desired levels of significance.
Perhaps the most common use of a power analysis is in the estimation of the minimum sample size required for an experiment.
As a practitioner, we can start with sensible defaults for some parameters, such as a significance level of 0.05 and a power level of 0.80. We can then estimate a desirable minimum effect size, specific to the experiment being performed. A power analysis can then be used to estimate the minimum sample size required.
In addition, multiple power analyses can be performed to provide a curve of one parameter against another, such as the change in the size of an effect in an experiment given changes to the sample size. More elaborate plots can be created varying three of the parameters. This is a useful tool for experimental design.
"
Statistics,Student’s t Test Power Analysis,"In this section, we will look at the Student’s t test, which is a statistical hypothesis test for comparing the means from two samples of Gaussian variables. The assumption, or null hypothesis, of the test is that the sample populations have the same mean, e.g. that there is no difference between the samples or that the samples are drawn from the same underlying population.
The test will calculate a p-value that can be interpreted as to whether the samples are the same (fail to reject the null hypothesis), or there is a statistically significant difference between the samples (reject the null hypothesis). A common significance level for interpreting the p-value is 5% or 0.05.
Significance level (alpha): 5% or 0.05.
The size of the effect of comparing two groups can be quantified with an effect size measure. A common measure for comparing the difference in the mean from two groups is the Cohen’s d measure. Cohen's d is determined by calculating the mean difference between your two groups, and then dividing the result by the pooled standard deviation.  It calculates a standard score that describes the difference in terms of the number of standard deviations that the means are different. A large effect size for Cohen’s d is 0.80 or higher, as is commonly accepted when using the measure.
Effect Size: Cohen’s d of at least 0.80.
We can use the default and assume a minimum statistical power of 80% or 0.8.
Statistical Power: 80% or 0.80.
For a given experiment with these defaults, we may be interested in estimating a suitable sample size. That is, how many observations are required from each sample in order to at least detect an effect of 0.80 with an 80% chance of detecting the effect if it is true (20% of a Type II error) and a 5% chance of detecting an effect if there is no such effect (Type I error).
"
Statistics,What is Pearson Correlation?,"Correlation between sets of data is a measure of how well they are related. The most common measure of correlation in stats is the Pearson Correlation. The full name is the Pearson Product Moment Correlation (PPMC). It shows the linear relationship between two sets of data. In simple terms, it answers the question, Can I draw a line graph to represent the data? Two letters are used to represent the Pearson correlation: Greek letter rho (ρ) for a population and the letter “r” for a sample.
Potential problems with Pearson correlation.
The PPMC is not able to tell the difference between dependent variables and independent variables. For example, if you are trying to find the correlation between a high calorie diet and diabetes, you might find a high correlation of .8. However, you could also get the same result with the variables switched around. In other words, you could say that diabetes causes a high calorie diet. That obviously makes no sense. Therefore, as a researcher you have to be aware of the data you are plugging in. In addition, the PPMC will not give you any information about the slope of the line; it only tells you whether there is a relationship.
Real Life Example
Pearson correlation is used in thousands of real life situations. For example, scientists in China wanted to know if there was a relationship between how weedy rice populations are different genetically. The goal was to find out the evolutionary potential of the rice. Pearson’s correlation between the two groups was analyzed. It showed a positive Pearson Product Moment correlation of between 0.783 and 0.895 for weedy rice populations. This figure is quite high, which suggested a fairly strong relationship.
"
Statistics,A comparison of the Pearson and Spearman correlation methods,"Pearson = +0.851, Spearman = +1
A correlation coefficient measures the extent to which two variables tend to change together. The coefficient describes both the strength and the direction of the relationship. Minitab offers two different correlation analyses:
The Pearson correlation evaluates the linear relationship between two continuous variables. A relationship is linear when a change in one variable is associated with a proportional change in the other variable.
For example, you might use a Pearson correlation to evaluate whether increases in temperature at your production facility are associated with decreasing thickness of your chocolate coating.
The Spearman correlation evaluates the monotonic relationship between two continuous or ordinal variables. In a monotonic relationship, the variables tend to change together, but not necessarily at a constant rate. The Spearman correlation coefficient is based on the ranked values for each variable rather than the raw data.
Spearman correlation is often used to evaluate relationships involving ordinal variables. For example, you might use a Spearman correlation to evaluate whether the order in which employees complete a test exercise is related to the number of months they have been employed.
"
Statistics,How do we check if a variable follows the normal distribution? ,"Plot a histogram out of the sampled data. If you can fit the bell-shaped ""normal"" curve to the histogram, then the hypothesis that the underlying random variable follows the normal distribution can not be rejected.
Check Skewness and Kurtosis of the sampled data. Skewness = 0 and kurtosis = 3 are typical for a normal distribution, so the farther away they are from these values, the more non-normal the distribution.
Use Kolmogorov-Smirnov or/and Shapiro-Wilk tests for normality. They take into account both Skewness and Kurtosis simultaneously.
Check for Quantile-Quantile plot. It is a scatterplot created by plotting two sets of quantiles against one another. Normal Q-Q plot place the data points in a roughly straight line.
"
Statistics,What could be some issues if the distribution of the test data is significantly different than the distribution of the training data?,"The model that has high training accuracy might have low test accuracy. Without further knowledge, it is hard to know which dataset represents the population data and thus the generalizability of the algorithm is hard to measure. This should be mitigated by repeated splitting of train vs test dataset (as in cross validation).
When there is a change in data distribution, this is called the dataset shift. If the train and test data has a different distribution, then the classifier would likely overfit to the train data.
This issue can be overcome by using a more general learning method.
This can occur when:
P(y|x) are the same but P(x) are different. (covariate shift)
P(y|x) are different. (concept shift)
The causes can be:
Training samples are obtained in a biased way. (sample selection bias)
Train is different from test because of temporal, spatial changes. (non-stationary environments)
Solution to covariate shift
importance weighted cv
"
Statistics,What is alpha- and beta-values?,"Alpha is also known as the level of significance. This represents the probability of obtaining your results due to chance. The smaller this value is, the more “unusual” the results, indicating that the sample is from a different population than it’s being compared to, for example. Commonly, this value is set to .05 (or 5%), but can take on any value chosen by the research not exceeding .05.
Alpha also represents your chance of making a Type I Error. What’s that? The chance that you reject the null hypothesis when in reality, you should fail to reject the null hypothesis. In other words, your sample data indicates that there is a difference when in reality, there is not. Like a false positive.
The other key-value relates to the power of your study. Power refers to your study’s ability to find a difference if there is one. It logically follows that the greater the power, the more meaningful your results are. Beta = 1 – Power. Values of beta should be kept small, but do not have to be as small as alpha values. Values between .05 and .20 are acceptable.
Beta also represents the chance of making a Type II Error. As you may have guessed, this means that you came to the wrong conclusion in your study, but it’s the opposite of a Type I Error. With a Type II Error, you incorrectly fail to reject the null. In simpler terms, the data indicates that there is not a significant difference when in reality there is. Your study failed to capture a significant finding. Like a false negative.
"
Statistics,What are the confidence intervals of the coefficients?,"Confidence interval (CI) is a type of interval estimate (of a population parameter) that is computed from the observed data. The confidence level is the frequency (i.e., the proportion) of possible confidence intervals that contain the true value of their corresponding parameter. In other words, if confidence intervals are constructed using a given confidence level in an infinite number of independent experiments, the proportion of those intervals that contain the true value of the parameter will match the confidence level.
Confidence intervals consist of a range of values (interval) that act as good estimates of the unknown population parameter. However, the interval computed from a particular sample does not necessarily include the true value of the parameter. Since the observed data are random samples from the true population, the confidence interval obtained from the data is also random. If a corresponding hypothesis test is performed, the confidence level is the complement of the level of significance, i.e. a 95% confidence interval reflects a significance level of 0.05. If it is hypothesized that a true parameter value is 0 but the 95% confidence interval does not contain 0, then the estimate is significantly different from zero at the 5% significance level.
The desired level of confidence is set by the researcher (not determined by data). Most commonly, the 95% confidence level is used. However, other confidence levels can be used, for example, 90% and 99%.
Factors affecting the width of the confidence interval include the size of the sample, the confidence level, and the variability in the sample. A larger sample size normally will lead to a better estimate of the population parameter. A Confidence Interval is a range of values we are fairly sure our true value lies in.
X ± Z*s/√(n), X is the mean, Z is the chosen Z-value from the table, s is the standard deviation, n is the number of samples. The value after the ± is called the margin of error.
"
Statistics,Bayesian and frequentist probabilities,"The Bayesian interpretation uses probability to specify the degree of uncertainty that the user has about an event. It is sometimes referred to as “subjective probability” or “degree of belief”. The frequentist interpretation considers the relative frequencies of events of interest to the total number of events that occurred. The probability of an event is defined as the relative frequency of the event in the limit when one has infinite data.
"
Statistics,What is difference between Probability and Statistics?,"Probability theory and statistics are often presented together, but they concern different aspects of uncertainty. One way of contrasting them is by the kinds of problems that are considered. Using probability, we can consider a model of some process, where the underlying uncertainty is captured by random variables, and we use the rules of probability to derive what happens. In statistics, we observe that something has happened and try to figure out the underlying process that explains the observations. In this sense, machine learning is close to statistics in its goals to construct a model that adequately represents the process that generated the data. We can use the rules of probability to obtain a “best-fitting” model for some data.
"
Statistics,Discrete and Continuous Probabilities ,"Depending on whether the target space is discrete or continuous, the natural way to refer to distributions is different. When the target space T is discrete, we can specify the probability that a random variable X takes a particular value x ∈ T , denoted as P(X = x). The expression P(X = x) for a discrete random variable X is known as the probability mass function. When the target space T is continuous, e.g., function the real line R, it is more natural to specify the probability that a random variable X is in an interval, denoted by P(a ⩽ X ⩽ b) for a < b. By convention, we specify the probability that a random variable X is less than a particular value x, denoted by P(X ⩽ x). The expression P(X ⩽ x) for cumulative a continuous random variable X is known as the cumulative distribution function
"
Statistics,"Joint, marginal and conditional probabilities ","For two random variables X and Y , the probability that X = x and Y = y is (lazily) written as p(x, y) and is called the joint probability. One can think of a probability as a function that takes state x and y and returns a real number, which is the reason we write p(x, y). The marginal probability that X takes the value x irrespective of the value of random variable Y is (lazily) written as p(x). We write X ∼ p(x) to denote that the random variable X is distributed according to p(x). If we consider only the instances where X = x, then the fraction of instances (the conditional probability) for which Y = y is written (lazily) as p(y | x).
"
Statistics,PDF (Probability Density Function),"A function f : RD → R is called a probability density function (pdf ) if probability density function 
1. ∀x ∈ R pdf D : f(x) ⩾ 0 
2. Its integral exists and Z RD f(x)dx = 1.
For probability mass functions (pmf) of discrete random variables, the integral in is replaced with a sum
"
Statistics,PMF (Probability Mass Function),"PMF is a statistical term that describes the probability distribution of the Discrete random variable
People often get confused between PDF and PMF. The PDF is applicable for continues random variable while PMF is applicable for discrete random variable For e.g, Throwing a dice (You can only select 1 to 6 numbers (countable) )
A probability mass function (PMF) is a function that gives the probability that a discrete random variable is exactly equal to some value. The PMF does not work for continuous random variables, because for a continuous random variable P(X=x)=0 for all x∈R. Instead, we can usually define the probability density function (PDF). The PDF is the density of probability rather than the probability mass:
"
Statistics,CDF (Cumulative Distribution Function),"A cumulative distribution function (cdf) of a multivariate real-valued random variable X with states x ∈ RD is given by FX(x) = P(X1 ⩽ x1, . . . , XD ⩽ xD), where X = [X1, . . . , XD] ⊤, x = [x1, . . . , xD] ⊤, and the right-hand side represents the probability that random variable Xi takes the value smaller than or equal to xi . There are cdfs, which do not have corresponding pdfs. The cdf can be expressed also as the integral of the probability density function f(x)
"
Statistics,Bayes’ Theorem,"In machine learning and Bayesian statistics, we are often interested in making inferences of unobserved (latent) random variables given that we have observed other random variables. Let us assume we have some prior knowledge p(x) about an unobserved random variable x and some relationship p(y | x) between x and a second random variable y, which we can observe. If we observe y, we can use Bayes’ theorem to draw some conclusions about x given the observed values of y.
p(x) is the prior, which encapsulates our subjective prior prior knowledge of the unobserved (latent) variable x before observing any data. We can choose any prior that makes sense to us, but it is critical to ensure that the prior has a nonzero pdf (or pmf) on all plausible x, even if they are very rare.
The likelihood p(y | x) describes how x and y are related, and in the case of discrete probability distributions, it is the probability of the data y if we were to know the latent variable x. Note that the likelihood is not a distribution in x, but only in y. We call p(y | x) either the “likelihood of x (given y)” or the “probability of y given x” but never the likelihood of y. 
The posterior p(x | y) is the quantity of interest in Bayesian statistics posterior because it expresses exactly what we are interested in, i.e., what we know about x after having observed y.
p(y) := Z p(y | x)p(x)dx = EX[p(y | x)] is the marginal likelihood/evidence. 
The marginal likelihood is independent of x, and it ensures that the posterior p(x | y) is normalized. The marginal likelihood can also be interpreted as the expected likelihood where we take the expectation with respect to the prior p(x).
"
Statistics,"Explain Prior, Posterior, Likelihood","Likelihood function (often simply called the likelihood) measures how well a statistical model explains observed data by calculating the probability of seeing that data under different parameter values of the model. – p(X| θ)
A prior probability distribution of an uncertain quantity, often simply called the prior, is its assumed probability distribution before some evidence is taken into account – p(θ)
The posterior probability is a type of conditional probability that results from updating the prior probability with information summarized by the likelihood via an application of Bayes' rule. – p(θ | x)
Bayes rule:
P(θ|x) = p(x| θ) * p(θ) / p(x)
p ( x ) = ∫ p ( x | θ ) p ( θ ) d θ 
"
Statistics,"Covariance, Variance, Correlation","The covariance between two univariate random variables X, Y ∈ R is given by the expected product of their deviations from their respective means, i.e., CovX,Y [x, y] := EX,Y [(x − EX[x])(y − EY [y])]
By using the linearity of expectations, the expression in Definition 6.5 can be rewritten as the expected value of the product minus the product of the expected values, i.e., Cov[x, y] = E[xy] − E[x]E[y] .
The covariance of a variable with itself Cov[x, x] is called the variance is denoted by VX[x]. The square root of the variance is called the standard deviation and is often denoted by σ(x). The notion of covariance can be generalized to multivariate random variables.
The correlation between two random variables X, Y is given by corr[x, y] = Cov[x, y] /sqrt(V[x]V[y]) ∈ [−1, 1] . The correlation matrix is the covariance matrix of standardized random variables, x/σ(x). In other words, each random variable is divided by its standard deviation (the square root of the variance) in the correlation matrix. The covariance (and correlation) indicate how two random variables are related. Positive correlation corr[x, y] means that when x grows, then y is also expected to grow. Negative correlation means that as x increases, then y decreases.
"
Statistics,Gaussian Mixture,"Consider a mixture of two univariate Gaussian densities p(x) = αp1(x) + (1 − α)p2(x), (6.80) where the scalar 0 < α < 1 is the mixture weight, and p1(x) and p2(x) are univariate Gaussian densities with different parameters, i.e., (µ1, σ2 1 ) ̸= (µ2, σ2 2 ). Then the mean of the mixture density p(x) is given by the weighted sum of the means of each random variable: E[x] = αµ1 + (1 − α)µ2 . The variance of the mixture density p(x) is given by V[x] =  ασ2 1 + (1 − α)σ 2 2  +  αµ2 1 + (1 − α)µ 2 2  − [αµ1 + (1 − α)µ2] 2 .
"
Statistics,Beta Distribution,"We may wish to model a continuous random variable on a finite interval. The Beta distribution is a distribution over a continuous random variable µ ∈ [0, 1], which is often used to represent the probability for some binary event (e.g., the parameter governing the Bernoulli distribution). The Beta distribution Beta(α, β) (illustrated in Figure 6.11) itself is governed by two parameters α > 0, β > 0 and is defined as
Intuitively, α moves probability mass toward 1, whereas β moves probability mass toward 0. There are some special cases: For α = 1 = β, we obtain the uniform distribution U[0, 1]. For α, β < 1, we get a bimodal distribution with spikes at 0 and 1. For α, β > 1, the distribution is unimodal. For α, β > 1 and α = β, the distribution is unimodal, symmetric, and centered in the interval [0, 1], i.e., the mode/mean is at 1/2 .
"
Statistics,Conjugate Prior,"According to Bayes’ theorem (6.23), the posterior is proportional to the product of the prior and the likelihood. The specification of the prior can be tricky for two reasons: First, the prior should encapsulate our knowledge about the problem before we see any data. This is often difficult to describe. Second, it is often not possible to compute the posterior distribution analytically. However, there are some priors that are computationally: conjugate priors.
A prior is conjugate for the likelihood function if the posterior is of the same form/type as the prior
Conjugacy is particularly convenient because we can algebraically calculate our posterior distribution by updating the parameters of the prior distribution.
The Beta distribution is the conjugate prior for the parameter µ in both the Binomial and the Bernoulli likelihood. For a Gaussian likelihood function, we can place a conjugate Gaussian prior on the mean. The reason why the Gaussian likelihood appears twice in the table is that we need to distinguish the univariate from the multivariate case. In the univariate (scalar) case, the inverse Gamma is the conjugate prior for the variance. In the multivariate case, we use a conjugate inverse Wishart distribution as a prior on the covariance matrix. The Dirichlet distribution is the conjugate prior for the multinomial likelihood function.
"
Statistics,Are there any differences between the expected value and mean value?,"Expected value is used when we want to calculate the mean of a probability distribution. This represents the average value we expect to occur before collecting any data. Mean is typically used when we want to calculate the average value of a given sample.

"
Statistics,How do you identify if a coin is biased?,"We collect data by flipping the coin 200 times. 
To perform a chi-square test (or any other statistical test), we first must establish our null hypothesis. In this example, our null hypothesis is that the coin should be equally likely to land head-up or tails-up every time. The null hypothesis allows us to state expected frequencies. For 200 tosses, we would expect 100 heads and 100 tails.
The Observed values are those we gather ourselves. The expected values are the frequencies expected, based on our null hypothesis. We total the rows and columns as indicated. It's a good idea to make sure that the row totals equal the column totals (both total to 400 in this example).
Using probability theory, statisticians have devised a way to determine if a frequency distribution differs from the expected distribution. To use this chi-square test, we first have to calculate chi-squared.
Chi-squared = (observed-expected)2/(expected)
We have two classes to consider in this example, heads and tails.
Now we have to consult a table of critical values of the chi-squared distribution. 
The left-most column list the degrees of freedom (df). We determine the degrees of freedom by subtracting one from the number of classes. In this example, we have two classes (heads and tails), so our degrees of freedom is 1. Our chi-squared value is 1.28. Move across the row for 1 df until we find critical numbers that bound our value. In this case, 1.07 (corresponding to a probability of 0.30) and 1.64 (corresponding to a probability of 0.20). We can interpolate our value of 1.24 to estimate a probability of 0.27. This value means that there is a 73% chance that our coin is biased. In other words, the probability of getting 108 heads out of 200 coin tosses with a fair coin is 27%. In biological applications, a probability � 5% is usually adopted as the standard. This value means that the chances of an observed value arising by chance is only 1 in 20. Because the chi-squared value we obtained in the coin example is greater than 0.05 (0.27 to be precise), we accept the null hypothesis as true and conclude that our coin is fair.
"
Statistics,Exponential Family,"An exponential family is a family of probability distributions, parameterized by θ ∈ RD, of the form p(x | θ) = h(x) exp (⟨θ, ϕ(x)⟩ − A(θ)) , (6.107) where ϕ(x) is the vector of sufficient statistics. In general, any inner product (Section 3.2) can be used in (6.107), and for concreteness we will use the standard dot product here (⟨θ, ϕ(x)⟩ = θ ⊤ϕ(x)). Note that the form of the exponential family is essentially a particular expression of gθ(ϕ(x)) in the Fisher-Neyman theorem
Exponential families include many of the most common distributions. Among many others, exponential families includes the following:[6]
normal
exponential
gamma
chi-squared
beta
Dirichlet
Bernoulli
categorical
Poisson
Wishart
inverse Wishart
geometric
"
Statistics,What Is a Statistical Interaction?,"A statistical interaction is when two or more variables interact, and this results in a third variable being affected. 
Examples. Real-world examples of interaction include: Interaction between adding sugar to coffee and stirring the coffee. Neither of the two individual variables has much effect on sweetness but a combination of the two does.
"
Statistics,What is a Skewed Distribution?,"A skewed distribution occurs when one tail is longer than the other. Skewness defines the asymmetry of a distribution. Unlike the familiar normal distribution with its bell-shaped curve, these distributions are asymmetric. The two halves of the distribution are not mirror images because the data are not distributed equally on both sides of the distribution’s peak.
"
Statistics,Examples of Right-Skewed Distributions,"Right skewed distributions are the more common form. These distributions tend to occur when there is a lower limit, and most values are relatively close to the lower bound. Values can’t be less than this bound but can fall far from the peak on the high end, causing them to skew positively.
For example, right skewed distributions can occur in the following cases:
Time to failure cannot be less than zero, but there is no upper bound.
Wait and response times cannot be less than zero, but there are no upper limits.
Sales data cannot be less than zero but can have unusually large values.
Humans have a minimum viable weight but can have large extreme values.
Income cannot be less than zero, but there are some extremely high incomes.
"
Statistics,You have an 50-50 mixture of two normal distributions with the same standard deviation. How far apart do the means need to be in order for this distribution to be bimodal?,"More than two standard deviations
"
Statistics,"Given draws from a normal distribution with known parameters, how can you simulate draws from a uniform distribution?","Plug in the value to the CDF of the same random variable
"
Statistics,You are told that your regression model is suffering from multicollinearity. How do verify this is true and build a better model?,"You should create a correlation matrix to identify and remove variables with a correlation above 75%. Keep in mind that our threshold here is subjective.
You could also calculate VIF (variance inflation factor) to check for the presence of multicollinearity. 
You can’t just remove variables, so you should use a penalized regression model or add random noise in the correlated variables, but this approach is less ideal.
"
Statistics,Chi-square tests vs t-tests,"Both chi-square tests and t tests can test for differences between two groups. However, a t test is used when you have a dependent quantitative variable and an independent categorical variable (with two groups). A chi-square test of independence is used when you have two categorical variables.
"
Statistics,Normal Distribution vs. t-Distribution: What’s the Difference?,"A Z-test is a hypothesis test with a normal distribution that uses a z-statistic. A z-test is used when you know the population variance or if you don’t know the population variance but have a large sample size.
A T-test is a hypothesis test with a t-distribution that uses a t-statistic. You would use a t-test when you don’t know the population variance and have a small sample size.
"
Statistics,What is the standard error? What is the standard error of mean?,"Standard error of a statistic is the standard deviation of its sampling distribution or an estimate of that standard deviation.
Using CLM, we can estimate the standard error of mean by using population standard deviation divided by the square root of sample size n. If the population standard deviation is unknown, we can use the sample standard deviation as an estimation.
"
Statistics,Standard Error of the Mean vs. Standard Deviation: What's the Difference?,"The standard deviation (SD) measures the amount of variability, or dispersion, from the individual data values to the mean. SD is a frequently-cited statistic in many applications from math and statistics to finance and investing.
Standard error of the mean (SEM) measures how far the sample mean (average) of the data is likely to be from the true population mean. The SEM is always smaller than the SD.
SEM is calculated simply by taking the standard deviation and dividing it by the square root of the sample size.
"
Statistics,Confidence Interval ,"CI = x  +/-  t1-α/2, n-1*(s/√n)
where:
x: sample mean
t: the critical t-value, based on the significance level α and sample size n
s: sample standard deviation
n: sample size
"
Statistics,Sampling Distributions and the Standard Error of the Mean,"Imagine you draw a random sample of 50 from a population, measure a property, and calculate the mean. Now, suppose you repeat that study many times. You repeatedly draw random samples of the same size, calculate the mean for each sample, and graph all the means on a histogram. Ultimately, the histogram displays the distribution of sample means for random samples of size 50 for the characteristic you’re measuring.
Statisticians call this type of distribution a sampling distribution. And, because we’re calculating the mean, it’s the sampling distribution of the mean. There’s a different sampling distribution for each sample size.
This distribution is the sampling distribution for the above experiment. Remember that the curve describes the distribution of sample means and not individual observations. Like other distributions, sampling distributions have a central location and variability around that center.
The center falls on the population mean because random sampling tends to converge on this value.
The variability, or spread, describes how far sample means tend to fall from the population mean.
The wider the distribution, the further the sample means tend to fall from the population mean. That’s not good when you’re using sample means to estimate population means! You want narrow sampling distributions where sample means fall near the population mean.
The variability of the sampling distribution is the standard error of the mean! More specifically, the SEM is the standard deviation of the sampling distribution. For the example sampling distribution, the SEM is 3. We’ll interpret that value shortly.
The mean of the sampling distribution of sample means is mean.
"
Statistics,What is the Central Limit Theorem (CLM)?,"The Central Limit Theorem states that no matter what is the population’s original distribution, when taking random samples from the population, the distribution of the means or sums from the random samples approaches a normal distribution, with mean equals to the population mean, as the random sample size gets larger
"
Statistics,What general conditions must be satisfied for the central limit theorem to hold?,"The data must be sampled randomly
The sample values must be independent of each other
The sample size must be sufficiently large, generally it should be greater or equal than 30
"
Statistics,What does it mean if a model is heteroscedastic? what about homoscedastic?,"A model is heteroscedastic when the variance in errors is not consistent. Conversely, a model is homoscedastic when the variances in errors is consistent.
"
Statistics,What does Design of Experiments mean?,"Design of experiments also known as DOE, it is the design of any task that aims to describe and explain the variation of information under conditions that are hypothesized to reflect the variable. In essence, an experiment aims to predict an outcome based on a change in one or more inputs (independent variables).
"
Statistics,What does the Poisson distribution represent?,"The Poisson distribution is a discrete distribution that gives the probability of the number of independent events occurring in a fixed time. An example of when you would use this is if you want to determine the likelihood of X patients coming into a hospital in a given hour.
The mean and variance are both equal to λ.
"
Statistics,"If you had draws from a normal distribution with known parameters, how would you simulate draws from a uniform distribution?","A question like this tests your knowledge of the concepts of uniform and normal distributions.
There’s a simple answer to this. To simulate draws from a uniform distribution, you would plug the values into the normal cumulative distribution function (CDF) for the same random variable.
This is known as the Universality of the Uniform or Probability Integral Transform.
"
Statistics,How do you transform a Skewed Distribution into a Normal Distribution?,"To transform a Skewed Distribution into a Normal Distribution we apply some linearized function on it. Some common functions that achieve this goal are:
Logarithmic function: We can use it to make extremely skewed distributions less skewed, especially for right-skewed distributions. The only condition is that this function is defined only for strictly positive numbers. $$ f(x) = ln(x) $$
Square root transformation n: this one has an average effect on distribution shape: it’s weaker than logarithmic transformation, and it’s also used for reducing right-skewed distributions, but is defined only for positive numbers. f(x) = \sqrt{x}
Reciprocal transformation: this one reverses the order among values of the same sign, so large values become smaller, but the negative reciprocal preserves the order among values of the same sign. The only condition is that this function is not defined for zero values. f(x) = 1/x
Exponential or Power transformation: has a reasonable effect on distribution shape; generally, we apply power transformation (power of two usually) to reduce left skewness. We could also try any exponent to see which one provides better results. f(x) = x^n
Box-Cox Transformation: in this transformation, we’re searching and evaluating all the other transformations and choosing the best one. It's defined as:
The exponent here is a variable called lambda (λ) that varies over the range of -5 to 5, and in the process of searching, we examine all values of λ. Finally, we choose the optimal value (resulting in the best approximation to a normal distribution) for the variable.
"
Statistics,What's the difference between Binomial Distribution and Geometric Distribution?,"The Binomial distribution describes the probability of obtaining k successes in n Bernoulli experiments, i.e an experiment which has only two possible outcomes, often call them success and failure. Its probability function describes the probability of getting exactly k successes in n independent Bernoulli trials:
The Geometric distribution describes the probability of experiencing a certain amount of failures before experiencing the first success in a series of Bernoulli experiments. This probability is given by:
P(X=k)=pk(1−p)n−k
So as we can see, the key difference is that in a binomial distribution, there is a fixed number of trials meanwhile in a geometric distribution, we’re interested in the number of trials required until we obtain a success.
"
Statistics,Bayesian Inference,"Focusing solely on some statistic of the posterior distribution (such as the parameter θ ∗ that maximizes the posterior) leads to loss of information, which can be critical in a system uses the prediction p(x | θ ∗ ) to make decisions. These decision-making systems typically have different objective functions than the likelihood, a squared-error loss or a mis-classification error. Therefore, having the full posterior distribution around can be extremely useful and leads to more robust decisions. Bayesian inference is about finding this posterior distribution. For a dataset X , a parameter prior p(θ), and a likelihood function, the posterior is obtained by applying Bayes’ theorem. 
Parameter estimation via maximum likelihood or MAP estimation yields a consistent point estimate θ∗ of the parameters, and the key computational problem to be solved is optimization. In contrast, Bayesian inference yields a (posterior) distribution, and the key computational problem to be solved is integration. Predictions with point estimates are straightforward, whereas predictions in the Bayesian framework require solving another integration problem. However, Bayesian inference gives us a principled way to incorporate prior knowledge, account for side information, and incorporate structural knowledge, all of which is not easily done in the context of parameter estimation. Moreover, the propagation of parameter uncertainty to the prediction can be valuable in decision-making systems for risk assessment and exploration in the context of data-efficient learning.
If we do not choose a conjugate prior on the parameters, the integrals are not analytically tractable, and we cannot compute the posterior in closed form.
"
Statistics,Latent-Variable Models,"In practice, it is sometimes useful to have additional latent variables z (besides the model parameters θ) as part of the model. These latent variables are different from the model parameters θ as they do not parametrize the model explicitly. Latent variables may describe the data-generating process, thereby contributing to the interpretability of the model. They also often simplify the structure of the model and allow us to define simpler and richer model structures. Simplification of the model structure often goes hand in hand with a smaller number of model parameters. Learning in latent-variable models (at least via maximum likelihood) can be done in a principled way using the expectation maximization (EM) algorithm.
Denoting data by x, the model parameters by θ and the latent variables by z, we obtain the conditional distribution 
p(x | z, θ)
"
Statistics,Simpson’s paradox,"Simpson’s paradox refers to the situations in which a trend or relationship that is observed within multiple groups disappears or reverses when the groups are combined. The quick answer to why there is Simpson’s paradox is the existence of confounding variables.
"
