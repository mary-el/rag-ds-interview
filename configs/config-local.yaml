db:
  host: ${DB_HOST}
  port: ${DB_PORT}
  dbname: ${DB_NAME}
  user: ${DB_USER}
  password: ${DB_PASSWORD}

faiss:
  index_path: "faiss.index"
  distance: "l2"
  model: "sentence-transformers/all-MiniLM-L6-v2"  # faiss embedding model
  dim: 384
  records_num: 20                                  # number of candidates to retrieve before reranking (or final number if reranking disabled)

reranker:
  enabled: true
  model: "cross-encoder/ms-marco-MiniLM-L6-v2"
  top_k_candidates: 3  # number of documents to use after reranking

doc_parsing:                                      # knowledge base settings
  input_dir: "data/docs"
  files: ['Classical_models.docx', 'Classical_NLP.docx', 'Data.docx', 'Deep_Learning.docx', 'LLM.docx',
         'Metrics.docx', 'Probability_and_Statistics.docx']
  chunk_size: 2000
  chunk_overlap: 100

llm:
  provider: "local"                              # openai, local
  model_name: "google/gemma-2-2b-it"
  api_key: ${API_KEY}
  temperature: 0.7
  top_p: 0.95
  top_k: 20
  repetition_penalty: 1.2
  base_url: "https://api.together.xyz"            # for openai inference
  template_files: {
                    'qa': 'qa_template_gemma.txt',
                    'quiz': 'quiz_template_gemma.txt'
  }
  max_tokens: 512
  do_sample: True
  num_beams: 1
  device: cuda                                  # for local inference

interface:
  telegram_token: ${TELEGRAM_TOKEN}
  bot_name: DS_interview_assistant_bot


app:
  log_folder: logs
  stream_log: False
