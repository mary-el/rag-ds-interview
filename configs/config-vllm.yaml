db:
  host: ${DB_HOST}
  port: ${DB_PORT}
  dbname: ${DB_NAME}
  user: ${DB_USER}
  password: ${DB_PASSWORD}

faiss:
  index_path: "faiss.index"
  distance: "l2"
  model: "sentence-transformers/all-MiniLM-L6-v2"  # faiss embedding model
  dim: 384
  records_num: 20                                  # number of candidates to retrieve before reranking (or final number if reranking disabled)

reranker:
  enabled: true
  model: "cross-encoder/ms-marco-MiniLM-L6-v2"
  top_k_candidates: 3  # number of documents to use after reranking

doc_parsing:                                      #   knowledge base settings
  input_dir: "data/docs"
  files: ['Classical_models.docx', 'Classical_NLP.docx', 'Data.docx', 'Deep_Learning.docx', 'LLM.docx',
         'Metrics.docx', 'Probability_and_Statistics.docx']
  chunk_size: 1000
  chunk_overlap: 100

llm:
  provider: "openai"                              # openai, local
  model_name: "openai/gpt-oss-20b"
  api_key: sk-fake-key
  temperature: 0.7
  top_p: 0.95
  top_k: 20
  repetition_penalty: 1.2
  base_url: "http://127.0.0.1:1234/v1"            # for openai inference
  template_files: {
                    'qa': 'qa_template.txt',
                    'quiz': 'quiz_template.txt',
                    'rate': 'rate_answer_template.txt'
                  }
  max_tokens: 512
  do_sample: True
  num_beams: 1
  device: cuda                                  # for local inference

interface:
  telegram_token: ${TELEGRAM_TOKEN}
  bot_name: DS_interview_assistant_bot


app:
  log_folder: logs
  stream_log: False
