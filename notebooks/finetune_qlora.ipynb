{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4166d5e-a08a-4e33-aeaf-08af14937d55",
   "metadata": {},
   "source": [
    "# Model Fine-tuning for RAG using QLoRA\n",
    "\n",
    "This notebook demonstrates how to fine-tune the Gemma-3-1B model using **QLoRA (Quantized Low-Rank Adaptation)** to improve RAG (Retrieval-Augmented Generation) answers for Data Science interview preparation.\n",
    "\n",
    "## Overview\n",
    "- **Objective**: Fine-tune Gemma-3-1B to better answer questions based on provided context\n",
    "- **Method**: QLoRA (4-bit quantization + LoRA adapters) for efficient fine-tuning\n",
    "- **Reference**: Following the [Google Gemma fine-tuning guide](https://ai.google.dev/gemma/docs/core/huggingface_text_finetune_qlora?authuser=1)\n",
    "\n",
    "## Key Benefits of QLoRA\n",
    "- **Memory Efficient**: 4-bit quantization reduces memory requirements significantly\n",
    "- **Fast Training**: Only trains a small subset of parameters (LoRA adapters)\n",
    "- **Maintains Quality**: Minimal performance degradation compared to full fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f76f898",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install required packages for fine-tuning:\n",
    "- `peft`: Parameter-Efficient Fine-Tuning library (for LoRA)\n",
    "- `bitsandbytes`: 4-bit quantization support\n",
    "- `accelerate`: Training acceleration utilities\n",
    "- `bert_score`: BERT-based evaluation metrics\n",
    "- `evaluate`: Hugging Face evaluation library\n",
    "- `trl`: Transformer Reinforcement Learning library (for SFTTrainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7670e7f6",
   "metadata": {},
   "source": [
    "Install the specific Transformers version that supports Gemma-3 models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb54d2a8",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Import all necessary libraries for model training, data processing, and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66f6c236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, BitsAndBytesConfig, AutoConfig, set_seed\n",
    "from torch.utils.data import Dataset\n",
    "from peft import PeftModel, get_peft_model, LoraConfig\n",
    "from typing import List, Tuple\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0531ee",
   "metadata": {},
   "source": [
    "## Load Training Data\n",
    "\n",
    "Load the question-answer dataset that will be used for fine-tuning. The dataset contains context, questions, and answers for Data Science interview preparation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1cfce5",
   "metadata": {},
   "source": [
    "## Device Configuration\n",
    "\n",
    "Set up the computing device (GPU if available, otherwise CPU). GPU is highly recommended for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0483900",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04e93215",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa = pd.read_csv(\"df_qa.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef7a4174",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the main goal of regression in machine...</td>\n",
       "      <td>To predict a continuous numerical value based ...</td>\n",
       "      <td>Classical models\\nLinear Regression\\nRegressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the two types of variables present in...</td>\n",
       "      <td>Dependent Variable (Target) and Independent Va...</td>\n",
       "      <td>Classical models\\nLinear Regression\\nRegressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What type of regression is used when there is ...</td>\n",
       "      <td>Simple Linear Regression.</td>\n",
       "      <td>Classical models\\nLinear Regression\\nRegressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What type of regression is used to model non-l...</td>\n",
       "      <td>Polynomial Regression.</td>\n",
       "      <td>Classical models\\nLinear Regression\\nRegressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the extensions of linear regression t...</td>\n",
       "      <td>Ridge and Lasso Regression.</td>\n",
       "      <td>Classical models\\nLinear Regression\\nRegressio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  What is the main goal of regression in machine...   \n",
       "1  What are the two types of variables present in...   \n",
       "2  What type of regression is used when there is ...   \n",
       "3  What type of regression is used to model non-l...   \n",
       "4  What are the extensions of linear regression t...   \n",
       "\n",
       "                                              Answer  \\\n",
       "0  To predict a continuous numerical value based ...   \n",
       "1  Dependent Variable (Target) and Independent Va...   \n",
       "2                          Simple Linear Regression.   \n",
       "3                             Polynomial Regression.   \n",
       "4                        Ridge and Lasso Regression.   \n",
       "\n",
       "                                             Context  \n",
       "0  Classical models\\nLinear Regression\\nRegressio...  \n",
       "1  Classical models\\nLinear Regression\\nRegressio...  \n",
       "2  Classical models\\nLinear Regression\\nRegressio...  \n",
       "3  Classical models\\nLinear Regression\\nRegressio...  \n",
       "4  Classical models\\nLinear Regression\\nRegressio...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_qa.head() # Inspect the first few rows to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9de1fb49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c0f82d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "torch_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93179053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login into Hugging Face Hub\n",
    "hf_token = 'YOUR_HF_TOKEN' \n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78beae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"models/gemma-3-1b-pt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18c9f67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_chat_template = \"\"\"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{{ '<start_of_turn>user\\n' + system_message + '\\n<end_of_turn>\\n' if system_message }}{% for message in loop_messages %}{% if message['role'] == 'user' %}{{ '<start_of_turn>user\\n' + message['content'] + '\\n<end_of_turn>\\n' }}{% elif message['role'] == 'assistant' %}{{ '<start_of_turn>model\\n' + message['content'] + '\\n<end_of_turn>\\n' }}{% endif %}{% endfor %}{{ '<start_of_turn>model\\n' if add_generation_prompt }}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "664c2704",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = gemma_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c9dc12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig( \n",
    "# BitsAndBytesConfig: Enables 4-bit quantization to reduce model size/memory usage\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_storage=torch_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f14d44d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have set `use_cache` to `False`, but cache_implementation is set to hybrid. cache_implementation will have no effect.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch_dtype,\n",
    "    attn_implementation='eager',\n",
    "    use_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f207de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an expert assistant helping a candidate prepare for a Data Scientist interview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a8ccc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "[CONTEXT]\n",
    "{context}\n",
    "[/CONTEXT]\n",
    "\n",
    "[QUESTION]\n",
    "{question}\n",
    "[/QUESTION]\n",
    "\n",
    "Answer the QUESTION based on the provided CONTEXT.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6204edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sample(example):\n",
    "    prompt = prompt_template.format(\n",
    "        context=example[\"Context\"],\n",
    "        question=example[\"Question\"],\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "            \"messages\": [\n",
    "              {\"role\": \"system\", \"content\": system_message},\n",
    "              {\"role\": \"user\", \"content\": prompt},\n",
    "              {\"role\": \"assistant\", \"content\": example[\"Answer\"]}\n",
    "                ]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2eabb102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a45a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(df_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8749c77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.train_test_split(shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d1fa5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Question': 'What is the difference between a frequency penalty and a presence penalty?',\n",
       " 'Answer': 'The frequency penalty is applied proportionally to how often a specific token has been used, while the presence penalty is only applied to tokens that have been used at least once.',\n",
       " 'Context': 'LLM\\nInference\\nFrequency and Presence Penalties_1\\nA frequency, or repetition, penalty, which is a decimal between -2.0 and 2.0, is a an LLM hyperparameter that indicates to a model that it should refrain from using the same tokens too often. It works by lowering the probabilities of tokens that were recently added to a response, so they’re less likely to be repeated to produce a more diverse output.\\n\\nThe presence penalty works in a similar way but is only applied to tokens that have been used at least once – while the frequency is applied proportionally to how often a specific token has been used. In other words, the frequency penalty affects output by preventing repetition, while the presence penalty encourages a wider assortment of tokens.',\n",
       " '__index_level_0__': 2122}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d186314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2793/2793 [00:00<00:00, 15311.68 examples/s]\n",
      "Map: 100%|██████████| 932/932 [00:00<00:00, 15591.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(format_sample, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7312d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Question': 'What is the difference between a frequency penalty and a presence penalty?',\n",
       " 'Answer': 'The frequency penalty is applied proportionally to how often a specific token has been used, while the presence penalty is only applied to tokens that have been used at least once.',\n",
       " 'Context': 'LLM\\nInference\\nFrequency and Presence Penalties_1\\nA frequency, or repetition, penalty, which is a decimal between -2.0 and 2.0, is a an LLM hyperparameter that indicates to a model that it should refrain from using the same tokens too often. It works by lowering the probabilities of tokens that were recently added to a response, so they’re less likely to be repeated to produce a more diverse output.\\n\\nThe presence penalty works in a similar way but is only applied to tokens that have been used at least once – while the frequency is applied proportionally to how often a specific token has been used. In other words, the frequency penalty affects output by preventing repetition, while the presence penalty encourages a wider assortment of tokens.',\n",
       " '__index_level_0__': 2122,\n",
       " 'messages': [{'content': 'You are an expert assistant helping a candidate prepare for a Data Scientist interview',\n",
       "   'role': 'system'},\n",
       "  {'content': '\\n[CONTEXT]\\nLLM\\nInference\\nFrequency and Presence Penalties_1\\nA frequency, or repetition, penalty, which is a decimal between -2.0 and 2.0, is a an LLM hyperparameter that indicates to a model that it should refrain from using the same tokens too often. It works by lowering the probabilities of tokens that were recently added to a response, so they’re less likely to be repeated to produce a more diverse output.\\n\\nThe presence penalty works in a similar way but is only applied to tokens that have been used at least once – while the frequency is applied proportionally to how often a specific token has been used. In other words, the frequency penalty affects output by preventing repetition, while the presence penalty encourages a wider assortment of tokens.\\n[/CONTEXT]\\n\\n[QUESTION]\\nWhat is the difference between a frequency penalty and a presence penalty?\\n[/QUESTION]\\n\\nAnswer the QUESTION based on the provided CONTEXT.\\n',\n",
       "   'role': 'user'},\n",
       "  {'content': 'The frequency penalty is applied proportionally to how often a specific token has been used, while the presence penalty is only applied to tokens that have been used at least once.',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b23c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "216cece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,  # The alpha parameter for Lora scaling.\n",
    "    r=4, # rank\n",
    "    lora_dropout=0.05,\n",
    "     task_type=\"CAUSAL_LM\", \n",
    "     target_modules=\"all-linear\",\n",
    "     # modules_to_save=[\"lm_head\", \"embed_tokens\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "be66a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel(model, peft_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "db4db676",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a2887b60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,261,440 || all params: 1,003,147,392 || trainable%: 0.3251\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b58835b2-e695-450b-affa-f165132b21d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    return {\n",
    "        \"perplexity\": math.exp(eval_preds[0]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6fe1b7",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "Configure SFT (Supervised Fine-Tuning) parameters for efficient training with QLoRA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ffed8125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"gemma-output\",         \n",
    "    max_length=512,                        \n",
    "    num_train_epochs=1,                     \n",
    "    per_device_train_batch_size=1,         \n",
    "    per_device_eval_batch_size=1,\n",
    "    # gradient_accumulation_steps=2,          \n",
    "    gradient_checkpointing=True,            \n",
    "    optim=\"adamw_torch_fused\",              \n",
    "    logging_steps=10,                      \n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    "    save_total_limit=3,\n",
    "    # eval_steps=2,\n",
    "    # eval_strategy=\"steps\",\n",
    "    learning_rate=2e-4,                     \n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,                      \n",
    "    warmup_ratio=0.03,                      \n",
    "    lr_scheduler_type=\"constant\",\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False, # We use template with special tokens\n",
    "        \"append_concat_token\": True, # Add EOS token as separator token between examples\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92aa1d9",
   "metadata": {},
   "source": [
    "## Initialize Trainer\n",
    "\n",
    "Create SFTTrainer with the model, datasets, and training configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a501dc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "Tokenizing train dataset: 100%|██████████| 2793/2793 [00:02<00:00, 1205.98 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 2793/2793 [00:00<00:00, 222797.47 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 932/932 [00:00<00:00, 1282.25 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 932/932 [00:00<00:00, 231457.83 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Create Trainer object\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a97761",
   "metadata": {},
   "source": [
    "## Configure Logging\n",
    "\n",
    "Set logging verbosity to info level for detailed training output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2cd69565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "\n",
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c346099c",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "Start the fine-tuning process. This trains only the LoRA adapter weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9225b5f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: Question, messages, __index_level_0__, Context, Answer. If Question, messages, __index_level_0__, Context, Answer are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 2,793\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2,793\n",
      "  Number of trainable parameters = 3,261,440\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2793' max='2793' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2793/2793 29:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.279500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.787800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.793100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.794400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.760900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.504700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.708100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.847100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.628100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.635700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.886900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.714100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.702700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.814800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.576400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.584500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.672000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.699500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.821800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.813700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.692500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.522300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.478700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.488100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.626000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.455500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.502200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.581000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.613700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.839600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.564100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.451900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.492200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.636900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.462400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.536000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.444100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.756600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.548100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.639500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.767200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.521400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.635500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.640700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.657800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.719700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.640300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.461900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.504800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.382400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.668300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.608500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.741900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.341800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.316500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.612100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.777700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>1.592800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.250200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.364400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.560800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.512500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.437100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>1.561900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.332000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>1.379200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.288800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>1.512400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.621100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.736900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.705700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.710500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.483600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>1.352400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.616100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>1.476900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.427000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>1.651700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.290600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>1.343800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.455400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.567700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.600900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>1.287000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.421400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>1.364100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.458900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>1.278400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.392800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>1.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.420400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.250600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.481000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>1.521100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.522400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>1.302300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.411000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>1.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.242600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>1.547200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.503800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.309800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.207300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>1.307300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.336100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>1.273600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.080900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>1.429600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.447100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>1.353800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>1.319000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.281600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>1.176500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>1.404700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>1.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.306900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>1.331700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>1.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>1.392100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>1.143100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.326500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>1.470200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>1.277700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>1.224200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>1.433400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.250400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>1.313800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>1.085300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>1.103800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>1.168100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.508500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>1.360200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>1.398000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>1.310500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>1.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.899700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>1.583900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>1.361100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>1.072700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>1.486100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.256200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>1.345700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>1.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>1.157300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>1.311200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.319800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>1.194600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>1.264800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>1.158700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>1.222800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.224700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>1.082500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>1.050100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>1.501800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>1.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.135600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>1.251600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>1.083800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>0.854900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>1.290300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.150900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>1.302100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>1.219700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>1.305700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>1.039900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>1.207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.992100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>1.214600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.932700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.184900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>1.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>1.131300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>1.469400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>1.071400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.141900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>1.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>1.285800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>1.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.903800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.826200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>1.224500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.978300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>1.382200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.808700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>1.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>1.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>0.971800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>1.125400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.931600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>1.007700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>1.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>0.917200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.049100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>0.819100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>1.075400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>0.857700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>1.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>1.146700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>1.049400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>1.101100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>1.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>1.082900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.997800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>0.933500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>0.997800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.105800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>1.235600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2170</td>\n",
       "      <td>0.726200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>0.860700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2190</td>\n",
       "      <td>0.981300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.992100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>1.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>0.917600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2230</td>\n",
       "      <td>1.039400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.984600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.049500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>0.940800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2270</td>\n",
       "      <td>1.097700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.799200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2290</td>\n",
       "      <td>1.285200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.794700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>0.829000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.854800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.978400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>0.739000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.080200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.969500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2370</td>\n",
       "      <td>1.205500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>0.959500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2390</td>\n",
       "      <td>0.659400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.798700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2410</td>\n",
       "      <td>1.183800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>0.781400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2430</td>\n",
       "      <td>0.748800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>0.826000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.706700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>0.581700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2470</td>\n",
       "      <td>1.058100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>1.072200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2490</td>\n",
       "      <td>0.910200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2510</td>\n",
       "      <td>0.690800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.969200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2530</td>\n",
       "      <td>1.301200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>0.742300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.874500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>0.961700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2570</td>\n",
       "      <td>0.826600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>0.916400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2590</td>\n",
       "      <td>0.768200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.825500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2610</td>\n",
       "      <td>0.649700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>0.988600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2630</td>\n",
       "      <td>0.904400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.763300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>1.046800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>0.690600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2670</td>\n",
       "      <td>0.795100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>0.720400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2690</td>\n",
       "      <td>1.121100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.913500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2710</td>\n",
       "      <td>1.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>0.733600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2730</td>\n",
       "      <td>0.894400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>0.942800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.788800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.730600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2770</td>\n",
       "      <td>0.583700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>0.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2790</td>\n",
       "      <td>0.883400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to gemma-output\\checkpoint-10\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-10\\special_tokens_map.json\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-20\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-20\\special_tokens_map.json\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-30\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-30\\special_tokens_map.json\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-40\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-10] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-50\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-20] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-60\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-30] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-70\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-40] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-80\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-50] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-90\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-60] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-100\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-100\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-100\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-70] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-110\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-110\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-110\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-80] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-120\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-120\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-120\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-90] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-130\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-130\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-130\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-100] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-140\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-140\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-140\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-110] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-150\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-150\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-150\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-120] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-160\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-160\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-160\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-130] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-170\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-170\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-170\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-140] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-180\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-180\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-180\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-150] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-190\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-190\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-190\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-160] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-200\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-200\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-200\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-170] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-210\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-210\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-210\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-180] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-220\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-220\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-220\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-190] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-230\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-230\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-230\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-200] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-240\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-240\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-240\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-210] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-250\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-250\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-250\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-220] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-260\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-260\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-260\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-230] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-270\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-270\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-270\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-240] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-280\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-280\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-280\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-250] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-290\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-290\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-290\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-260] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-300\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-300\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-300\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-270] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-310\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-310\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-310\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-280] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-320\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-320\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-320\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-290] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-330\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-330\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-330\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-300] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-340\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-340\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-340\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-310] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-350\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-350\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-350\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-320] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-360\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-360\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-360\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-330] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-370\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-370\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-370\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-340] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-380\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-380\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-380\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-350] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-390\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-390\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-390\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-360] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-400\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-400\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-400\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-370] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-410\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-410\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-410\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-380] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-420\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-420\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-420\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-390] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-430\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-430\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-430\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-400] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-440\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-440\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-440\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-410] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-450\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-450\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-450\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-420] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-460\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-460\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-460\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-430] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-470\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-470\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-470\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-440] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-480\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-480\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-480\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-450] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-490\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-490\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-490\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-460] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-500\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-500\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-470] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-510\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-510\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-510\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-480] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-520\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-520\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-520\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-490] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-530\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-530\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-530\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-500] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-540\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-540\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-540\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-510] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-550\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-550\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-550\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-520] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-560\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-560\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-560\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-530] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-570\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-570\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-570\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-540] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-580\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-580\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-580\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-550] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-590\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-590\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-590\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-560] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-600\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-600\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-600\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-570] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-610\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-610\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-610\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-580] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-620\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-620\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-620\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-590] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-630\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-630\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-630\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-600] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-640\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-640\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-640\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-610] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-650\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-650\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-650\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-620] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-660\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-660\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-660\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-630] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-670\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-670\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-670\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-640] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-680\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-680\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-680\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-650] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-690\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-690\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-690\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-660] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-700\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-700\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-700\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-670] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-710\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-710\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-710\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-680] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-720\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-720\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-720\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-690] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-730\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-730\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-730\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-700] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-740\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-740\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-740\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-710] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-750\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-750\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-750\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-720] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-760\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-760\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-760\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-730] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-770\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-770\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-770\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-740] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-780\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-780\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-780\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-750] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-790\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-790\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-790\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-760] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-800\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-800\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-800\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-770] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-810\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-810\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-810\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-780] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-820\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-820\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-820\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-790] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-830\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-830\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-830\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-800] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-840\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-840\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-840\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-810] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-850\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-850\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-850\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-820] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-860\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-860\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-860\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-830] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-870\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-870\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-870\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-840] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-880\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-880\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-880\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-850] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-890\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-890\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-890\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-860] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-900\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-900\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-900\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-870] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-910\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-910\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-910\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-880] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-920\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-920\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-920\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-890] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-930\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-930\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-930\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-900] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-940\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-940\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-940\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-910] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-950\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-950\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-950\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-920] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-960\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-960\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-960\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-930] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-970\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-970\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-970\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-940] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-980\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-980\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-980\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-950] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-990\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-990\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-990\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-960] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1000\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1000\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-970] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1010\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1010\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1010\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-980] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1020\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1020\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1020\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-990] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1030\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1030\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1030\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1000] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1040\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1040\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1040\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1010] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1050\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1050\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1050\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1020] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1060\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1060\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1060\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1030] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1070\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1070\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1070\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1040] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1080\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1080\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1080\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1050] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1090\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1090\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1090\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1060] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1100\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1100\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1100\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1070] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1110\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1110\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1110\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1080] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1120\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1120\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1120\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1090] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1130\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1130\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1130\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1100] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1140\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1140\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1140\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1110] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1150\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1150\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1150\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1120] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1160\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1160\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1160\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1130] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1170\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1170\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1170\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1140] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1180\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1180\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1180\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1150] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1190\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1190\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1190\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1160] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1200\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1200\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1200\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1170] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1210\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1210\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1210\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1180] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1220\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1220\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1220\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1190] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1230\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1230\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1230\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1200] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1240\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1240\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1240\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1210] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1250\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1250\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1250\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1220] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1260\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1260\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1260\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1230] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1270\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1270\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1270\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1240] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1280\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1280\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1280\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1250] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1290\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1290\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1290\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1260] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1300\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1300\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1300\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1270] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1310\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1310\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1310\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1280] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1320\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1320\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1320\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1290] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1330\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1330\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1330\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1300] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1340\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1340\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1340\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1310] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1350\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1350\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1350\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1320] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1360\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1360\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1360\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1330] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1370\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1370\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1370\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1340] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1380\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1380\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1380\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1350] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1390\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1390\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1390\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1360] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1400\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1400\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1400\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1370] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1410\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1410\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1410\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1380] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1420\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1420\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1420\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1390] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1430\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1430\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1430\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1400] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1440\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1440\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1440\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1410] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1450\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1450\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1450\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1420] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1460\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1460\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1460\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1430] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1470\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1470\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1470\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1440] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1480\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1480\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1480\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1450] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1490\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1490\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1490\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1460] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1500\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1500\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1470] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1510\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1510\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1510\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1480] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1520\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1520\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1520\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1490] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1530\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1530\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1530\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1500] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1540\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1540\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1540\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1510] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1550\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1550\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1550\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1520] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1560\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1560\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1560\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1530] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1570\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1570\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1570\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1540] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1580\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1580\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1580\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1550] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1590\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1590\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1590\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1560] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1600\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1600\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1600\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1570] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1610\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1610\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1610\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1580] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1620\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1620\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1620\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1590] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1630\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1630\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1630\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1600] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1640\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1640\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1640\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1610] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1650\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1650\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1650\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1620] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1660\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1660\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1660\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1630] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1670\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1670\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1670\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1640] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1680\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1680\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1680\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1650] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1690\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1690\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1690\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1660] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1700\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1700\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1700\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1670] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1710\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1710\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1710\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1680] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1720\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1720\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1720\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1690] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1730\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1730\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1730\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1700] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1740\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1740\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1740\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1710] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1750\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1750\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1750\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1720] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1760\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1760\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1760\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1730] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1770\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1770\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1770\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1740] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1780\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1780\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1780\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1750] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1790\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1790\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1790\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1760] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1800\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1800\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1800\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1770] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1810\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1810\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1810\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1780] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1820\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1820\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1820\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1790] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1830\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1830\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1830\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1800] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1840\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1840\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1840\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1810] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1850\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1850\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1850\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1820] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1860\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1860\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1860\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1830] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1870\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1870\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1870\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1840] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1880\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1880\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1880\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1850] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1890\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1890\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1890\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1860] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1900\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1900\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1900\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1870] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1910\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1910\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1910\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1880] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1920\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1920\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1920\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1890] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1930\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1930\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1930\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1900] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1940\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1940\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1940\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1910] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1950\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1950\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1950\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1920] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1960\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1960\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1960\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1930] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1970\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1970\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1970\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1940] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1980\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1980\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1980\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1950] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-1990\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-1990\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-1990\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1960] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2000\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2000\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1970] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2010\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2010\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2010\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1980] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2020\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2020\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2020\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-1990] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2030\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2030\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2030\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2000] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2040\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2040\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2040\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2010] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2050\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2050\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2050\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2020] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2060\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2060\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2060\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2030] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2070\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2070\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2070\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2040] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2080\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2080\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2080\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2050] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2090\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2090\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2090\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2060] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2100\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2100\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2100\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2070] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2110\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2110\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2110\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2080] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2120\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2120\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2120\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2090] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2130\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2130\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2130\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2100] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2140\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2140\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2140\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2110] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2150\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2150\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2150\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2120] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2160\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2160\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2160\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2130] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2170\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2170\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2170\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2140] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2180\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2180\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2180\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2150] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2190\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2190\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2190\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2160] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2200\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2200\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2200\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2170] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2210\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2210\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2210\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2180] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2220\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2220\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2220\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2190] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2230\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2230\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2230\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2200] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2240\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2240\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2240\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2210] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2250\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2250\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2250\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2220] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2260\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2260\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2260\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2230] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2270\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2270\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2270\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2240] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2280\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2280\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2280\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2250] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2290\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2290\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2290\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2260] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2300\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2300\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2300\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2270] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2310\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2310\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2310\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2280] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2320\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2320\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2320\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2290] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2330\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2330\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2330\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2300] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2340\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2340\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2340\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2310] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2350\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2350\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2350\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2320] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2360\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2360\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2360\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2330] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2370\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2370\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2370\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2340] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2380\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2380\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2380\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2350] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2390\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2390\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2390\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2360] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2400\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2400\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2400\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2370] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2410\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2410\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2410\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2380] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2420\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2420\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2420\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2390] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2430\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2430\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2430\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2400] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2440\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2440\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2440\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2410] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2450\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2450\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2450\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2420] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2460\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2460\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2460\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2430] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2470\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2470\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2470\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2440] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2480\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2480\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2480\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2450] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2490\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2490\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2490\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2460] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2500\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2500\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2470] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2510\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2510\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2510\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2480] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2520\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2520\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2520\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2490] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2530\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2530\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2530\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2500] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2540\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2540\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2540\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2510] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2550\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2550\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2550\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2520] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2560\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2560\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2560\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2530] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2570\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2570\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2570\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2540] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2580\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2580\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2580\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2550] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2590\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2590\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2590\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2560] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2600\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2600\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2600\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2570] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2610\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2610\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2610\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2580] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2620\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2620\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2620\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2590] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2630\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2630\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2630\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2600] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2640\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2640\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2640\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2610] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2650\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2650\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2650\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2620] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2660\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2660\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2660\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2630] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2670\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2670\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2670\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2640] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2680\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2680\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2680\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2650] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2690\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2690\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2690\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2660] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2700\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2700\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2700\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2670] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2710\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2710\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2710\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2680] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2720\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2720\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2720\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2690] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2730\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2730\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2730\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2700] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2740\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2740\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2740\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2710] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2750\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2750\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2750\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2720] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2760\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2760\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2760\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2730] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2770\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2770\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2770\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2740] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2780\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2780\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2780\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2750] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2790\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2790\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2790\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2760] due to args.save_total_limit\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to gemma-output\\checkpoint-2793\n",
      "loading configuration file models/gemma-3-1b-pt\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "tokenizer config file saved in gemma-output\\checkpoint-2793\\tokenizer_config.json\n",
      "Special tokens file saved in gemma-output\\checkpoint-2793\\special_tokens_map.json\n",
      "Deleting older checkpoint [gemma-output\\checkpoint-2770] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2793, training_loss=1.262669873416872, metrics={'train_runtime': 1794.079, 'train_samples_per_second': 1.557, 'train_steps_per_second': 1.557, 'total_flos': 3818256959342592.0, 'train_loss': 1.262669873416872, 'entropy': 0.5947626928488413, 'num_tokens': 907608.0, 'mean_token_accuracy': 0.8280795812606812, 'epoch': 1.0})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a0d29",
   "metadata": {},
   "source": [
    "## Merge and Save Model\n",
    "\n",
    "Merge LoRA adapters into the base model and save the fine-tuned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a93dfeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unloading and merging model:   0%|          | 0/632 [00:00<?, ?it/s]C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "Unloading and merging model: 100%|██████████| 632/632 [00:01<00:00, 473.86it/s]\n"
     ]
    }
   ],
   "source": [
    "merged_model = model.merge_and_unload(progressbar=True)\n",
    "merged_model._hf_peft_config_loaded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7cd96817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in model_finetuned\\config.json\n",
      "Configuration saved in model_finetuned\\generation_config.json\n",
      "Model weights saved in model_finetuned\\model.safetensors\n"
     ]
    }
   ],
   "source": [
    "merged_model.save_pretrained('model_finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4fee8f",
   "metadata": {},
   "source": [
    "## Load Fine-tuned Model\n",
    "\n",
    "Load the merged fine-tuned model for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "798a57b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_finetuned\\config.json\n",
      "Model config Gemma3TextConfig {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": null,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6912,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"gemma3_text\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_local_base_freq\": 10000,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": 512,\n",
      "  \"sliding_window_pattern\": 6,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 262144\n",
      "}\n",
      "\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "C:\\Users\\user\\anaconda3\\envs\\ml-interview-assistant-env\\Lib\\site-packages\\transformers\\quantizers\\auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  quantization_config = AutoRoundConfig.from_dict(quantization_config)\n",
      "The device_map was not initialized. Setting device_map to {'': 0}. If you want to use the model for inference, please set device_map ='auto' \n",
      "loading weights file model_finetuned\\model.safetensors\n",
      "Instantiating Gemma3ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing Gemma3ForCausalLM.\n",
      "\n",
      "All the weights of Gemma3ForCausalLM were initialized from the model checkpoint at model_finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Gemma3ForCausalLM for predictions without further training.\n",
      "loading configuration file model_finetuned\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    1,\n",
      "    106\n",
      "  ],\n",
      "  \"pad_token_id\": 0,\n",
      "  \"top_k\": 64,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_ft = AutoModelForCausalLM.from_pretrained(\n",
    "    \"model_finetuned\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f28b5b",
   "metadata": {},
   "source": [
    "## Test Model - Example 1\n",
    "\n",
    "Test the fine-tuned model on a test example where the answer is in the context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "670fde12",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ds[\"test\"][0]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "ae1c6433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'You are an expert assistant helping a candidate prepare for a Data Scientist interview',\n",
       "  'role': 'system'},\n",
       " {'content': \"\\n[CONTEXT]\\nLLM\\nPositional Embeddings\\nGPT vs BERT: What’s The Difference?_1\\nBERT is a Transformer encoder, which means that, for each position in the input, the output at the same position is the same token (or the [MASK] token for masked tokens), that is the inputs and output positions of each token are the same. Models with only an encoder stack like BERT generate all its outputs at once.\\nBERT has two training objectives, and the most important of them is the Masked Language Modeling (MLM) objective. is With the MLM objective, at step the following happens:\\nselect some tokens\\n(each token is selected with the probability of 15%)\\nreplace these selected tokens\\n(with the special token\\xa0[MASK]\\xa0- with p=80%, with a random token - with p=10%, with the original token (remain unchanged) - with p=10%)\\npredict original tokens (compute loss).\\nThe illustration below shows an example of a training step for one sentence. You can go over the slides to see the whole process.\\nGPT is an autoregressive transformer decoder, which means that each token is predicted and conditioned on the previous token. We don't need an encoder, because the previous tokens are received by the decoder itself. This makes these models really good at tasks like language generation, but not good at classification. These models can be trained with unlabeled large text corpora from books or web articles.\\nIn conclusion, while both GPT and BERT are examples of transformer architectures that have been influencing the field of natural language processing in recent years, they have different strengths and weaknesses that make them suitable for different types of tasks. GPT excels at generating long sequences of text with high accuracy whereas BERT focuses more on the understanding context within given texts in order to perform more sophisticated tasks such as question answering or sentiment analysis. Data scientists, developers, and machine learning engineers should decide which architecture best fits their needs before embarking on any NLP project using either model. Ultimately, both GPT\\n[/CONTEXT]\\n\\n[QUESTION]\\nWhat type of Transformer architecture is BERT?\\n[/QUESTION]\\n\\nAnswer the QUESTION based on the provided CONTEXT.\\n\",\n",
       "  'role': 'user'},\n",
       " {'content': 'BERT is a Transformer encoder.', 'role': 'assistant'}]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "685ad203",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer.apply_chat_template(example, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "fed560cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_ft.generate(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "032bf73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>user\n",
      "You are an expert assistant helping a candidate prepare for a Data Scientist interview\n",
      "<end_of_turn>\n",
      "<start_of_turn>user\n",
      "\n",
      "[CONTEXT]\n",
      "LLM\n",
      "Positional Embeddings\n",
      "GPT vs BERT: What’s The Difference?_1\n",
      "BERT is a Transformer encoder, which means that, for each position in the input, the output at the same position is the same token (or the [MASK] token for masked tokens), that is the inputs and output positions of each token are the same. Models with only an encoder stack like BERT generate all its outputs at once.\n",
      "BERT has two training objectives, and the most important of them is the Masked Language Modeling (MLM) objective. is With the MLM objective, at step the following happens:\n",
      "select some tokens\n",
      "(each token is selected with the probability of 15%)\n",
      "replace these selected tokens\n",
      "(with the special token [MASK] - with p=80%, with a random token - with p=10%, with the original token (remain unchanged) - with p=10%)\n",
      "predict original tokens (compute loss).\n",
      "The illustration below shows an example of a training step for one sentence. You can go over the slides to see the whole process.\n",
      "GPT is an autoregressive transformer decoder, which means that each token is predicted and conditioned on the previous token. We don't need an encoder, because the previous tokens are received by the decoder itself. This makes these models really good at tasks like language generation, but not good at classification. These models can be trained with unlabeled large text corpora from books or web articles.\n",
      "In conclusion, while both GPT and BERT are examples of transformer architectures that have been influencing the field of natural language processing in recent years, they have different strengths and weaknesses that make them suitable for different types of tasks. GPT excels at generating long sequences of text with high accuracy whereas BERT focuses more on the understanding context within given texts in order to perform more sophisticated tasks such as question answering or sentiment analysis. Data scientists, developers, and machine learning engineers should decide which architecture best fits their needs before embarking on any NLP project using either model. Ultimately, both GPT\n",
      "[/CONTEXT]\n",
      "\n",
      "[QUESTION]\n",
      "What type of Transformer architecture is BERT?\n",
      "[/QUESTION]\n",
      "\n",
      "Answer the QUESTION based on the provided CONTEXT.\n",
      "\n",
      "<end_of_turn>\n",
      "<start_of_turn>model\n",
      "BERT is a Transformer encoder.\n",
      "<end_of_turn>\n",
      "<start_of_turn>model\n",
      "BERT is a Transformer encoder that has an MLM objective.\n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c64691-7d41-4c9f-b08b-140671b0e78d",
   "metadata": {},
   "source": [
    "## Test Model - Example 2\n",
    "\n",
    "Test the model on a case where the answer isn't in the context to evaluate handling of out-of-context questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "196fd52c-cff0-4bc7-b1c9-e1428c6037af",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ds[\"test\"][102]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ae8866e5-a9c2-4045-96c5-4cae51b549b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'You are an expert assistant helping a candidate prepare for a Data Scientist interview',\n",
       "  'role': 'system'},\n",
       " {'content': '\\n[CONTEXT]\\nProbability and Statistics\\nProbability \\nJoint, marginal and conditional probabilities _1\\nFor two random variables X and Y , the probability that X = x and Y = y is (lazily) written as p(x, y) and is called the joint probability. One can think of a probability as a function that takes state x and y and returns a real number, which is the reason we write p(x, y). The marginal probability that X takes the value x irrespective of the value of random variable Y is (lazily) written as p(x). We write X ∼ p(x) to denote that the random variable X is distributed according to p(x). If we consider only the instances where X = x, then the fraction of instances (the conditional probability) for which Y = y is written (lazily) as p(y | x).\\n[/CONTEXT]\\n\\n[QUESTION]\\nHow is the final prediction determined in the decoding process?\\n[/QUESTION]\\n\\nAnswer the QUESTION based on the provided CONTEXT.\\n',\n",
       "  'role': 'user'},\n",
       " {'content': \"It's not possible for me to answer this question at this time. \",\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7b55f8ff-03ca-4430-9739-15d30e248bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer.apply_chat_template(example, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b1853f9a-731c-48ee-b48c-75b53368eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_ft.generate(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c08263cc-9c87-41ee-92e7-5fa844fa13f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>user\n",
      "You are an expert assistant helping a candidate prepare for a Data Scientist interview\n",
      "<end_of_turn>\n",
      "<start_of_turn>user\n",
      "\n",
      "[CONTEXT]\n",
      "Probability and Statistics\n",
      "Probability \n",
      "Joint, marginal and conditional probabilities _1\n",
      "For two random variables X and Y , the probability that X = x and Y = y is (lazily) written as p(x, y) and is called the joint probability. One can think of a probability as a function that takes state x and y and returns a real number, which is the reason we write p(x, y). The marginal probability that X takes the value x irrespective of the value of random variable Y is (lazily) written as p(x). We write X ∼ p(x) to denote that the random variable X is distributed according to p(x). If we consider only the instances where X = x, then the fraction of instances (the conditional probability) for which Y = y is written (lazily) as p(y | x).\n",
      "[/CONTEXT]\n",
      "\n",
      "[QUESTION]\n",
      "How is the final prediction determined in the decoding process?\n",
      "[/QUESTION]\n",
      "\n",
      "Answer the QUESTION based on the provided CONTEXT.\n",
      "\n",
      "<end_of_turn>\n",
      "<start_of_turn>model\n",
      "It's not possible for me to answer this question at this time. \n",
      "<end_of_turn>\n",
      "<start_of_turn>model\n",
      "This is not possible for me to answer at this time. \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0], skip_special_tokens=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-interview-assistant-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
